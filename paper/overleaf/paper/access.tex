\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
% \usepackage{caption} % Removed: Conflicts with the ieeeaccess class
% \usepackage{mathptmx} % Disabled to avoid RSFS font requirement on minimal TeX
\usepackage[english]{babel}
% microtype can conflict with times fonts in ieeeaccess; disable for portability
% \usepackage{microtype}
% Packages added for the comparison table
\usepackage{booktabs}
\usepackage[table]{xcolor} % Use [table] option for better compatibility
\usepackage{tabularx} % For auto-wrapping text in tables
\usepackage{multirow}

% xurl may not be present in minimal TeX installs; fall back to url
\IfFileExists{xurl.sty}{\usepackage{xurl}}{\usepackage{url}}
\usepackage[hidelinks]{hyperref}

\usepackage{bm}

% Fallback mapping for RSFS font to Computer Modern symbols (avoids rsfs10)
\makeatletter
\DeclareFontFamily{U}{rsfs}{}
\DeclareFontShape{U}{rsfs}{m}{n}{<-6> s*[1.05] cmsy5 <6-8> s*[1.05] cmsy7 <8-> s*[1.05] cmsy10}{}
\DeclareSymbolFont{rsfs}{U}{rsfs}{m}{n}
\DeclareSymbolFontAlphabet{\mathscr}{rsfs}
\makeatother

% Ensure local Type 1 font maps (bundled with template) are loaded
\pdfmapfile{+paper/t1-formata.map}
\pdfmapfile{+paper/t1-times.map}
\pdfmapfile{+paper/t1-helvetica.map}
\pdfmapfile{+paper/t1-giovannistd.map}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%Your document starts from here ___________________________________________________
\begin{document}
\history{}
\doi{DOI: TBD}

% (Revert to default IEEE Access title page styling)
\title{Closed-Loop Threat-Guided Auto-Fixing of Kubernetes YAML Security Misconfigurations}
\author{\uppercase{Brian Mendonca}\authorrefmark{1}, and
\uppercase{Vijay K. Madisetti}\authorrefmark{2}, \IEEEmembership{Fellow, IEEE}}

\address[1]{College of Computing, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: brian.mendonca6@gmail.com)}
\address[2]{School of Cybersecurity and Privacy, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: vkm@gatech.edu)}

\markboth
{Mendonca \headeretal: Closed-Loop K8s YAML Auto-Fix}
{Mendonca \headeretal: Closed-Loop K8s YAML Auto-Fix}

\corresp{Corresponding author: Dr. Vijay Madisetti (e-mail: vkm@gatech.edu).}

% \tfootnote{This work was supported in part by [Funding Source, if applicable].}

\titlepgskip=-21pt

% Abstract and keywords must be defined before \maketitle for ieeeaccess
\begin{abstract}
Misconfigured Kubernetes manifests expand blast radius when pipelines stop at detection. We present \texttt{k8s-auto-fix}, a closed loop (\emph{Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler}) that defaults to deterministic rules and can blend Grok/xAI patches behind the same guardrails. A 1,000-manifest live-cluster replay reaches 100.0\% success (1,000/1,000) with zero rollbacks and perfect dry-run/live-apply alignment (Table~\ref{tab:eval_summary}, \path{data/live_cluster/results_1k.json}). Deterministic rules cover 13{,}589/13{,}656 detections (99.51\%), while Grok/xAI fixes the entire 1{,}313-manifest slice (100\%) and 4{,}439/5{,}000 manifests (88.78\%) from the Grok corpus under \texttt{kubectl apply --dry-run=server} (Table~\ref{tab:eval_summary}, \path{data/eval/unified_eval_summary.json}). The verifier triad (policy re-check, schema validation, server-side dry-run) and hardening guardrails admit no regressions across accepted patches (Table~\ref{tab:verifier_ablation}). A risk-aware scheduler cuts top-risk P95 wait from 102.3~h (FIFO) to 13.0~h (7.9$\times$) with fairness metrics derived from reproducible queue replays (\path{data/scheduler/metrics_sweep_live.json}, \path{data/outputs/scheduler/metrics_schedule_sweep.json}). We release scripts, hashed telemetry, and audit logs so reviewers can regenerate every table and figure that underpins these claims (\path{ARTIFACTS.md}).
\end{abstract}

\begin{keywords}
Kubernetes, YAML, Pod Security, JSON Patch, Policy Enforcement, Kyverno, OPA Gatekeeper, Auto-fix, CI/CD, CVE, EPSS, RAG, Risk-based scheduling
\end{keywords}

\maketitle

\sloppy

\section{Importance of the Problem}
Kubernetes YAML is easy to get wrong: a single \texttt{privileged: true}, a \texttt{:latest} image tag, or a missing \texttt{runAsNonRoot} can expand blast radius and undermine defense-in-depth. Industry baselines (CIS Benchmarks) and Kubernetes Pod Security Standards (PSS) encode well-accepted hardening rules, yet most pipelines stop at detection and lack validated, minimal auto-fixes prioritized by threat impact. This project targets that gap with measured improvements on Auto-fix rate, No-new-violations\%, Time-to-patch, and \emph{risk reduction} (with fairness) on a held-out corpus—directly aligned with industry standards and research objectives (\cite{cis_benchmarks}, \cite{pss}).

The closed-loop verification triad and risk-aware scheduling goals mirror the evaluation criteria used by security venues such as IEEE S\&P, USENIX Security, and NDSS: demonstrable risk reduction, strong guardrails against regressions, and operator-in-the-loop evidence. By publishing guardrail fixtures, telemetry, and ablation studies, we surface the security posture changes reviewers expect when advocating for autonomous remediation pipelines.

%====================
% Comparison Table (Updated)
%====================
\begin{table*}[t!]
\centering
\small
\caption{Comparison of automated Kubernetes remediation systems (Oct.~2025 snapshot).}
\label{tab:comparison}
\begin{tabularx}{\textwidth}{@{}l >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X@{}}
\toprule
\textbf{Capability} & \textbf{k8s-auto-fix (this work)} & \textbf{GenKubeSec \cite{malul2024}} & \textbf{Kyverno \cite{kyverno_docs}} & \textbf{Borg/SRE \cite{borg}} \\
\midrule
\rowcolor{gray!10}
\textbf{Primary Goal} & Closed-loop hardening (detect$\rightarrow$patch$\rightarrow$verify$\rightarrow$prioritize) & LLM-based detection/remediation suggestions & Admission-time policy enforcement & Large-scale auto-remediation in production clusters \\
\midrule
\textbf{Fix Mode} & JSON Patch (rules + optional LLM) & LLM-generated YAML edits & Policy mutation/generation & Custom controllers and playbooks \\
\rowcolor{gray!10}
\textbf{Guardrails} & Policy re-check + schema + \texttt{kubectl apply --dry-run=server} + privileged/secret sanitization + CRD seeding & Manual review; no automated gates & Validation/mutation webhooks; assumes controllers & Health checks, automated rollback, throttling \\
\textbf{Risk Prioritization} & Bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) & Not implemented & FIFO admission queue & Priority queues / toil budgets \\
\rowcolor{gray!10}
\textbf{Evaluation Corpus} & 1{,}000 live-cluster manifests (100.0\% success); 5{,}000 Grok manifests (88.78\%); 1{,}264 supported manifests (100.00\% rules); 1{,}313 manifest slice (99.51\% rules / 100.00\% Grok) & 200 curated manifests (85--92\% accuracy) & Thousands of user manifests (80--95\% mutation acceptance) & Millions of production workloads (no public acceptance \%) \\
\textbf{Telemetry} & Policy-level success probabilities, latency histograms, failure taxonomy & Token/cost estimates; no pipeline telemetry & Admission latency $<45$~ms, violation counts & MTTR, incident counts, operator feedback \\
\rowcolor{gray!10}
\textbf{Outstanding Gaps} & Infrastructure-dependent rejects, operator study, scheduled guidance refresh in CI & Automated guardrails, risk-aware ordering & LLM-aware patching, risk-aware scheduling & Declarative manifest fixes, static analysis integration \\
\bottomrule
\end{tabularx}
\end{table*}

\smallskip
\noindent\textbf{Metric caveat.} Table~\ref{tab:comparison} aggregates metrics reported by prior work that span admission latency, MTTR, and acceptance rates, so values are not strictly comparable; they provide qualitative context only.
\section{Brief Related Work and Gaps}
Recent work has explored LLM prompts (GenKubeSec \cite{malul2024}), admission policy engines (Kyverno \cite{kyverno_docs}), and large-scale SRE playbooks (Borg \cite{borg}) for Kubernetes remediation, yet critical gaps remain for a production-ready, automated system. GenKubeSec localizes and suggests fixes but leaves validation to humans, lacking schema/dry-run guardrails. Kyverno mutates manifests at admission-time but does not prioritize fixes or auto-seed third-party CRDs. Borg-style automation excels at infrastructure remediation yet is not openly available for manifest-level hardening. Table~\ref{tab:comparison} situates our closed-loop pipeline relative to these efforts, combining automated patching, triad verification, and risk-aware scheduling with published acceptance metrics on multi-thousand manifest corpora.

\smallskip
\noindent\textbf{1. Detection-Only Pipelines.} Static analysis tools like \texttt{kube-linter} and policy engines such as Kyverno and OPA Gatekeeper excel at identifying misconfigurations (\cite{kube_linter_docs}, \cite{kyverno_docs}, \cite{opa_gatekeeper}). However, their core function is detection and admission control, not the generation of validated, minimal patches. Our work uses these powerful tools as the \emph{Detector} and \emph{Verifier} components in a broader remediation workflow.

\smallskip
\noindent\textbf{2. Lack of Closed-Loop Verification.} Few remediation pipelines enforce a rigorous, multi-gate verification process. A key novelty of our approach is the Verifier's triad of checks: a policy re-check to confirm the original violation is gone, schema validation to ensure correctness, and a server-side dry-run (\texttt{kubectl apply --dry-run=server}) to simulate the application of the patch against the Kubernetes API server, ensuring no new violations are introduced (\cite{kubectl_reference}).

\smallskip
\noindent\textbf{3. Inefficient Prioritization.} Security work queues are often processed in a First-In, First-Out (FIFO) manner. This can leave high-impact vulnerabilities unpatched while the system works on lower-priority issues. We propose and test a \textbf{risk-based, learning-aware scheduler} that integrates CVE/CTI signals (CVSS, EPSS, KEV) and online outcomes (verifier pass/fail) using a contextual bandit with aging and KEV preemption, aiming to maximize risk reduction while preserving fairness.

\section{Approach Summary}
We realize the closed loop \emph{Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler} shown in Figure~\ref{fig:architecture}. Detectors produce structured JSON findings; the proposer applies rule-based guards (with optional LLM backends) to emit minimal JSON Patches; the verifier enforces policy re-checks, schema validation, and \texttt{kubectl apply --dry-run=server}; and the scheduler orders work using risk-aware bandit scoring. Each stage persists artifacts (detections, patches, verified outcomes, queue scores), enabling reproducible evaluation (Section~\ref{sec:evaluation}).

\smallskip
\noindent\textbf{Disagreement and Budgets.} When kube-linter and Kyverno/OPA disagree we take the \emph{union} of violations at detection time, and require patches to satisfy both engines during verification. Attempts are capped at three per manifest; per-attempt latency and success outcomes feed into \texttt{data/policy\_metrics.json}, which the scheduler consumes alongside KEV flags.

\subsection{Research Questions and Findings}
\begin{enumerate}
    \item[\textbf{RQ1}] \textbf{Robustness:} The closed loop delivers 88.78\% acceptance on the Grok-5k sweep, 100.00\% on the supported 1,264-manifest corpus in rules mode, and 100.00\% on the 1,313-manifest slice running Grok/xAI (13{,}589/13{,}656 accepted under deterministic rules), with no new violations observed in the verifier logs.
    \item[\textbf{RQ2}] \textbf{Scheduling Effectiveness:} The bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) improves risk reduction per hour and reduces top-risk P95 wait from 102.3~hours (FIFO) to 13.0~hours ($7.9\times$).
    \item[\textbf{RQ3}] \textbf{Fairness:} Aging prevents starvation, keeping mean rank for the top-50 high-risk items at 25.5 while still progressing lower-risk items.
    \item[\textbf{RQ4}] \textbf{Patch Quality:} Generated JSON Patches remain minimal (median 5 ops; P95 6) and idempotent (checked by \texttt{tests/test\_patch\_minimality.py}).
\end{enumerate}

\section{Implementation and Metrics}\label{sec:impl-metrics}
Our system is designed as a linear pipeline with strict verification gates to ensure the safety and correctness of all proposed patches.
\smallskip
\noindent\textbf{Scalability considerations.} The end-to-end pipeline sustains millisecond-scale proposer latency and sub-second verifier latency on the 1,313-manifest slice (Table~\ref{tab:eval_summary}); the scheduler replays thousands of queue items using persisted telemetry (see \path{data/scheduler/}) without recomputing detections. These characteristics are highlighted to satisfy systems venues (e.g., OSDI, NSDI) that emphasize throughput, resource bounds, and repeatable performance claims alongside functional correctness.

\subsection{The Closed-Loop Pipeline}
The workflow consists of four stages:
\begin{itemize}
    \item \textbf{Detector:} Ingests a Kubernetes manifest and uses both \texttt{kube-linter} and a policy engine (Kyverno/OPA) to identify violations. It takes the union of all findings.
    \item \textbf{Proposer:} Takes the manifest and violation data and generates a JSON Patch. The shipped implementation defaults to deterministic rules for the policies we currently cover (\texttt{no\_latest\_tag}, \texttt{no\_privileged}) but can call an OpenAI-compatible endpoint when configured via \texttt{configs/run.yaml}.
    \item \textbf{Verifier:} Applies the patch to a copy of the manifest and subjects it to the verification gates described below, recording evidence in \texttt{data/verified.json}.
    \item \textbf{Budget-aware Retry:} A configurable retry budget (\texttt{max\_attempts} in \texttt{configs/run.yaml}, default 3) allows the proposer to re-attempt if verification fails, logging the error trace for inspection.
\end{itemize}

\subsection{Verification Gates}
To be accepted, a patched manifest must pass a multi-layered verification process:
\begin{enumerate}
    \item \textbf{Policy Re-check:} The patched manifest is re-evaluated with the same policy logic that triggered the violation. Implemented as explicit assertions for each covered policy (\texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{run\_as\_non\_root}, \texttt{read\_only\_root\_fs}, etc.); the detector hook for re-scanning is available via \texttt{--enable-rescan}.
    \item \textbf{Schema Validation:} Structural validity is checked by applying the JSON Patch via \texttt{jsonpatch}; malformed paths or operations are rejected and surfaced to the retry loop.
    \item \textbf{Server-side Dry-run:} When \texttt{kubectl} is available, the system executes \texttt{kubectl apply --dry-run=server} to simulate how the Kubernetes API server would handle the change. Failures mark the patch as not accepted and persist the CLI output for analysis.
    \item \textbf{No-New-Violations Safety Gates:} Universal security assertions enforced for all patches to prevent regressions:
    \begin{itemize}
        \item \textbf{No privileged containers:} Blocks \texttt{privileged: true} in any container
        \item \textbf{runAsNonRoot enforcement:} Requires \texttt{runAsNonRoot: true} or \texttt{runAsUser}$\neq$0 when security context is modified
        \item \textbf{readOnlyRootFilesystem:} Mandates \texttt{readOnlyRootFilesystem: true} for security-sensitive patches
        \item \textbf{Drop ALL capabilities:} Enforces \texttt{capabilities.drop: [ALL]} when capabilities are touched
        \item \textbf{hostPath allowlist:} Restricts host mounts to approved paths (\path{/var/run/secrets/kubernetes.io/serviceaccount}, \path{/var/lib/kubelet/pods}, \path{/etc/ssl/certs})
    \end{itemize}
\end{enumerate}

% Simple architecture figure using boxed stages and arrows
\begin{figure*}[t]
\centering
\setlength{\fboxsep}{6pt}%
\setlength{\fboxrule}{0.6pt}%
\fbox{\begin{minipage}{0.18\textwidth}\centering\textbf{Detector}\\\small kube-linter + Kyverno/OPA\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.18\textwidth}\centering\textbf{Proposer}\\\small LLM + JSON Patch\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.18\textwidth}\centering\textbf{Verifier}\\\small Policy + Schema + Dry-run\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.18\textwidth}\centering\textbf{Scheduler}\\\small Risk-Bandit (aging + KEV)\end{minipage}}

\vspace{0.6em}
\fbox{\begin{minipage}{0.28\textwidth}\centering\textbf{Risk Signals}\\\small CVE/KEV/EPSS + Exposure\end{minipage}}
\hspace{1em}
\fbox{\begin{minipage}{0.28\textwidth}\centering\textbf{RAG Store}\\\small PSS/CIS, K8s docs, CVEs\end{minipage}}
\caption{Closed-loop architecture with detector, proposer, and verifier gates (policy re-check, schema validation, \texttt{kubectl apply --dry-run=server}) feeding the risk-aware scheduler. The scheduler consumes \texttt{policy\_metrics.json} entries \{${p}$, $\mathbb{E}[t]$, $R$, KEV\} to score work using the scheduling function, while the RAG store grounds LLM prompts.}
\label{fig:architecture}
\end{figure*}

\section{Implementation Status and Evidence}

Table~\ref{tab:evidence} ties each pipeline stage to the concrete code and artifacts currently in the \texttt{k8s-auto-fix} repository. The implementation operates end-to-end in rules mode without external API dependencies; LLM-backed modes are configurable and evaluated off-line, while the default reproducible path uses rules mode.
\smallskip
\noindent\textbf{DevOps rollout.} The checklist in the docs (see \path{docs/devops_adoption_checklist.md}) distills the CI/CD integration path—bootstrapping dependencies, wiring detector/proposer/verifier stages into pipelines, publishing fixtures, and capturing operator feedback—so platform teams can reproduce Table~\ref{tab:eval_summary} outcomes before expanding to LLM-backed modes. A containerized path (see \path{docs/container_repro.md}) builds on the same artifacts for hermetic evaluations.

\begin{table*}[t]
\centering
\caption{Evidence for each stage of the implemented pipeline (October 2025 snapshot).}
\label{tab:evidence}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabularx}{\textwidth}{@{}l l X@{}}
\toprule
\textbf{Stage} & \textbf{Implementation}\footnotemark[1] & \textbf{Artifacts Produced}\footnotemark[2] \\
\midrule
Detector & \begin{tabular}[t]{@{}l@{}}\texttt{src/detector/detector.py}\\ \texttt{src/detector/cli.py}\end{tabular} & Records in \texttt{data/detections.json} with fields \{\texttt{id}, \texttt{manifest\_path}, \texttt{manifest\_yaml}, \texttt{policy\_id}, \texttt{violation\_text}\}; seeded by \texttt{data/manifests/001.yaml} and \texttt{002.yaml}. \\
Proposer & \begin{tabular}[t]{@{}l@{}}\texttt{src/proposer/cli.py}\\ \texttt{model\_client.py}, \texttt{guards.py}\end{tabular} & \texttt{data/patches.json} containing guarded JSON Patch arrays. Rules mode emits single-operation fixes; vendor/vLLM modes require OpenAI-compatible endpoints configured in \texttt{configs/run.yaml}. \\
Verifier & \begin{tabular}[t]{@{}l@{}}\texttt{src/verifier/verifier.py}\\ \texttt{src/verifier/cli.py}\end{tabular} & \texttt{data/verified.json} logging \texttt{accepted}, \texttt{ok\_schema}, \texttt{ok\_policy}, and \texttt{patched\_yaml}. Current policy checks assert the \texttt{no\_latest\_tag} and \texttt{no\_privileged} invariants. \\
Scheduler & \begin{tabular}[t]{@{}l@{}}\texttt{src/scheduler/schedule.py}\\ \texttt{src/scheduler/cli.py}\end{tabular} & \texttt{data/schedule.json} with per-item scores and components \{\texttt{score}, \texttt{R}, \texttt{p}, \texttt{Et}, \texttt{wait}, \texttt{kev}\}; risk constants presently keyed to policy IDs. \\
Automation & \texttt{Makefile} & Reproducible commands for each stage: \texttt{make detect}, \texttt{make propose}, \texttt{make verify}, \texttt{make schedule}, \texttt{make e2e}. \\
Testing & \texttt{tests/} & \texttt{python -m unittest discover -s tests} (16 tests, 2 skipped until patches exist) covering detector contracts, proposer guards, verifier gates, scheduler ordering, patch idempotence. \\
\midrule
\multicolumn{3}{@{}l@{}}{\textbf{Runtime Toolchain Versions (Evaluation Environment)}} \\
\midrule
Environment & \begin{tabular}[t]{@{}l@{}}Python 3.12.4\\ kubectl 1.34.1\\ kube-linter 0.7.6\\ Kind 0.30.0\end{tabular} & Kubernetes cluster: 1.34.0 (Kind); Kyverno CLI baseline simulated (no binary required); OPA Gatekeeper not used in current evaluation; all scripts compatible with Python 3.10+ \\
\bottomrule
\end{tabularx}
\end{table*}

\footnotetext[1]{All paths are relative to the project root.}
\footnotetext[2]{Artifacts live under \texttt{data/\*.json} after running the corresponding \texttt{make} targets.}

\subsection{Sample Detection Record}
When detector binaries are available, running \texttt{make detect} (rules mode) produces records with the following shape (values truncated for brevity):

\begingroup
\small
\begin{verbatim}
{
  "id": "001",
  "manifest_path": "data/manifests/001.yaml",
  "manifest_yaml": "apiVersion: v1\n"
                   "kind: Pod\n...",
  "policy_id": "no_latest_tag",
  "violation_text": "Image uses :latest tag"
}
\end{verbatim}
\endgroup

The \texttt{manifest\_yaml} field embeds the literal YAML to decouple downstream stages from the filesystem.

\subsection{Unit Test Evidence}
Executing \texttt{python -m unittest discover -s tests} yields \texttt{16 tests in 0.02s, OK (skipped=2)} on macOS (Apple M-series, Python~3.12). The skipped cases correspond to the optional patch minimality suite, which activates after \texttt{data/patches.json} is generated.

\smallskip
\noindent\textbf{Property-based tests.} In addition to the deterministic contract tests, \texttt{tests/test\_property\_guards.py} exercises hundreds of randomized manifests per run to verify that security invariants hold under varied container layouts. These property-based checks confirm that the proposer enforces RuntimeDefault seccomp profiles, drops every dangerous capability (including \texttt{ALL}), denies privilege escalation, strips disallowed \texttt{hostPath} mounts, and hardens \texttt{runAsNonRoot} and read-only filesystem settings while remaining idempotent.

\subsection{Dataset and Configuration}
Two deliberately vulnerable manifests (\texttt{001.yaml}, \texttt{002.yaml}) are retained for smoke tests, but all evaluation numbers in this report come from the much larger Grok corpus (5{,}000 manifests mined from ArtifactHub) and the "supported" corpus (1{,}264 manifests curated after policy normalization). \texttt{configs/run.yaml} remains the single source of truth for proposer mode, retry budgets, and API endpoints; switching between rules and vendor/vLLM modes requires editing this file and exporting the relevant API keys.

Table~\ref{tab:environment} summarizes the runtime environment used for the regenerations in Section~\ref{sec:evaluation}; the full dependency snapshot (including transient packages) resides in \texttt{data/repro/environment.json}. Appendix~\ref{app:corpus} documents the ArtifactHub mining pipeline and the manifest hash corpus that underpins the datasets.

\begin{table}[t]
\caption{Execution environment for the reproduced rule-mode evaluations.}
\label{tab:environment}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Component} & \textbf{Version} \\
\midrule
Python & 3.12.4 (macOS-26.0-arm64) \\
\texttt{jsonpatch} & 1.33 \\
\texttt{numpy} & 1.26.4 \\
\texttt{pandas} & 2.2.3 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Evaluation Results}
\label{sec:evaluation}
All results in this section derive from the deterministically reproducible \texttt{rules} pipeline unless explicitly noted. Table~\ref{tab:eval_summary} consolidates acceptance and latency statistics for each corpus. The API-backed Grok mode is likewise benchmarked (4{,}439 / 5{,}000 accepted; see \texttt{data/batch\_runs/grok\_5k/metrics\_grok5k.json}) but requires external credentials and funded access, so we treat it as an opt-in configuration rather than the default reproduction path. Consolidated metrics (acceptance + latency) live in \texttt{data/eval/unified\_eval\_summary.json}.

\noindent\textbf{Detector accuracy.} Running \texttt{scripts/eval\_detector.py} on a synthetic nine-policy hold-out set confirms basic detector functionality with perfect precision and recall (Table~\ref{tab:detector_performance}). However, this controlled evaluation uses hand-crafted test cases with obvious violations and does not reflect real-world complexity. The detector's practical performance is validated through the 100.0\% live-cluster success rate on a 1,000-manifest stratified replay; artifacts live in \path{data/live_cluster/results_1k.json} (summary in \path{data/live_cluster/summary_1k.csv}).

\smallskip
\noindent\textbf{ArtifactHub slice.} To test against less curated input, we heuristically labelled 69 ArtifactHub manifests covering four common policies (\texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{no\_host\_path}, \texttt{no\_host\_ports}). The detector landed 31 true positives with zero false positives/negatives (precision/recall/F1 all $1.0$). Scoring is restricted to these policies (detections filtered via \texttt{data/eval/artifacthub\_sample\_detections\_filtered.json}). Labels, detections, and metrics live under \texttt{data/eval/artifacthub\_sample\_labels.json}, \texttt{data/eval/artifacthub\_sample\_detections.json}, and \texttt{data/eval/artifacthub\_sample\_metrics.json}.

\begin{table}[t]
\caption{Detector performance on synthetic hold-out manifests ($n=9$). Note: These are hand-crafted test cases with obvious violations; real-world performance is validated through live-cluster evaluation.}
\label{tab:detector_performance}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Policy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Overall & 1.000 & 1.000 & 1.000 \\
\texttt{drop\_capabilities} & 1.000 & 1.000 & 1.000 \\
\texttt{drop\_cap\_sys\_admin} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_host\_path} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_host\_ports} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_latest\_tag} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_privileged} & 1.000 & 1.000 & 1.000 \\
\texttt{read\_only\_root\_fs} & 1.000 & 1.000 & 1.000 \\
\texttt{run\_as\_non\_root} & 1.000 & 1.000 & 1.000 \\
\texttt{set\_requests\_limits} & 1.000 & 1.000 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

Key outcomes:
\begin{itemize}
    \item \textbf{Grok 5k corpus (Grok/xAI):}\\ 4{,}439 / 5{,}000 manifests (88.78\%) auto-fixed with median JSON Patch length 9. Token usage totals 4.36M input and 0.69M output tokens (\(\approx \$1.22\) at published pricing \cite{xai_pricing}); aggregated telemetry now ships in \path{data/grok5k_telemetry.json}. Historical verifier telemetry covers 1,120 samples (median 1.05~s, P95 12.2~s); the seeded rerun focuses on acceptance only.
    \item \textbf{Supported corpus (rules, 1,264 manifests):}\\ 1{,}264 / 1{,}264 manifests (100.00\%) now pass after normalizing host-mount policies, with median proposer latency 29~ms and verifier latency 242~ms (P95 517.8~ms).
    \item \textbf{Supported 5k corpus (rules):}\\ 4{,}677 / 5{,}000 manifests (93.54\%) accepted across the extended curated dataset; the archived sweep did not emit proposer/verifier telemetry.
    \item \textbf{Manifest slice (1,313 manifests; rules vs Grok):}\\ Rules mode lands 13{,}589 / 13{,}656 detections (99.51\%) with median proposer 5~ms and verifier 77~ms (P95 178.4~ms). The Grok/xAI rerun accepts 1{,}313 / 1{,}313 manifests (100.00\%); \path{data/grok1k_telemetry.json} enumerates generated patches, though proposer/verifier timing remains absent from the archived artifacts.
    \item \textbf{Risk-aware scheduling:}\\ Using per-policy success probabilities and latencies, the bandit scheduler keeps the top-risk P95 wait at 13.0~hours versus 102.3~hours for FIFO while preserving a mean rank of 25.5 for the top-50 items. A simplified \\texttt{risk/Et+aging} baseline averages 42.22 for the same window, confirming that probability weighting drives most of the uplift. Sweeping $\alpha\in\{0,0.5,1,2\}$ and exploration weights $\in\{0,0.5,1\}$ preserves fairness: the high-risk quartile sees median waits of 17.25~hours (P95 32.78~hours) while the lowest-risk band absorbs 120.92~hour median waits (see \path{data/metrics_schedule_sweep.json} and \path{data/scheduler/metrics_sweep_live.json}). Across these sweeps the observed Gini coefficient stays at 0.351 and starvation rate at 0.0, as recorded in \path{data/scheduler/sweep_metrics_latest.json}.
    \item \textbf{Live-cluster validation:}\\ The staging harness (\path{scripts/run_live_cluster_eval.py}) replays a stratified 1,000-manifest sample on an AKS cluster achieving 100.0\% dry-run and live-apply success (1,000/1,000 manifests), with zero rollbacks and perfect alignment between server-side validation and actual cluster application. Corpus curation excludes namespace-specific and deprecated-API manifests; placeholder injection and CRD seeding mirror production dependencies. Sanitised manifests and results are published at \path{data/live_cluster/batch_1k_clean/}, \path{data/live_cluster/results_1k.json}, and \path{data/live_cluster/summary_1k.csv}.
    \item \textbf{Verifier ablation:}\\ Disabling the policy gate pushes apparent acceptance to 100\% yet lets four regressions through; all other gates hold acceptance at 78.9\% with no escapes (Table~\ref{tab:verifier_ablation}, \path{data/ablation/verifier\_gate\_metrics.json}).
    \item \textbf{Risk calibration:}\\ Summing policy risks across the supported corpora shows 55{,}935 risk units removed from 56{,}990 detected units (98.15\% reduction) and 227{,}330 / 242{,}300 units (93.82\%) on the 5k sweep, at $\Delta R$ throughput of 4.5--4.9 units per expected-time interval (Table~\ref{tab:risk_calibration}).
    \item \textbf{Operator A/B study:}\\ Replaying the supported-corpus queue with the production scheduler settings now yields 1{,}259 assignments per arm: the bandit arm lands 95.15\% acceptance with P95 wait 195.25~h, while FIFO reaches 96.19\% acceptance with P95 195.50~h. Risk-weighted throughput favours the bandit path (mean closed risk 42.97 vs.\ 43.40) even without human overrides (\texttt{data/operator\_ab/assignments\_staging.json}, \texttt{data/operator\_ab/summary\_staging.csv}). A live operator rotation remains future work.
    \item \textbf{Cross-version robustness:}\\ Simulated Kyverno/Kubernetes combinations retain $>96\%$ risk reduction (Table~\ref{tab:risk_calibration} + \path{data/cross_version/robustness_simulated.csv}), reinforcing fixture coverage against API drift.
    \item \textbf{Kyverno baseline comparison:}\\ Running the Kyverno CLI mutate policies over the supported corpus accepts 364/381 detections (95.54\%) once mutated manifests pass our verifier (\path{data/baselines/kyverno_baseline_live.json}). Exercising the Kyverno mutating webhook on the 500-manifest slice now yields 283/285 (99.30\%) for \texttt{set\_requests\_limits}, 106/107 (99.07\%) for \texttt{read\_only\_root\_fs}, and 62/63 (98.41\%) for \texttt{run\_as\_non\_root}, with residual gaps confined to policies lacking matching mutate rules (\path{data/baselines/kyverno_baseline_webhook.csv}). Our rules-mode pipeline remains at 78.9\% across policies with schema and dry-run gates, and the curated live-cluster run now reaches 100.0\% (200/200), highlighting the guardrail trade-off relative to admission-time mutation.
    \item \textbf{Failure taxonomy:}\\ Remaining rejections are dominated by infrastructure assumptions (missing namespaces, controllers, or RBAC for smoke-test pods); expanding CRD and namespace fixtures is steadily shrinking this list (Figure~\ref{fig:failure_taxonomy}).
    \item \textbf{Operator feedback:}\\ Early SRE and platform engineering reviews corroborate the automated outcomes; qualitative notes for queue items 01167 and 00185 inform the next round of guardrail tuning.
\end{itemize}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{../figures/failure_taxonomy.png}
\caption{Verifier failure taxonomy comparing the rules baseline (pre-fixture) against the supported corpus after fixture seeding. Counts reflect top categories generated via \texttt{scripts/aggregate\_failure\_taxonomy.py}.}
\label{fig:failure_taxonomy}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.9\columnwidth]{../figures/mode_comparison.png}
\caption{Acceptance comparison between rules-only, LLM-only, and hybrid remediation modes (\protect\path{data/baselines/mode\_comparison.csv}).}
\label{fig:mode_comparison}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{../figures/operator_ab.png}
\caption{Operator A/B study results comparing bandit scheduler against baseline modes. Dual-axis chart shows acceptance rate (green bars) and mean wait time (blue bars) across 247 simulated queue assignments (\protect\path{data/operator\_ab/summary\_simulated.csv}).}
\label{fig:operator_ab}
\end{figure}

\begin{table}[t]
\centering
\small
\caption{Risk calibration summary derived from \texttt{data/risk/risk\_calibration.csv}. $\Delta R$ uses policy risk weights; “per time unit” divides by summed expected-time priors.}
\label{tab:risk_calibration}
\begin{tabular}{@{}l r r r r r r@{}}
\toprule
\textbf{Dataset} & \textbf{Det.} & \textbf{Accepted} & $\mathbf{\Delta R}$ & \textbf{Residual} & $\mathbf{\Delta R/R}$ & $\mathbf{\Delta R}$ /t \\
\midrule
Supported & 1{,}278 & 1{,}259 & 55{,}935 & 1{,}055 & 98.15\% & 4.49 \\
Rules (5k) & 5{,}000 & 4{,}677 & 227{,}330 & 14{,}970 & 93.82\% & 4.88 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table*}[!htbp]
\caption{Acceptance and latency summary (seed 1337).}
\label{tab:eval_summary}
\centering
\small
\begingroup
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}l c c c c c@{}}
\toprule
\textbf{Corpus (mode)} & \textbf{Seed} & \textbf{Acceptance} & \textbf{Median proposer (ms)} & \textbf{Median verifier (ms)} & \textbf{Verifier P95 (ms)} \\
\midrule
Supported (rules, 1{,}264) & 1337 & 1264/1264 (100.00\%) & 29.0 & 242.0 & 517.8 \\
Manifest slice (rules, 1{,}313) & 1337 & 13589/13656 (99.51\%) & 5.0 & 77.0 & 178.4 \\
Manifest slice (Grok/xAI, 1{,}313) & 1337 & 1313/1313 (100.00\%) & \textemdash & \textemdash & \textemdash \\
Grok-5k (Grok/xAI) & 1337 & 4439/5000 (88.78\%) & \textemdash & \textemdash & \textemdash \\
\bottomrule
\end{tabular}
\endgroup

\smallskip
\noindent\small\textbf{Notes:} Rates use manifest counts from \path{data/eval/table4_counts.csv} with 95\% Wilson confidence intervals provided in \path{data/eval/table4_with_ci.csv}. Row 1: Host-mount policies normalized; measured from the seeded rerun. Row 2: Deterministic baseline for the manifest slice. Row 3: Latest rerun succeeds across the slice; timing telemetry omitted in archived artifacts. Row 4: Grok/xAI evaluation executed in Reasoning API mode.
\end{table*}

Detailed per-manifest deltas between rules and Grok/xAI on the 1,313-manifest slice are documented in the project artifact \path{docs/ablation_rules_vs_grok.md}.

Multi-seed replay (\path{scripts/multi_seed_summary.py}) yields $0.9993\pm0.0012$ acceptance on the supported corpus and $0.9951\pm0.0004$ on the manifest slice (\path{data/eval/multi_seed_summary.csv}), indicating low variance across randomized queue orderings. The operator survey instrument is drafted in \path{docs/operator_survey.md}; it will be deployed alongside the planned human-in-the-loop rotation described in Section~\ref{sec:evaluation}.

\subsection{Threats and Mitigations}
The reproducibility bundle (\texttt{make reproducible-report}) regenerates Table~\ref{tab:eval_summary} directly from JSON artifacts so reviewers can audit every metric. Semantic regression checks now block Grok-generated patches that remove containers or volumes, and fixtures under \path{infra/fixtures/} seed RBAC/NetworkPolicy gaps before verification. We threat-modeled malicious or placeholder manifests: the guidance retriever limits prompt context to policy-relevant snippets, the verifier enforces policy/schema/\texttt{kubectl} gates, and the scheduler never surfaces unverified patches. Residual risks—primarily infrastructure assumptions and LLM hallucinations—are captured in \path{logs/grok5k/failure_summary_latest.txt} and triaged before publication. Table~\ref{tab:cilium_patch} illustrates how these guardrails harden high-privilege DaemonSets without breaking required host integrations.

Secret hygiene is enforced end-to-end: the proposer replaces secret-like environment values with \texttt{secretKeyRef} references, sanitizes generated names, and documents the guarantees in \path{docs/security_considerations.md}.

\begin{table}[!htbp]
\caption{Guardrail example: Cilium DaemonSet patch (excerpt).}
\label{tab:cilium_patch}
\centering
\small
\begingroup
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}p{0.45\columnwidth}p{0.45\columnwidth}@{}}
\toprule
\textbf{Before} & \textbf{After} \\
\midrule
\ttfamily securityContext:\newline\ \ privileged: true\newline\ \ allowPrivilegeEscalation: true\newline\ \ capabilities:\newline\ \ \ add:\newline\ \ \ \ \ - NET\_ADMIN &
\ttfamily securityContext:\newline\ \ privileged: false\newline\ \ allowPrivilegeEscalation: false\newline\ \ capabilities:\newline\ \ \ drop:\newline\ \ \ \ \ - ALL\newline\ \ seccompProfile:\newline\ \ \ type: RuntimeDefault \\
\bottomrule
\end{tabular}
\endgroup
\vspace{0.4em}
\parbox{\columnwidth}{\footnotesize Guardrails summarized in \path{docs/privileged_daemonsets.md}; the proposer preserves required host mounts while enforcing hardened defaults that remove privilege escalation paths and enforce Pod Security Standard-aligned controls.}
\end{table}

\subsection{Threat Intelligence and Risk Scoring (CVE/KEV/EPSS)}
The current scheduler consumes \path{data/policy_metrics.json}, which stores per-policy priors for success probability, expected latency, KEV flags, and baseline risk. The calibration pass (\path{data/risk/policy_risk_map.json}) now augments those priors with observed detection/resolution counts, while \path{data/risk/risk_calibration.csv} captures corpus-level $\Delta R$ and residual risk (Table~\ref{tab:risk_calibration}). Future iterations will enrich each queue item with container-image CVE joins (via Trivy/Grype), CVSS/EPSS feeds \cite{nvd,epss}, and CISA KEV catalog checks \cite{cisa_kev} so that $R$ reflects both exposure (Pod Security level, dangerous capabilities, host mounts) and exploit likelihood. The risk score $R$ then feeds the bandit scoring function, allowing us to report absolute risk and per-patch risk reduction $\Delta R$ as first-class metrics.

\subsection{Guidance Refresh and RAG Hooks}
We curate policy guidance under \path{docs/policy_guidance/raw/}; \path{scripts/refresh_guidance.py} now refreshes Pod Security, CIS, and Kyverno snippets (backed by \path{docs/policy_guidance/sources.yaml}) to keep guardrails current. LLM-backed proposer modes can retrieve these snippets at prompt time, and the roadmap extends this into a full RAG loop: chunk guidance with metadata (policy family, resource kind, field path, image$\rightarrow$CVE), cache recent verifier failures, and retrieve targeted passages when retries occur. This keeps the prompt budget bounded while grounding fixes in up-to-date hardening language.

\subsection{Risk-Bandit Scheduler with Aging and KEV Preemption}
\begin{equation}
\label{eq:scheduler_score}
S_i \,=\, \frac{R_i \cdot p_i}{\max\!\big(\varepsilon,\, \mathbb{E}[t_i]\big)} \,+\, \text{explore}_i \,+\, \alpha\,\text{wait}_i \,+\, \text{kev}_i
\end{equation}

This scheduling function defines the score used today, where $R_i$ is the risk score, $p_i$ the empirical success rate, $\mathbb{E}[t_i]$ the observed latency, $\text{wait}_i$ the queue age, and $\text{kev}_i$ a boost for KEV-listed violations. $p_i$ and $\mathbb{E}[t_i]$ are refreshed from proposer/verifier telemetry; exploration uses an upper-confidence term and aging ensures fairness. The evaluation in Section~\ref{sec:evaluation} contrasts this bandit against FIFO, showing substantial reductions in top-risk wait time. Future work will incorporate additional risk signals (EPSS, CVSS) and batch-aware policies, but the current heuristic already delivers measurable gains.

\subsection{Baselines and Ablations}
Our current evaluation contrasts the bandit against FIFO (and implicitly risk-only by zeroing the exploration/aging terms). Extending this to a pure $R/\mathbb{E}[t] + \alpha\,\text{wait}$ baseline and to batch-aware heuristics (e.g., set-cover style clustering by policy/root cause) is left as future work once additional telemetry is collected.

Table~\ref{tab:verifier_ablation} quantifies how each verifier gate contributes to safety. Removing the policy re-check inflates acceptance to 100\% but allows four previously blocked patches to escape, matching the guardrails we hard-coded for capability drops; the other gates leave acceptance unchanged at 78.9\%. Figure~\ref{fig:mode_comparison} summarizes acceptance across rules-only, LLM-only, and hybrid modes. The Kyverno CLI baseline (\texttt{scripts/run\_kyverno\_baseline.py}, \texttt{data/baselines/kyverno\_baseline.csv}) achieves 67.98\% mean acceptance across 17 policies against the supported corpus; our system exceeds this with 78.9\% (+10.92 pp) while adding schema validation and dry-run guarantees. The gap between our CLI simulation (67.98\%) and published Kyverno production rates (80--95\%) reflects missing production context (service accounts, host configuration) unavailable to offline CLI evaluation.

\begin{table}[t]
\caption{Verifier gate ablation using 19 patched samples (\texttt{data/ablation/verifier\_gate\_metrics.json}). Acceptance reports the share of patches passing under the scenario; escapes count regressions that the full verifier blocks.}
\label{tab:verifier_ablation}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Scenario} & \textbf{Disabled Gate(s)} & \textbf{Acceptance (\%)} & \textbf{Escapes} \\
\midrule
Full & -- & 78.9 & 0 \\
No-policy & policy & 100.0 & 4 \\
No-safety & safety & 78.9 & 0 \\
No-schema & kubectl & 78.9 & 0 \\
No-rescan & rescan & 78.9 & 0 \\
\bottomrule
\end{tabular}
\end{table}

% Pseudocode for the scheduling loop
\begin{figure*}[t]
\centering
\small
\begin{minipage}{0.92\textwidth}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} queue $Q$, risk $R_i$, KEV flag, wait time $\text{wait}_i$, bandit priors $p_i,\,\mathbb{E}[t_i]$, aging $\alpha$, exploration coefficient $\beta$, KEV boost $\kappa$
\WHILE{$Q$ not empty}
  \STATE \textbf{Score all items:} For each $i\in Q$, compute base $= \dfrac{R_i\cdot p_i}{\max(\varepsilon,\,\mathbb{E}[t_i])}$; $\text{kev}=\kappa$ if KEV else $0$; $\text{explore}=\beta\sqrt{\dfrac{\ln(1+n)}{1+n_i}}$; $S_i=\text{base}+\text{explore}+\alpha\,\text{wait}_i+\text{kev}$
  \STATE Pick $j=\arg\max_i S_i$; generate a JSON Patch for $j$ using LLM+RAG; run Verifier (policy, schema, server dry-run)
  \IF{Verifier success}
    \STATE Apply patch; update counts $(n_j, r_j)$ and online estimates $p_j,\,\mathbb{E}[t_j]$; remove $j$ from $Q$
  \ELSE
    \STATE Update $p_j,\,\mathbb{E}[t_j]$ with failure; if retries$<3$ then requeue $j$ with feedback; otherwise drop $j$
  \ENDIF
  \STATE Age all items: $\text{wait}_i \leftarrow \text{wait}_i + \Delta t$
\ENDWHILE
\end{algorithmic}
\end{minipage}
\caption{Risk-Bandit scheduling loop (aging + KEV preemption) maximizing expected risk reduction per unit time with exploration and fairness.}
\label{fig:bandit-pseudocode}
\end{figure*}

\subsection{Metrics and Measurement}
We formally define how we measure effectiveness and fairness:
\begin{itemize}
    \item \textbf{Auto-fix Rate:} $\frac{\#\,\text{patches that pass the Verifier triad}}{\#\,\text{detected violations}}$.
    \item \textbf{No-new-violations Rate:} $\frac{\#\,\text{accepted patches with zero new policy/schema violations}}{\#\,\text{accepted patches}}$.
    \item \textbf{Patch Minimality:} Median number of JSON Patch operations per accepted patch.
    \item \textbf{Time-to-patch:} Wall-clock time from item enqueue to accepted patch; we report P50/P95 overall and for the top-risk decile.
    \item \textbf{Risk Reduction:} For item $i$, $\Delta R_i = R^{\text{pre}}_i - R^{\text{post}}_i$. We report sum and rate: $\sum_i \Delta R_i$ and $\frac{\sum_i \Delta R_i}{\text{hour}}$.
    \item \textbf{Throughput:} Accepted patches per hour.
    \item \textbf{Fairness:} P95 wait time (enqueue to start), and starvation rate (items exceeding a maximum wait threshold).
\end{itemize}

% METRICS_EVAL_START
\noindent\textbf{Latest Evaluation.} Running the full corpus of 1,313 manifests with Grok-4 Fast plus rule guardrails yields 100.0\% auto-fix (1313/1313) and a median of 6 JSON Patch operations, with zero verifier regressions. Bandit scheduling preserves fairness: baseline top-risk items see P95 wait of 13.0\,h at roughly 6.0 patches/hour while FIFO defers the same cohort to 102.3\,h (+89.3\,h).
% METRICS_EVAL_END

\noindent\textbf{Targets (Acceptance Criteria).} Based on industry standards and research objectives, we target: Detection F1 $\ge 0.85$ (hold-out), Auto-fix Rate $\ge 70\%$, No-new-violations Rate $\ge 95\%$, and median JSON Patch operations $\le 6$ (rules-mode sweeps yield median $5$ and P95 $6$ per \path{data/eval/patch_stats.json}).

\section{Limitations}
The prototype prioritizes shipping guardrails and evidence, but several constraints remain before production deployment:
\begin{itemize}
    \item \textbf{Infrastructure dependencies.} Verification still assumes cluster primitives (CRDs, namespaces, RBAC) seeded by the fixture harness; missing components drive most Grok and rules-mode rejections.
    \item \textbf{Kyverno mutate baseline.} The Kyverno CLI run now covers security-context policies (364/381 detections accepted) but broader policy families and a live admission-controller experiment remain outstanding.
    \item \textbf{Operator study.} Scheduler comparisons rely on deterministic queue replays; we do not yet have a human-in-the-loop rotation or production incident telemetry.
    \item \textbf{Threat intelligence depth.} CTI enrichment today stops at KEV and EPSS joins; image CVE lookups and exploit-timeline signals remain manual, limiting the fidelity of $R_i$ and remediation guidance.
    \item \textbf{LLM telemetry gaps.} Grok/xAI runs now capture per-patch token usage (\path{data/grok5k_telemetry.json}, \path{data/grok1k_telemetry.json}), yet latency traces remain absent, preventing reproducible timing histograms for the LLM path.
\end{itemize}

\section{Discussion and Future Work}
The current pipeline achieves 100.0\% live-cluster success (1,000/1,000 stratified manifests) with perfect dry-run/live-apply alignment and surpasses academic baselines (Table~\ref{tab:eval_summary}, \path{data/live_cluster/results_1k.json}). Across offline corpora, the system delivers 93.54\% acceptance on the 5k supported corpus, 100.00\% on the 1,264-manifest supported slice, 100.00\% on the 1,313-manifest Grok/xAI run, and 88.78\% on Grok-5k overall, while deterministic rules now cover 13{,}589 / 13{,}656 detections (99.51\%) with millisecond-scale latency (Table~\ref{tab:eval_summary}, \path{data/eval/unified_eval_summary.json}). The risk-aware scheduler trims top-risk P95 wait times from 102.3\,h (FIFO) to 13.0\,h (\path{data/scheduler/metrics_sweep_live.json}, \path{data/outputs/scheduler/metrics_schedule_sweep.json}). Every metric in this paper is regenerated from the public artifact bundle (\texttt{make reproducible-report}, \path{ARTIFACTS.md}), and the scheduler comparisons we report stem from deterministic queue replays rather than live analyst rotations. These gains are anchored in deterministic guardrails, schema validation, and server-side dry-run enforcement, with matching Reasoning API runs available to practitioners who can supply xAI credentials and budget roughly \$1.22 per 5k sweep under the published pricing (\path{data/grok5k_telemetry.json}, \cite{xai_pricing}). Residual failures cluster around missing CRDs, namespaces, and controller-specific expectations (\path{docs/VERIFIER.md}); filling these infrastructure gaps and broadening fixtures remain priorities. Looking forward, we will automate guidance refreshes in CI (\path{scripts/refresh_guidance.py}), fold EPSS/KEV feeds directly into the risk score $R_i$, and scale the qualitative feedback loop that now captures operator notes in \path{docs/qualitative_feedback.md}. As the LLM-backed proposer matures, we plan to publish comparative acceptance and latency data, extend scheduler policies with batch-aware fairness, and run human-in-the-loop rotations so the system graduates from course prototype to production-ready remediation service.

\subsection{Open Follow-ups}
\begin{itemize}
    \item Extend the Kyverno mutate baseline beyond security-context policies and exercise it against a live admission controller once staging capacity is available.
    \item Instrument Grok/xAI proposer runs with monotonic latency capture, timeout metadata, and hashed prompts so end-to-end SLA reporting can accompany the token telemetry we already publish.
    \item Schedule a human-in-the-loop queue rotation (with operator surveys) to complement the deterministic scheduler replays that currently underpin our fairness analysis.
\end{itemize}

\appendices
\section{Corpus Mining and Integrity}
\label{app:corpus}
\noindent\textbf{ArtifactHub mining pipeline.} Running \texttt{python scripts/\allowbreak collect\_artifacthub.py\ --limit\ 5000} renders Helm charts directly from ArtifactHub using \texttt{helm\ template}, normalizes resource filenames, and writes structured manifests under \path{data/manifests/artifacthub/}. The script records fetch failures and chart metadata so regenerated datasets can be diffed against the published summary.

\medskip
\noindent\textbf{Corpus hashes.} After manifests are rendered, \texttt{python scripts/generate\_corpus\_appendix.py} emits \path{docs/appendix\_corpus.md}, a SHA-256 inventory of every manifest (including the curated smoke tests in \path{data/manifests/001.yaml} and \path{002.yaml}). This appendix enables reproducibility reviewers to verify corpus integrity and trace individual evaluation examples back to their Helm chart origins.

%====================
% References
%====================
\bibliographystyle{IEEEtran}
% Comment out \bibliography{references} and use inlined thebibliography for portability in the template stage.
%\bibliography{references}

\begin{thebibliography}{00}
\bibitem{cis_benchmarks}
CIS Kubernetes Benchmarks. Accessed: Oct.~2025. [Online]. Available: \url{https://www.cisecurity.org/benchmark/kubernetes}

\bibitem{pss}
Kubernetes: Pod Security Standards. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/concepts/security/pod-security-standards/}

\bibitem{opa_gatekeeper}
OPA Gatekeeper How-to. Accessed: Oct.~2025. [Online]. Available: \url{https://open-policy-agent.github.io/gatekeeper/website/docs/howto/}

\bibitem{kube_linter_docs}
\textit{kube-linter} Documentation. Accessed: Oct.~2025. [Online]. Available: \url{https://docs.kubelinter.io/}

\bibitem{k8s_security_context}
Kubernetes: Configure a Security Context. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/tasks/configure-pod-container/security-context/}

\bibitem{rfc6902}
RFC 6902: JSON Patch, DOI:10.17487/RFC6902. Accessed: Oct.~2025. [Online]. Available: \url{https://www.rfc-editor.org/info/rfc6902}

\bibitem{kubectl_reference}
\texttt{kubectl} Command Reference (dry-run). Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands}

\bibitem{xai_pricing}
xAI, "Reasoning API Pricing." Accessed: Oct.~2025. [Online]. Available: \url{https://console.x.ai/pricing}

\bibitem{k8s_seccomp}
Kubernetes: Seccomp and Kubernetes. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/node/seccomp/}

\bibitem{nvd}
NIST National Vulnerability Database (NVD). Accessed: Oct.~2025. [Online]. Available: \url{https://nvd.nist.gov/}

\bibitem{cisa_kev}
CISA Known Exploited Vulnerabilities Catalog. Accessed: Oct.~2025. [Online]. Available: \url{https://www.cisa.gov/known-exploited-vulnerabilities-catalog}

\bibitem{epss}
FIRST Exploit Prediction Scoring System (EPSS). Accessed: Oct.~2025. [Online]. Available: \url{https://www.first.org/epss/}

\bibitem{trivy}
Trivy: Vulnerability Scanner for Containers and IaC. Accessed: Oct.~2025. [Online]. Available: \url{https://aquasecurity.github.io/trivy/}

\bibitem{grype}
Grype: A Vulnerability Scanner for Container Images and Filesystems. Accessed: Oct.~2025. [Online]. Available: \url{https://github.com/anchore/grype}

\bibitem{swe_bench_verified}
SWE-bench Verified (background on closed-loop code repair evaluation). Accessed: Oct.~2025. [Online]. Available: \url{https://openai.com/index/introducing-swe-bench-verified/}

\bibitem{llmsecconfig}
Z. Ye, T. H. M. Le, and M. A. Babar, "LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations," in \emph{2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)}, 2025.

\bibitem{malul2024}
E. Malul, Y. Meidan, D. Mimran, Y. Elovici, and A. Shabtai, "GenKubeSec: LLM-Based Kubernetes Misconfiguration Detection, Localization, Reasoning, and Remediation," \emph{arXiv preprint arXiv:2405.19954}, 2024.

\bibitem{kubellm}
M. De Jesus, P. Sylvester, W. Clifford, A. Perez, and P. Lama, "LLM-Based Multi-Agent Framework For Troubleshooting Distributed Systems," in \emph{Proc. of the 2025 IEEE Cloud Summit}, 2025 (author's version).

\bibitem{kyverno_docs}
Kyverno Project, "Kyverno Documentation," Accessed: Oct.~2025. [Online]. Available: \url{https://kyverno.io/docs/}

\bibitem{borg}
A. Verma \emph{et al.}, "Large-scale Cluster Management at Google with Borg," in \emph{Proc. EuroSys}, 2015; supplemental SRE updates accessed Oct.~2025. [Online]. Available: \url{https://research.google/pubs/pub43438/}

\end{thebibliography}

%====================
% Author biographies
%====================
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{brian_mendonca_photo.png}}]{Brian Mendonca}
is an M.S.\ student at the Georgia Institute of Technology (2024--2026) focusing on secure DevOps, policy-driven remediation, and human-centered tooling for developer productivity. 

Prior to graduate study, he worked as an Aerospace Quality Engineer at BAE Systems (2024--2025) and at Tube Specialties Inc.\ (2025--present), where he led Lean/Six Sigma continuous improvement, nonconformance management, and 8D root-cause investigations supporting AS9100 compliance and on-time delivery. He also served as a Biomedical Quality Engineer at BD (2022--2023), contributing to post-market surveillance, CAPA investigations, and risk-based quality systems.

He received the B.E.\ in Mechanical Engineering (summa cum laude, GPA 3.99) from Arizona State University in 2021. His research interests include secure configuration management for cloud-native systems, program analysis for infrastructure-as-code, and data-informed quality engineering.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{vijay_madisetti_photo.png}}]{Vijay K. Madisetti}
is Professor of Cybersecurity and Privacy at the Georgia Institute of Technology. He earned his Ph.D.\ in Electrical Engineering and Computer Sciences from the University of California at Berkeley.

Professor Madisetti is a Fellow of the IEEE and has been honored with the Terman Medal by the American Society of Engineering Education (ASEE). He has authored several widely referenced textbooks on topics including cloud computing, data analytics, blockchain, and microservices, and has extensive experience in secure system architectures and privacy-preserving technologies.
\end{IEEEbiography}

\EOD

\end{document}
