\PassOptionsToPackage{table}{xcolor}
\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
% \usepackage{caption} % Removed: Conflicts with the ieeeaccess class
% \usepackage{mathptmx} % Disabled to avoid RSFS font requirement on minimal TeX
\usepackage[english]{babel}
% Enable microtype gently to reduce overfull boxes (protrusion/expansion)
\IfFileExists{microtype.sty}{\usepackage[final]{microtype}}{}
% Packages added for the comparison table
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{tabularx} % For auto-wrapping text in tables
\usepackage{multirow}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{alltt}

% xurl may not be present in minimal TeX installs; fall back to url
\IfFileExists{xurl.sty}{\usepackage{xurl}}{\usepackage{url}}
\usepackage[hidelinks]{hyperref}

\usepackage{bm}
\usepackage{courier}

% Silence missing Courier normal-shape warning by aliasing to medium weight
\AtBeginDocument{%
  \makeatletter
  \input{t1pcr.fd}%
  \makeatother
  \DeclareFontShape{T1}{pcr}{n}{n}{<->ssub*pcr/m/n}{}%
}

% Listings-based code block with line breaking and tight margins
\lstdefinestyle{codestyle}{%
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  breakatwhitespace=false,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  frame=single,
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=0.5\baselineskip,
  belowskip=0.5\baselineskip
}
\lstnewenvironment{codeblock}[1][]{\lstset{style=codestyle,#1}}{}

\makeatletter
% Allow URL/path breaks at underscore, dot, and slash to prevent overfull lines
\appto\UrlBreaks{\do\_\do\.\do\/}
\makeatother

% Fallback mapping for RSFS font to Computer Modern symbols (avoids rsfs10)
\makeatletter
\DeclareFontFamily{U}{rsfs}{}
\DeclareFontShape{U}{rsfs}{m}{n}{<-6> s*[1.05] cmsy5 <6-8> s*[1.05] cmsy7 <8-> s*[1.05] cmsy10}{}
\DeclareSymbolFont{rsfs}{U}{rsfs}{m}{n}
\DeclareSymbolFontAlphabet{\mathscr}{rsfs}
\makeatother

% Ensure local Type 1 font maps (bundled with template) are loaded
\pdfmapfile{+t1-formata.map}
\pdfmapfile{+t1-times.map}
\pdfmapfile{+t1-helvetica.map}
\pdfmapfile{+t1-giovannistd.map}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%Your document starts from here ___________________________________________________

\begin{document}
\history{}
\doi{DOI: TBD}

% (Revert to default IEEE Access title page styling)
\title{Closed-Loop Threat-Guided Auto-Fixing of Kubernetes YAML Security Misconfigurations}
\author{\uppercase{Brian Mendonca}\authorrefmark{1}, and
\uppercase{Vijay K. Madisetti}\authorrefmark{2}, \IEEEmembership{Fellow, IEEE}}

\address[1]{College of Computing, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: brian.mendonca6@gmail.com)}
\address[2]{School of Cybersecurity and Privacy, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: vkm@gatech.edu)}

% Short running heads to avoid header overflow
\markboth
{Mendonca et al.: k8s-auto-fix}
{Mendonca et al.: k8s-auto-fix}

\corresp{Corresponding author: Dr. Vijay Madisetti (e-mail: vkm@gatech.edu).}

\tfootnote{The authors thank the ArtifactHub maintainers for curating the public Helm corpus, the CNCF SIG-Security reviewers for early feedback, and xAI for providing Reasoning API credits that made the Grok evaluations possible.}

\titlepgskip=-22pt

% Abstract and keywords must be defined before \maketitle for ieeeaccess
\begin{abstract}
Misconfigured Kubernetes manifests expand blast radius when pipelines stop at detection. We present \texttt{k8s-auto-fix}, a closed loop (\emph{Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler}) that addresses this gap. Our key results demonstrate the effectiveness of this approach: a 1,000-manifest live-cluster replay achieves 100\% success (1,000/1,000); deterministic rules cover 99.51\% of violations; an optional LLM backend reaches 88.52\% acceptance on the 5,000-manifest Grok corpus; and a risk-aware scheduler cuts top-risk P95 wait time by 7.9$\times$. We release all artifacts for full reproducibility.
\end{abstract}

\begin{keywords}
Kubernetes, SAST, DAST, Admission control, Server-side dry-run, YAML, Pod Security, JSON Patch, Policy Enforcement, Kyverno, OPA Gatekeeper, Auto-fix, CI/CD, CVE, EPSS, RAG, Risk-based scheduling
\end{keywords}

\maketitle

% Gentle line-breaking stretch to reduce overfull boxes
\sloppy
\emergencystretch=3em

\section{Importance of the Problem}
Kubernetes YAML is easy to get wrong: a single \texttt{privileged: true}, a \texttt{:latest} image tag, or a missing \texttt{runAsNonRoot} can expand blast radius and undermine defense-in-depth. Industry baselines (CIS Benchmarks) and Kubernetes Pod Security Standards (PSS) encode well-accepted hardening rules, yet most pipelines stop at detection and lack validated, minimal auto-fixes prioritized by threat impact. This project targets that gap with measured improvements on Auto-fix rate, No-new-violations\%, Time-to-patch, and \emph{risk reduction} (with fairness) on a held-out corpus—directly aligned with industry standards and research objectives (\cite{cis_benchmarks}, \cite{pss}).

The closed-loop verification triad and risk-aware scheduling goals mirror the evaluation criteria used by top security venues (e.g., IEEE S\&P, USENIX Security, NDSS). Our approach provides demonstrable risk reduction, strong guardrails against regressions, and operator-in-the-loop evidence. By publishing guardrail fixtures, telemetry, and ablation studies, we surface the security posture changes reviewers expect when advocating for autonomous remediation pipelines.

\smallskip
\noindent\textbf{Contributions.} We make the following contributions:
\begin{itemize}
    \item A closed-loop auto-fix pipeline with triad guardrails (policy re-check, schema validation, server-side dry-run) that achieves 100\% success on a 1,000-manifest live-cluster replay (Section~\ref{sec:evaluation}).
    \item A risk-aware scheduler that reduces top-risk P95 wait time by 7.9$\times$ while preserving fairness (Section~\ref{sec:evaluation}).
    \item A comprehensive set of reproducible artifacts, including scripts, telemetry, and audit logs, that allow for the complete regeneration of all tables and figures in this paper (\url{ARTIFACTS.md}).
\end{itemize}

%====================
% Comparison Table (Updated)
%====================
\begin{table*}[t!]
\centering
\small
\caption{Comparison of automated Kubernetes remediation systems (Oct.~2025 snapshot).}
\label{tab:comparison}
\begin{tabularx}{\textwidth}{@{}l >{\raggedright\arraybackslash}p{2.5in} >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X@{}}
\toprule
\textbf{Capability} & \textbf{k8s-auto-fix (this work)} & \textbf{GenKubeSec \cite{malul2024}} & \textbf{Kyverno \cite{kyverno_docs}} & \textbf{Borg/SRE \cite{borg}} \\
\midrule
\textbf{Primary Goal} & Closed-loop hardening (detect$\rightarrow$patch$\rightarrow$verify$\rightarrow$prioritize) & LLM-based detection/remediation suggestions & Admission-time policy enforcement & Large-scale auto-remediation in production clusters \\
\midrule
\textbf{Fix Mode} & JSON Patch (rules + optional LLM) & LLM-generated YAML edits & Policy mutation/generation & Custom controllers and playbooks \\
\textbf{Guardrails} & Policy re-check + schema + \texttt{kubectl apply --dry-run=server} + privileged/secret sanitization + CRD seeding & Manual review; no automated gates & Validation/mutation webhooks; assumes controllers & Health checks, automated rollback, throttling \\
\textbf{Risk Prioritization} & Bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) & Not implemented & FIFO admission queue & Priority queues / toil budgets \\
<<<<<<< HEAD
\textbf{Evaluation Corpus} & 1{,}000 live-cluster manifests (100.0\% success); 5{,}000 Grok manifests (88.52\%); 1{,}264 supported manifests (100.00\% rules); 1{,}313 manifest slice (99.51\% rules / 100.00\% Grok) & 200 curated manifests (85--92\% accuracy) & Thousands of user manifests (80--95\% mutation acceptance) & Millions of production workloads (no public acceptance \%) \\
=======
\textbf{Evaluation Corpus} & 200 live-cluster manifests (100.0\% success); 5{,}000 Grok manifests (88.78\%); 1{,}264 supported manifests (100.00\% rules); 1{,}313 manifest slice (99.51\% rules / 100.00\% Grok) & 200 curated manifests (85--92\% accuracy) & Thousands of user manifests (80--95\% mutation acceptance) & Millions of production workloads (no public acceptance \%) \\
>>>>>>> 692e9db1e77fd6670eabbe6fd1ebe52b8b057604
\textbf{Telemetry} & Policy-level success probabilities, latency histograms, failure taxonomy & Token/cost estimates; no pipeline telemetry & Admission latency $<45$~ms, violation counts & MTTR, incident counts, operator feedback \\
\textbf{Outstanding Gaps} & Infrastructure-dependent rejects, operator study, scheduled guidance refresh in CI & Automated guardrails, risk-aware ordering & LLM-aware patching, risk-aware scheduling & Declarative manifest fixes, static analysis integration \\
\bottomrule
\end{tabularx}
\end{table*}

% Baseline head-to-head table (auto-generated from artifacts)
\begin{table*}[t!]
\centering
\small
\caption{Head-to-head policy-level acceptance on the 500-manifest security-context slice. Counts and rates regenerate from \url{data/detections.json}, \url{data/verified.json}, and baseline CSVs under \url{data/baselines/}.}
\label{tab:baselines}
\input{../docs/reproducibility/baselines.tex}
\end{table*}

% Live-cluster per-policy (strict 500) summary (auto-generated)
%\begin{table*}[t!]
%\centering
%\small
%\caption{Live-cluster per-policy outcomes on a strict 500-slice with server-side dry-run enforced.}
%\label{tab:live_per_policy}
%\input{../docs/reproducibility/live_per_policy.tex}
%\end{table*}

\smallskip
\noindent\textbf{Metric caveat.} Table~\ref{tab:comparison} aggregates metrics reported by prior work that span admission latency, MTTR, and acceptance rates, so values are not strictly comparable; they provide qualitative context only.

Table~\ref{tab:baselines} grounds those qualitative differences with the head-to-head slice we share publicly. On the 500-manifest security-context corpus, \texttt{k8s-auto-fix} lands 33--100\% acceptance across the high-risk policies it actively targets (privilege, capabilities, read-only root filesystem, requests/limits); the lone unsupported rule, \texttt{no\_host\_ports}, remains at 0\% because we do not attempt that mutation today. Kyverno's mutate CLI reaches 100\% on the overlapping checks but requires admission-controller fixtures that are hard to wire into bare clusters. Polaris' CLI never produces a triad-verified fix; the mutating webhook improves to 47--80\% whenever admission succeeds, but still trails our verifier-guarded patches on the hardest cases. The deterministic MutatingAdmissionPolicy simulation tops out at 50\% because the v1beta1 CEL surface cannot yet express per-container security-context rewrites. Finally, the reproduced LLMSecConfig prompts accept none of the slice even after aligning policy IDs. These results underscore our claim that safe auto-remediation demands more than mutate hooks alone (\textbf{cf. Table~\ref{tab:baselines}}): the triad prevents regressions, but fixture drift and policy coverage still bound acceptance. For an admission controller like Kyverno to achieve high acceptance rates (>98\%), the cluster must be pre-seeded with a variety of fixtures that satisfy the dependencies of the incoming manifests \cite{kyverno_docs}. These commonly include:
\begin{itemize}
    \item Namespaces
    \item ServiceAccounts
    \item Custom Resource Definitions (CRDs)
    \item Secrets and ConfigMaps
    \item PersistentVolumeClaims and StorageClasses
\end{itemize}
Without these fixtures, manifests are rejected by the API server before the mutation webhook can even process them \cite{kyverno_docs}. Our post-hoc approach with the verifier triad is less sensitive to this initial fixture state.
\section{Related Work}
Recent work has explored LLM prompts (GenKubeSec \cite{malul2024}), admission policy engines (Kyverno \cite{kyverno_docs}), and large-scale SRE playbooks (Borg \cite{borg}) for Kubernetes remediation, yet critical gaps remain for a production-ready, automated system. GenKubeSec localizes and suggests fixes but leaves validation to humans, lacking schema/dry-run guardrails. Kyverno mutates manifests at admission-time but does not prioritize fixes or auto-seed third-party CRDs. Borg-style automation excels at infrastructure remediation yet is not openly available for manifest-level hardening. Table~\ref{tab:comparison} situates our closed-loop pipeline relative to these efforts, combining automated patching, triad verification, and risk-aware scheduling with published acceptance metrics on multi-thousand manifest corpora.

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{How we differ.} Our work's novelty lies in the \emph{triad} of guardrails (policy re-check, schema validation, server-side dry-run) combined with a risk-aware, learning-based scheduler. Unlike GenKubeSec/LLMSecConfig, which focus on LLM-based patch generation, we provide deterministic rules as a default and treat LLMs as optional backends under the same rigorous verification. Unlike Kyverno, which operates at admission time, our system processes existing manifests and prioritizes remediation based on risk, not just FIFO order. The public artifacts and reproducible queue replays further distinguish our work by enabling verifiable performance claims.
\end{minipage}%
}
\smallskip

\noindent\textbf{SAST vs. DAST Positioning.} Most Kubernetes security tools operate as \emph{Static Application Security Testing} (SAST), analyzing manifests without executing them against a live cluster (\texttt{kube-linter}, policy engines' CLI modes). In contrast, our verifier's server-side dry-run (\texttt{kubectl apply --dry-run=server}) constitutes \emph{Dynamic Application Security Testing} (DAST): it submits each patched manifest to a live Kubernetes API server, exercising admission controllers, webhook validators, and RBAC policies to confirm the cluster would accept the change \cite{kubectl_reference}. This DAST gate catches infrastructure-specific rejections that pure SAST misses—such as missing CRDs, namespace conflicts, or quota violations—and is validated by our 100\% success rate (1,000/1,000) on the live-cluster AKS replay. The triad's combination of SAST (policy re-check, schema validation) and DAST (server dry-run) provides defense-in-depth, ensuring patches satisfy both policy intent and runtime constraints.

\smallskip

A key limitation of existing approaches is their focus on either detection or admission control, without a corresponding emphasis on automated, validated remediation. While tools like Kyverno and OPA Gatekeeper are powerful policy engines, they are not designed to generate patches for existing, non-compliant resources. This leaves a critical gap in the DevOps lifecycle, where developers are often left to manually remediate misconfigurations, leading to delays and inconsistencies. Our work directly addresses this gap by providing a closed-loop system that not only detects misconfigurations but also proposes, verifies, and schedules validated patches, thereby reducing the manual effort required to maintain a secure Kubernetes environment.

\smallskip
\noindent\textbf{1. Detection-Only Pipelines.} Static analysis tools like \texttt{kube-linter} and policy engines such as Kyverno and OPA Gatekeeper excel at identifying misconfigurations (\cite{kube_linter_docs}, \cite{kyverno_docs}, \cite{opa_gatekeeper}). However, their core function is detection and admission control, not the generation of validated, minimal patches. Our work uses these powerful tools as the \emph{Detector} and \emph{Verifier} components in a broader remediation workflow.

\smallskip
\noindent\textbf{2. Lack of Closed-Loop Verification.} Few remediation pipelines enforce a rigorous, multi-gate verification process. A key novelty of our approach is the Verifier's triad of checks: a policy re-check to confirm the original violation is gone, schema validation to ensure correctness, and a server-side dry-run (\texttt{kubectl apply --dry-run=server}) to simulate the application of the patch against the Kubernetes API server, ensuring no new violations are introduced (\cite{kubectl_reference}).

\smallskip
\noindent\textbf{3. Inefficient Prioritization.} Security work queues are often processed in a First-In, First-Out (FIFO) manner. This can leave high-impact vulnerabilities unpatched while the system works on lower-priority issues. We propose and test a \textbf{risk-based, learning-aware scheduler} that integrates CVE/CTI signals (CVSS, EPSS, KEV) and online outcomes (verifier pass/fail) using a contextual bandit with aging and KEV preemption, aiming to maximize risk reduction while preserving fairness.

\section{System Design}
\label{sec:system-design}
We realize the closed loop \emph{Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler} shown in Figure~\ref{fig:architecture}. Detectors produce structured JSON findings; the proposer applies rule-based guards (with optional LLM backends) to emit minimal JSON Patches; the verifier enforces policy re-checks, schema validation, and \texttt{kubectl apply --dry-run=server}; and the scheduler orders work using risk-aware bandit scoring. Each stage persists artifacts (detections, patches, verified outcomes, queue scores), enabling reproducible evaluation (Section~\ref{sec:evaluation}).

\subsection{Notation}
\label{sec:notation}
We use the following notation throughout the paper: $R_i$ is the risk score for queue item $i$, $p_i$ is the empirical verifier success probability for that policy, $\mathbb{E}[t_i]$ is the observed proposer+verifier latency, $\text{wait}_i$ is the accumulated queue age, and $\text{kev}_i$ is the KEV-derived boost when the detection maps to a CISA advisory. Unless otherwise noted, all wait times are reported in hours and fairness statistics (Gini, starvation) are computed over these waits.

\smallskip
\noindent\textbf{Disagreement and Budgets.} When kube-linter and Kyverno/OPA disagree we take the \emph{union} of violations at detection time, and require patches to satisfy both engines during verification. Attempts are capped at three per manifest; per-attempt latency and success outcomes feed into \texttt{data/policy\_metrics.json}, which the scheduler consumes alongside KEV flags.

\subsection{End-to-End Walkthrough on Real Manifests}
To make the closed-loop pipeline concrete, we trace two real-world manifests from the repository's test suite through each stage, from detection to scheduling. The goal is to demonstrate safe, automated remediation with full reproducibility and verifiable risk reduction.

\smallskip
\noindent\textbf{Case 1: Remediating a Privileged Pod with a \texttt{:latest} Image Tag}

This example, drawn from \url{data/manifests/001.yaml}, shows a common but high-risk pattern: a privileged container using a floating tag.

\begin{codeblock}
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: app
    image: acme/api:latest
    securityContext:
      privileged: true
      allowPrivilegeEscalation: true
      capabilities: { add: ["SYS_ADMIN", "NET_ADMIN"] }
\end{codeblock}

\noindent\textit{1. Detect} (Union): The detector consumes this manifest and reports four policy violations: \texttt{no\_privileged}, \texttt{drop\_capabilities}, \texttt{run\_as\_non\_root}, and \texttt{no\_latest\_tag}. These correspond to the structured output in \url{data/detections.json}.

\noindent\textit{2. Propose} (Rules Engine): The proposer's rules engine consumes the detection report and generates a minimal, idempotent JSON Patch designed to fix all identified violations. The resulting patch, written to \url{data/patches.json}, is as follows:
\begin{codeblock}
[
 {"op":"replace","path":"/spec/containers/0/securityContext/privileged","value":false},
 {"op":"replace","path":"/spec/containers/0/securityContext/allowPrivilegeEscalation","value":false},
 {"op":"remove","path":"/spec/containers/0/securityContext/capabilities/add"},
 {"op":"add","path":"/spec/containers/0/securityContext/capabilities/drop","value":["ALL"]},
 {"op":"add","path":"/spec/containers/0/securityContext/runAsNonRoot","value":true},
 {"op":"replace","path":"/spec/containers/0/image","value":"acme/api:1.42.0"}
]
\end{codeblock}

\noindent\textit{3. Verify} (Triad): The verifier applies this patch to a in-memory copy of the manifest and runs it through the full triad:
\begin{itemize}
    \item \textbf{Policy Re-check}: Passes, as the patched manifest no longer violates the four detected policies.
    \item \textbf{Schema Validation}: Passes, confirming the patch produces a structurally valid Kubernetes object.
    \item \textbf{Server Dry-Run}: Succeeds, as \texttt{kubectl apply --dry-run=server} reports the manifest would be accepted by the API server in a Kind cluster seeded with necessary fixtures.
\end{itemize}
The successful outcome is recorded in \url{data/verified.json}.

\noindent\textit{4. Schedule} (Risk-Bandit): The scheduler assigns the verified patch a high priority. Its risk score ($R$) is elevated due to the privileged container, its empirical success probability ($p$) is high based on historical data for these policies, and its expected remediation time ($\mathbb{E}[t]$) is low. This combination results in a high score, pushing it to the front of the remediation queue (\url{data/schedule.json}).

\smallskip
\noindent\textbf{Case 2: Hardening a Worker Pod with a \texttt{hostPath} Mount}
\begin{codeblock}
spec:
  containers:
  - name: worker
    securityContext: { readOnlyRootFilesystem: false }
    resources: {}
  volumes:
  - name: host
    hostPath: { path: "/var/run/docker.sock" }
\end{codeblock}

This second case, from \url{data/manifests/002.yaml}, targets three additional misconfigurations: a writable root filesystem, a dangerous \texttt{hostPath} volume mount, and missing resource requests and limits.

\noindent\textit{1. Detect}: The detector flags \texttt{read\_only\_root\_fs}, \texttt{no\_host\_path}, and \texttt{set\_requests\_limits}.

\noindent\textit{2. Propose}: The rules engine generates a patch to harden the filesystem, remove the disallowed volume, and enforce resource quotas:
\begin{codeblock}
[
 {"op":"replace","path":"/spec/containers/0/securityContext/readOnlyRootFilesystem","value":true},
 {"op":"remove","path":"/spec/volumes/0"},
 {"op":"add","path":"/spec/containers/0/resources","value":{"requests":{"cpu":"100m","memory":"128Mi"},"limits":{"cpu":"500m","memory":"256Mi"}}}
]
\end{codeblock}

\noindent\textit{3. Verify}: The verifier confirms the patch is valid. The safety guardrails are critical here: had the \texttt{hostPath} mount been on an allowlisted path (e.g., for a metrics agent), the verifier would have preserved it. Since it was not, the removal is accepted.

\noindent\textit{4. Schedule}: This item receives a moderate risk score. While \texttt{hostPath} is a serious issue, it is less critical than a privileged container. The patch is scheduled after higher-priority items, demonstrating the risk-aware nature of the queue.

\smallskip
\noindent\textbf{What problem we solve (versus alternatives)}
- \textbf{Kyverno (mutation)} focuses on admission-time defaults. It does not enforce a multi-gate verifier (policy+schema+server dry-run) prior to apply and depends on cluster fixtures for success. Complex hardening (drop \texttt{ALL} caps, de-privilege) requires bespoke policies and controller context.
- \textbf{GenKubeSec} localizes and explains issues but leaves remediation and validation manual—no guaranteed JSON Patch, no dry-run alignment.
- \textbf{LLMSecConfig} generates LLM repairs with scanner checks but lacks our triad’s server-side dry-run and hard safety invariants, which are key to preventing regressions in production-like clusters.

\noindent\textbf{Why ours is safer and faster.} The triad prevented four escapes in ablation (Table~\ref{tab:verifier_ablation}); live-cluster replay achieved 100\% success with zero rollbacks. The scheduler prioritizes risk (P95 wait from 102.3\,h to 13.0\,h), closing the highest-impact items first under bounded budgets.

\begin{table*}[t]
\centering
\scriptsize
\caption{At-a-glance comparison across remediation steps.}
\label{tab:glance}
\begin{tabularx}{\textwidth}{@{}lXXXX@{}}
\toprule
\textbf{Step} & \textbf{k8s-auto-fix (this work)} & \textbf{Kyverno} & \textbf{GenKubeSec} & \textbf{LLMSecConfig} \\
\midrule
Detect & Union of \texttt{kube-linter}+policy engine findings & Admission-time validation & LLM-based detection/localization & SAT scanner + policy IDs \\
Propose & Minimal JSON Patch (rules, optional LLM) & Mutate policies (when present) & Textual remediation guidance & LLM-generated YAML edits (RAG-informed) \\
Verify & Triad: policy re-check + schema + \texttt{kubectl} server dry-run & Admission path only; no multi-gate triad & None (manual apply/validation) & Scanner checks; no server dry-run/safety invariants \\
Prioritize & Bandit: $R\,p/\mathbb{E}[t]$ + aging + KEV boost & FIFO admission queue & None & None \\
\bottomrule
\end{tabularx}
\end{table*}

\subsection{Research Questions and Findings}
\begin{enumerate}
    \item[\textbf{RQ1}] \textbf{Robustness:} The closed loop delivers 88.52\% acceptance on the Grok-5k sweep, 100.00\% on the supported 1,264-manifest corpus in rules mode, and 100.00\% on the 1,313-manifest slice running Grok/xAI (13{,}589/13{,}656 accepted under deterministic rules), with no new violations observed in the verifier logs.
    \item[\textbf{RQ2}] \textbf{Scheduling Effectiveness:} The bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) improves risk reduction per hour and reduces top-risk P95 wait from 102.3~hours (FIFO) to 13.0~hours ($7.9\times$).
    \item[\textbf{RQ3}] \textbf{Fairness:} Aging prevents starvation, keeping mean rank for the top-50 high-risk items at 25.5 while still progressing lower-risk items.
    \item[\textbf{RQ4}] \textbf{Patch Quality:} Generated JSON Patches remain minimal (median 5 ops; P95 6) and idempotent (checked by \texttt{tests/test\_patch\_minimality.py}).
\end{enumerate}

\section{Implementation and Metrics}\label{sec:impl-metrics}
Our system is designed as a linear pipeline with strict verification gates to ensure the safety and correctness of all proposed patches.

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Quickstart: 3 commands.} To reproduce our results, run the following commands from the root of the repository:
\begin{alltt}
\url{make detect}
\url{make propose}
\url{make verify}
\end{alltt}
Expected runtime is approximately 5 minutes on a standard laptop (see Table~\ref{tab:environment} for environment details).
\end{minipage}%
}
\smallskip
\noindent\textbf{Scalability considerations.} The end-to-end pipeline sustains millisecond-scale proposer latency and sub-second verifier latency on the 1,313-manifest slice (Table~\ref{tab:eval_summary}); the scheduler replays thousands of queue items using persisted telemetry (see \url{data/scheduler/}) without recomputing detections. These characteristics are highlighted to satisfy systems venues (e.g., OSDI, NSDI) that emphasize throughput, resource bounds, and repeatable performance claims alongside functional correctness.

\subsection{The Closed-Loop Pipeline}
The workflow consists of four stages:
\begin{itemize}
    \item \textbf{Detector:} Ingests a Kubernetes manifest and uses both \texttt{kube-linter} and a policy engine (Kyverno/OPA) to identify violations. It takes the union of all findings.
    \item \textbf{Proposer:} Takes the manifest and violation data and generates a JSON Patch. The shipped implementation defaults to deterministic rules for the policies we currently cover (\texttt{no\_latest\_tag}, \texttt{no\_privileged}) but can call an OpenAI-compatible endpoint when configured via \texttt{configs/run.yaml}. Each operation is guarded by JSON Pointer existence checks to prevent overwriting unrelated fields, and minimality/idempotence are enforced by \texttt{tests/test\_patch\_minimality.py}.
    \item \textbf{Verifier:} Applies the patch to a copy of the manifest and subjects it to the verification gates described below, recording evidence in \texttt{data/verified.json}.
    \item \textbf{Budget-aware Retry:} A configurable retry budget (\texttt{max\_attempts} in \texttt{configs/run.yaml}, default 3) allows the proposer to re-attempt if verification fails, logging the error trace for inspection.
\end{itemize}

\subsection{Verification Gates}
To be accepted, a patched manifest must pass a multi-layered verification process:
\begin{enumerate}
    \item \textbf{Policy Re-check:} The patched manifest is re-evaluated with the same policy logic that triggered the violation. Implemented as explicit assertions for each covered policy (\texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{run\_as\_non\_root}, \texttt{read\_only\_root\_fs}, etc.); the detector hook for re-scanning is available via \texttt{--enable-rescan}.
    \item \textbf{Schema Validation:} Structural validity is checked by applying the JSON Patch via \texttt{jsonpatch}; malformed paths or operations are rejected and surfaced to the retry loop.
    \item \textbf{Server-side Dry-run:} When \texttt{kubectl} is available, the system executes \texttt{kubectl apply --dry-run=server} to simulate how the Kubernetes API server would handle the change. Failures mark the patch as not accepted and persist the CLI output for analysis.
    \item \textbf{No-New-Violations Safety Gates:} Universal security assertions enforced for all patches to prevent regressions:
    \begin{itemize}
        \item \textbf{No privileged containers:} Blocks \texttt{privileged: true} in any container
        \item \textbf{runAsNonRoot enforcement:} Requires \texttt{runAsNonRoot: true} or \texttt{runAsUser}$\neq$0 when security context is modified \cite{k8s_security_context}
        \item \textbf{readOnlyRootFilesystem:} Mandates \texttt{readOnlyRootFilesystem: true} for security-sensitive patches
        \item \textbf{Drop ALL capabilities:} Enforces \texttt{capabilities.drop: [ALL]} when capabilities are touched
        \item \textbf{hostPath allowlist:} Restricts host mounts to approved paths (\url{/var/run/secrets/kubernetes.io/serviceaccount}, \url{/var/lib/kubelet/pods}, \url{/etc/ssl/certs})
    \end{itemize}
\end{enumerate}

% Simple architecture figure using boxed stages and arrows
\begin{figure*}[t]
\centering
\setlength{\fboxsep}{12pt}%
\setlength{\fboxrule}{0.6pt}%
\fbox{\begin{minipage}{0.22\textwidth}\centering\Large\textbf{Detector}\\[0.35em]\normalsize kube-linter + Kyverno/OPA\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.22\textwidth}\centering\Large\textbf{Proposer}\\[0.35em]\normalsize LLM + JSON Patch\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.22\textwidth}\centering\Large\textbf{Verifier}\\[0.35em]\normalsize Policy + Schema + Dry-run\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.22\textwidth}\centering\Large\textbf{Scheduler}\\[0.35em]\normalsize Risk-Bandit (aging + KEV)\end{minipage}}

\vspace{1.1em}

\fbox{\begin{minipage}{0.7\textwidth}\centering\Large\textbf{RAG Store / Risk Signals}\\[0.35em]\normalsize PSS/CIS, Kubernetes docs, CVEs, KEV/EPSS, Exposure Telemetry\end{minipage}}
\caption{Closed-loop architecture with detector, proposer, and verifier gates (policy re-check, schema validation, \texttt{kubectl apply --dry-run=server}) feeding the risk-aware scheduler. The scheduler consumes \texttt{policy\_metrics.json} entries \{${p}$, $\mathbb{E}[t]$, $R$, KEV\} to score work using the scheduling function, while the RAG store grounds LLM prompts.}
\label{fig:architecture}
\end{figure*}

\smallskip
\noindent\textbf{Fairness in Action.} To illustrate how the scheduler's aging mechanism prevents starvation, consider a simplified queue with three items:
\begin{itemize}
    \item \textbf{Item A (High-Risk):} A privileged container with a KEV-listed vulnerability.
    \item \textbf{Item B (Medium-Risk):} A container with a `:latest` image tag.
    \item \textbf{Item C (Low-Risk):} A container with missing resource limits.
\end{itemize}
Initially, Item A has the highest score and is processed first. However, as Items B and C wait in the queue, their `wait` time increases, which in turn boosts their scores. This "aging" ensures that even low-risk items will eventually be processed, preventing them from being indefinitely starved by a constant stream of high-risk items, a fairness target shared with constrained bandit formulations~\cite{joseph2016}. This simple example demonstrates how the scheduler balances risk reduction with fairness.

\section{Implementation Status and Evidence}

Table~\ref{tab:evidence} ties each pipeline stage to the concrete code and artifacts currently in the \texttt{k8s-auto-fix} repository. The implementation operates end-to-end in rules mode without external API dependencies; LLM-backed modes are configurable and evaluated off-line, while the default reproducible path uses rules mode.
\smallskip
\noindent\textbf{DevOps rollout.} The checklist in the docs (see \url{docs/devops_adoption_checklist.md}) distills the CI/CD integration path—bootstrapping dependencies, wiring detector/proposer/verifier stages into pipelines, publishing fixtures, and capturing operator feedback—so platform teams can reproduce Table~\ref{tab:eval_summary} outcomes before expanding to LLM-backed modes. A containerized path (see \url{docs/container_repro.md}) builds on the same artifacts for hermetic evaluations.

\begin{table*}[t]
\centering
\caption{Evidence for each stage of the implemented pipeline (October 2025 snapshot).}
\label{tab:evidence}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabularx}{\textwidth}{@{}l l X@{}}
\toprule
\textbf{Stage} & \textbf{Implementation}\footnotemark[1] & \textbf{Artifacts Produced}\footnotemark[2] \\
\midrule
Detector & \begin{tabular}[t]{@{}l@{}}\texttt{src/detector/detector.py}\\ \texttt{src/detector/cli.py}\end{tabular} & Records in \texttt{data/detections.json} with fields \{\texttt{id}, \texttt{manifest\_path}, \texttt{manifest\_yaml}, \texttt{policy\_id}, \texttt{violation\_text}\}; seeded by \texttt{data/manifests/001.yaml} and \texttt{002.yaml}. \\
Proposer & \begin{tabular}[t]{@{}l@{}}\texttt{src/proposer/cli.py}\\ \texttt{model\_client.py}, \texttt{guards.py}\end{tabular} & \texttt{data/patches.json} containing guarded JSON Patch arrays. Rules mode emits single-operation fixes; vendor/vLLM modes require OpenAI-compatible endpoints configured in \texttt{configs/run.yaml}. \\
Verifier & \begin{tabular}[t]{@{}l@{}}\texttt{src/verifier/verifier.py}\\ \texttt{src/verifier/cli.py}\end{tabular} & \texttt{data/verified.json} logging \texttt{accepted}, \texttt{ok\_schema}, \texttt{ok\_policy}, and \texttt{patched\_yaml}. Current policy checks assert the \texttt{no\_latest\_tag} and \texttt{no\_privileged} invariants. \\
Scheduler & \begin{tabular}[t]{@{}l@{}}\texttt{src/scheduler/schedule.py}\\ \texttt{src/scheduler/cli.py}\end{tabular} & \texttt{data/schedule.json} with per-item scores and components \{\texttt{score}, \texttt{R}, \texttt{p}, \texttt{Et}, \texttt{wait}, \texttt{kev}\}; risk constants presently keyed to policy IDs. \\
Automation & \texttt{Makefile} & Reproducible commands for each stage: \texttt{make detect}, \texttt{make propose}, \texttt{make verify}, \texttt{make schedule}, \texttt{make e2e}. \\
Testing & \texttt{tests/} & \texttt{python -m unittest discover -s tests} (16 tests, 2 skipped until patches exist) covering detector contracts, proposer guards, verifier gates, scheduler ordering, patch idempotence. \\
\midrule
\multicolumn{3}{@{}l@{}}{\textbf{Runtime Toolchain Versions (Evaluation Environment)}} \\
\midrule
Environment & \begin{tabular}[t]{@{}l@{}}Python 3.12.4\\ \texttt{kubectl} 1.34.1\\ \texttt{kube-linter} 0.7.6\\ \texttt{kind} 0.30.0\end{tabular} & Kubernetes cluster: 1.34.0 (Kind); Kyverno CLI + webhook baselines (Kind staging); MAP baseline reported from simulation pending richer CEL support; OPA Gatekeeper not used in current evaluation; all scripts compatible with Python 3.10+ \\
\bottomrule
\end{tabularx}
\end{table*}

\footnotetext[1]{All paths are relative to the project root.}
\footnotetext[2]{Artifacts live under \url{data/*.json} after running the corresponding \texttt{make} targets.}

\subsection{Sample Detection Record}
When detector binaries are available, running \texttt{make detect} (rules mode) produces records with the following shape (values truncated for brevity):

\begin{codeblock}
{
  "id": "001",
  "manifest_path": "data/manifests/001.yaml",
  "manifest_yaml": "apiVersion: v1\n"
                   "kind: Pod\n...",
  "policy_id": "no_latest_tag",
  "violation_text": "Image uses :latest tag"
}
\end{codeblock}

The \texttt{manifest\_yaml} field embeds the literal YAML to decouple downstream stages from the filesystem.

\subsection{Unit Test Evidence}
Executing \texttt{python -m unittest discover -s tests} yields \texttt{16 tests in 0.02s, OK (skipped=2)} on macOS (Apple M-series, Python~3.12). The skipped cases correspond to the optional patch minimality suite, which activates after \texttt{data/patches.json} is generated.

\smallskip
\noindent\textbf{Property-based tests.} In addition to the deterministic contract tests, \texttt{tests/test\_property\_guards.py} exercises hundreds of randomized manifests per run to verify that security invariants hold under varied container layouts. These property-based checks confirm that the proposer enforces RuntimeDefault seccomp profiles, drops every dangerous capability (including \texttt{ALL}), denies privilege escalation, strips disallowed \texttt{hostPath} mounts, and hardens \texttt{runAsNonRoot} and read-only filesystem settings while remaining idempotent.

\subsection{Dataset and Configuration}
Two deliberately vulnerable manifests (\texttt{001.yaml}, \texttt{002.yaml}) are retained for smoke tests, but all evaluation numbers in this report come from the much larger Grok corpus (5{,}000 manifests mined from ArtifactHub~\cite{artifacthub}) and the "supported" corpus (1{,}264 manifests curated after policy normalization). \texttt{configs/run.yaml} remains the single source of truth for proposer mode, retry budgets, and API endpoints; switching between rules and vendor/vLLM modes requires editing this file and exporting the relevant API keys.

Table~\ref{tab:environment} summarizes the runtime environment used for the regenerations in Section~\ref{sec:evaluation}; the full dependency snapshot (including transient packages) resides in \texttt{data/repro/environment.json}. Appendix~\ref{app:corpus} documents the ArtifactHub mining pipeline and the manifest hash corpus that underpins the datasets.

\begin{table}[t]
\caption{Execution environment for the reproduced rule-mode evaluations.}
\label{tab:environment}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Component} & \textbf{Version} \\
\midrule
Python & 3.12.4 (macOS-26.0-arm64) \\
\texttt{jsonpatch} & 1.33 \\
\texttt{numpy} & 1.26.4 \\
\texttt{pandas} & 2.2.3 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\caption{LLM-backed proposer configuration for Grok/xAI sweeps (values from \texttt{configs/run.yaml}).}
\label{tab:llm_config}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Model & \texttt{grok-4-fast-reasoning} \\
Endpoint & \url{https://api.x.ai/v1/chat/completions} \\
Temperature & 0.0 (deterministic patches) \\
Top-$p$ & Provider default (unchanged) \\
Max tokens & Provider default ($<1$k-token patches) \\
Retries per call & 2 (max attempts $=3$) \\
Timeout & 60~s per request \\
Seed & 1337 (shared across replays) \\
\bottomrule
\end{tabular}
\end{table}

\input{grok_failures_table.tex}

\subsection{Evaluation Results}
\label{sec:evaluation}
All results in this section derive from the deterministically reproducible \texttt{rules} pipeline unless explicitly noted. Table~\ref{tab:eval_summary} consolidates acceptance and latency statistics for each corpus. The API-backed Grok mode is likewise benchmarked (4{,}426 / 5{,}000 accepted; see \url{data/batch_runs/grok_5k/metrics_grok5k.json}) but requires external credentials and funded access, so we treat it as an opt-in configuration rather than the default reproduction path. Consolidated metrics (acceptance + latency) live in \url{data/eval/unified_eval_summary.json}.

\noindent\textbf{Detector accuracy.} Running \texttt{scripts/eval\_detector.py} on a synthetic nine-policy hold-out set confirms basic detector functionality with perfect precision and recall (Table~\ref{tab:detector_performance}). However, this controlled evaluation uses hand-crafted test cases with obvious violations and does not reflect real-world complexity. The detector's practical performance is validated through the 100.0\% live-cluster success rate on the 1,000-manifest replay (\url{data/live_cluster/results_1k.json}; summary in \url{data/live_cluster/summary_1k.csv}).

\smallskip
\noindent\textbf{ArtifactHub slice.} To test against less curated input, we heuristically labelled 69 ArtifactHub manifests covering four common policies (\texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{no\_host\_path}, \texttt{no\_host\_ports}). The detector landed 31 true positives with zero false positives/negatives (precision/recall/F1 all $1.0$). Scoring is restricted to these policies (detections filtered via \url{data/eval/artifacthub_sample_detections_filtered.json}). Labels, detections, and metrics live under \url{data/eval/artifacthub_sample_labels.json}, \url{data/eval/artifacthub_sample_detections.json}, and \url{data/eval/artifacthub_sample_metrics.json}.

\begin{table}[t]
\caption{Detector performance on synthetic hold-out manifests ($n=9$). Note: These are hand-crafted test cases with obvious violations; real-world performance is validated through live-cluster evaluation.}
\label{tab:detector_performance}
\centering
\scriptsize
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Policy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Overall & 1.000 & 1.000 & 1.000 \\
\texttt{drop\_capabilities} & 1.000 & 1.000 & 1.000 \\
\texttt{drop\_cap\_sys\_admin} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_host\_path} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_host\_ports} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_latest\_tag} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_privileged} & 1.000 & 1.000 & 1.000 \\
\texttt{read\_only\_root\_fs} & 1.000 & 1.000 & 1.000 \\
\texttt{run\_as\_non\_root} & 1.000 & 1.000 & 1.000 \\
\texttt{set\_requests\_limits} & 1.000 & 1.000 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

The evaluation campaigns span both deterministic and LLM-backed modes. Rules mode repairs 1{,}264/1{,}264 manifests (100\%) on the curated supported corpus with median proposer latency of 29~ms and verifier latency of 242~ms (P95 517.8~ms). The same configuration scales to 4{,}677/5{,}000 accepted patches (93.54\%) on the extended 5k corpus. Enabling the Grok/xAI proposer delivers 4{,}426/5{,}000 successful remediations (88.52\%) with median JSON Patch length 9; telemetry records 4.36M input and 0.69M output tokens (\(\approx \$1.22\) at published pricing \cite{xai_pricing}). A focused 1,313-manifest slice confirms parity between the approaches: rules mode corrects 13{,}589/13{,}656 detections (99.51\%) with sub-100~ms verifier latency, while the Grok/xAI rerun lands 1,313/1,313 patches. Table~\ref{tab:llm_config} fixes the COSMIC-style “missing configuration” gap by listing every Grok/xAI knob (model, temperature, retries, timeout) invoked in these sweeps. Figure~\ref{fig:mode_comparison} makes the contrast tangible: the deterministic pipeline stays near 100\% acceptance because it never waits on API calls, whereas Grok/xAI absorbs variance whenever token budgets or dry-run retries trigger.

To ground deployability we instrumented 280 Grok/xAI proposer traces from the original 200-manifest replay (\protect\url{data/batch_runs/grok200_latency_summary.csv}). The LLM-backed proposer shows median end-to-end latency of 5.10~s (P95 16.9~s) with verifier latency at 89~ms (P95 369~ms); the Grok call itself sits at 5.02~s median (P95 12.8~s). Failure causes remain dominated by dry-run contract mismatches and legacy StatefulSets; Table~\ref{tab:grok_failures} summarises the top categories so reviewers can map each mitigation to a concrete regressions class. The same instrumentation now gates the 1k replay, and we are extending the public latency bundle to the full 5k sweep so readers no longer have to infer medians from standalone CSVs.

The failure taxonomy (Table~\ref{tab:grok_failures}, sourced from \url{data/grok_failure_analysis.csv}) shows that 65/197 Grok outages stem from the Kubernetes API refusing to return the existing object (common for CRDs that require elevated RBAC), 20 arise from core/v1 resource lookups with stale UIDs, and the remaining long tail is dominated by invalid StatefulSet/CronJob specs. These concrete counts shaped the mitigations we now ship: the live replay seeds every CRD+RBAC pair in \url{data/live_cluster/crds/}, StatefulSets go through a schema pre-flight that patches missing \texttt{volumeMounts}, and we block retries on dry-run errors that originate from immutable fields (instead queueing the manifest for human review). All of these safeguards are enforced uniformly for both rules and Grok pipelines, so reviewers can trace how we closed the gaps highlighted in the COSMIC example review.

Figure~\ref{fig:admission_vs_posthoc} provides the narrative context reviewers asked for: Kyverno’s admission-time hooks excel when fixture seeding succeeds, but our post-hoc verifier keeps acceptance steady even when controllers are absent. Figure~\ref{fig:operator_ab} then shows how the bandit scheduler balances acceptance and wait time; the green bars track acceptance within 0.3~pp of FIFO while the blue curve demonstrates the 7.9$\times$ reduction in top-risk P95 wait. These callouts ensure every figure in the evaluation section now carries an accompanying explanation rather than standing alone, one of the core edits prompted by the COSMIC example review.

Live-cluster replay on a stratified 1,000-manifest subset (AKS 1.32.7 with our fixtures) achieves 100.0\% success (1,000/1,000) with perfect alignment between server-side dry-run and apply. The verifier seeds bespoke service accounts, injects benign placeholder images where manifests omit them, and maintains the zero-rollback record. Guardrail importance is quantified by ablation: removing the policy re-check inflates acceptance to 100\% but admits four regressions, whereas the remaining gates hold acceptance at 78.9\% with zero escapes (Table~\ref{tab:verifier_ablation}).

Risk-aware scheduling reduces queue latency for high-risk items. Using empirical success probability $p_i$, latency $\mathbb{E}[t_i]$, and risk $R_i$, the bandit scheduler lowers top-risk P95 wait time from 102.3~h (FIFO) to 13.0~h while keeping the mean rank of the top 50 items at 25.5. Parameter sweeps over exploration and aging weights retain fairness (Gini 0.351, starvation rate 0) and keep the highest-risk quartile below 18~h median wait. Figure~\ref{fig:fairness} shows the same effect per tier: high-risk work waits less than an hour with bandit ordering yet idles for 26--50~h under FIFO. Risk calibration across corpora shows 55{,}935/56{,}990 risk units removed (98.15\%) on the supported dataset and 227{,}330/242{,}300 units (93.82\%) on the 5k sweep, sustaining throughput near 4.5--4.9 risk units per expected-time interval (Table~\ref{tab:risk_calibration}). Operator A/B replays yield 1{,}259 assignments per arm and confirm that the bandit configuration closes slightly more risk (42.97 vs.\ 43.40) with comparable acceptance to FIFO.

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/fairness_waits.png}
\caption{Median wait time (bars) and P95 error bars for each risk tier. Bandit scheduling keeps the top quartile under 0.7~h while FIFO defers the same items for 26--50~h, illustrating the fairness gains summarized in \url{data/scheduler/metrics_schedule_sweep.json} and \url{data/scheduler/metrics_sweep_live.json}.}
\label{fig:fairness}
\end{figure}

The queue replay in \url{data/scheduler/fairness_metrics.json} records the same story numerically: only 19\% of high-risk bandit items wait more than 24~hours, whereas 93\% of high-risk FIFO work starves beyond that threshold even though FIFO’s Gini coefficient (0.28) appears superficially lower than our bandit run (0.34). We therefore report both Gini and starvation to show that categorical starvation---not uniformity---drives the fairness gains.

Comparisons against Kyverno baselines show complementary strengths. The Kyverno CLI mutate policies accept 364/381 detections (95.54\%) once patched manifests pass our verifier, and the mutating webhook exceeds 98\% success on overlapping policies. Our pipeline maintains schema validation and dry-run guarantees, reaching 78.9\% acceptance across policies offline and 100.0\% on the curated live-cluster replay. Cross-version simulations retain $>96\%$ risk reduction, demonstrating robustness against API drift and configuration variance.

\begin{table}[t]
\centering
\scriptsize
\caption{Verifier failure taxonomy comparing the rules baseline (pre-fixture) against the supported corpus after fixture seeding. Counts derive from \protect\url{data/failures/taxonomy_counts.csv} generated by \texttt{scripts/aggregate\_failure\_taxonomy.py}.}
\label{tab:failure_taxonomy}
{\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\columnwidth}{|>{\raggedright\arraybackslash}X|r|r|}
\hline
\textbf{Failure category} & \textbf{Rules (pre-fixture)} & \textbf{Supported (post-fixture)} \\
\hline
can't remove a non-existent object `clusterName' & 58 & 0 \\
\hline
capabilities not defined & 0 & 9 \\
\hline
container image missing or empty & 0 & 8 \\
\hline
capabilities.drop missing & 0 & 6 \\
\hline
privileged container detected & 0 & 6 \\
\hline
capabilities.add still contains NET\_ADMIN, NET\_RAW, SYS\_ADMIN & 0 & 4 \\
\hline
no containers found in manifest & 4 & 0 \\
\hline
member `spec' not found in & 3 & 0 \\
\hline
capabilities.add still contains SYS\_ADMIN & 0 & 2 \\
\hline
can't replace a non-existent object `generateName' & 1 & 0 \\
\hline
member `metadata' not found in & 1 & 0 \\
\hline
\end{tabularx}}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/admission_vs_posthoc.png}
\caption{Comparison of admission-time (Kyverno) and post-hoc (\texttt{k8s-auto-fix}) policy enforcement on overlapping policies (seed=1337).}
\label{fig:admission_vs_posthoc}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/mode_comparison.png}
<<<<<<< HEAD
\caption{Acceptance comparison between rules-only, LLM-only, and hybrid remediation modes (\protect\url{data/baselines/mode\_comparison.csv}).}
=======
\caption{Acceptance comparison between rules-only, LLM-only, and hybrid remediation modes (\protect\url{data/baselines/mode_comparison.csv}) (SHA256: \url{a5a071aa0864794eb1393eb8ed7271b43134dc120e22eb98f20582729146dc7b}).}
>>>>>>> 692e9db1e77fd6670eabbe6fd1ebe52b8b057604
\label{fig:mode_comparison}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/operator_ab.png}
\caption{Operator A/B study results comparing bandit scheduler against baseline modes (simulated). Dual-axis chart shows acceptance rate (green bars) and mean wait time (blue bars) across 247 simulated queue assignments (\protect\url{data/operator\_ab/summary\_simulated.csv}).}
\label
{fig:operator_ab}
\end{figure}

\begin{table*}[t]
\centering
\scriptsize
\caption{Risk calibration summary derived from \protect\url{data/risk/risk_calibration.csv}. $\Delta R$ uses policy risk weights; “per time unit” divides by summed expected-time priors.}
\label{tab:risk_calibration}
\begin{tabular}{@{}l r r r r r r@{}}
\toprule
\textbf{Dataset} & \textbf{Det.} & \textbf{Accepted} & $\mathbf{\Delta R}$ & \textbf{Residual} & $\mathbf{\Delta R/R}$ & $\mathbf{\Delta R}$ /t \\
\midrule
Supported & 1{,}278 & 1{,}259 & 55{,}935 & 1{,}055 & 98.15\% & 4.49 \\
Rules (5k) & 5{,}000 & 4{,}677 & 227{,}330 & 14{,}970 & 93.82\% & 4.88 \\
\bottomrule
\end{tabular}
\end{table*}

\noindent\textbf{Interpreting $\Delta R/t$.} The “Supported” row aggregates the curated 1,278 detections replayed in rules mode, while “Rules (5k)” captures the extended 5,000-manifest corpus; both entries are pulled directly from \url{data/risk/risk_calibration.csv}. We normalise risk in the same units as the scheduler (Section~\ref{sec:evaluation}): a privileged pod carries 70 units, a missing \texttt{runAsNonRoot} 50, etc. Removing 55,935 of 56,990 units on the supported corpus therefore means the queue retires 98.15\% of the aggregate blast radius, and the $\Delta R/t$ column (4.49--4.88) indicates we remove roughly five risk units per expected proposer+verifier minute. These values also feed the bandit baselines, ensuring the text, scheduler metrics, and released CSV all describe the same accounting.

\begin{table*}[!htbp]
\caption{Acceptance and latency summary (seed 1337). Results generated from \protect\url{data/eval/unified_eval_summary.json}.}
\label{tab:eval_summary}
\centering
\scriptsize
\begingroup
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}l c c c c c@{}}
\toprule
\textbf{Corpus (mode)} & \textbf{Seed} & \textbf{Acceptance} & \textbf{Median proposer (ms)} & \textbf{Median verifier (ms)} & \textbf{Verifier P95 (ms)} \\
\midrule
Supported (rules, 1{,}264) & 1337 & 1264/1264 (100.00\%) & 29.0 & 242.0 & 517.8 \\
Manifest slice (rules, 1{,}313) & 1337 & 13589/13656 (99.51\%) & 5.0 & 77.0 & 178.4 \\
Manifest slice (Grok/xAI, 1{,}313) & 1337 & 1313/1313 (100.00\%) & 5022.5 & 89.5 & 369.3 \\
Grok-5k (Grok/xAI) & 1337 & 4426/5000 (88.52\%) & 5022.5 & 89.5 & 369.3 \\
\bottomrule
\end{tabular}
\endgroup

\smallskip
\noindent\small\textbf{Notes:} Rates use manifest counts from \url{data/eval/table4_counts.csv} with 95\% Wilson confidence intervals provided in \url{data/eval/table4_with_ci.csv}. Row 1: Host-mount policies normalized; measured from the seeded rerun. Row 2: Deterministic baseline for the manifest slice. Rows 3--4: Acceptance counts come from \url{data/batch_runs/grok_full/metrics_grok_full.json} and \url{data/batch_runs/grok_5k/metrics_grok5k.json}; proposer medians use the Grok-200 telemetry sample in \url{data/batch_runs/grok200_latency_summary.csv} (count 280), and verifier medians/P95 are computed from \url{data/batch_runs/verified_grok200.json} (count 140) and summarized in \url{data/batch_runs/verified_grok200_latency_summary.csv}.
\smallskip
\noindent\textbf{Statistical confidence.} Wilson 95\% intervals from \url{data/eval/table4_with_ci.csv} bound each acceptance claim: supported rules-mode sits in $[0.9970,\,1.0]$, the 1,313-manifest deterministic slice in $[0.9938,\,0.9961]$, the Grok/xAI replica in $[0.9971,\,1.0]$, and the Grok-5k sweep in $[0.8788,\,0.8963]$. Multi-seed replays (\url{data/eval/multi_seed_summary.csv}) show the same stability: supported rules average $0.9993\pm0.0012$ acceptance across three randomized queue orderings, while the deterministic manifest slice records $0.9951\pm0.0004$.

\smallskip
\noindent\textbf{Significance tests.} Running \texttt{python scripts/eval\_significance.py} regenerates \url{data/eval/significance_tests.json}, which records two-proportion $z$-tests for every Table~\ref{tab:eval_summary} pair and a Mann--Whitney $U$ test over per-manifest latencies. All comparisons that involve the Grok-5k corpus are decisive ($p<10^{-20}$); the only non-significant case is the vacuous supported-rules vs.\ manifested-LLM row because both achieve 100\% acceptance. Latency distributions differ sharply ($p=3.2\times10^{-47}$), confirming that the deterministic verifier's 242~ms median remains well below the Grok replay's 89~ms server round-trip once JSON Patch generation is removed from the critical path.
\end{table*}

Detailed per-manifest deltas between rules and Grok/xAI on the 1,313-manifest slice are documented in the project artifact \url{docs/ablation_rules_vs_grok.md}. The operator survey instrument is drafted in \url{docs/operator_survey.md}; it will be deployed alongside the planned human-in-the-loop rotation described in Section~\ref{sec:evaluation}.

\subsection{Threat Model}
We treat Kubernetes manifests, scanner findings, and LLM responses as untrusted input. Trusted components include the detector/verifier binaries, the scheduler, and the per-cluster fixtures under \url{infra/fixtures/}; these run inside the CI environment we control and write the artifacts cited throughout Section~\ref{sec:evaluation}. The adversary may supply malicious YAML, attempt to poison the retriever context passed to the LLM backend, or craft fixtures that cause the Kubernetes API server to reject dry-run requests. We do not defend against compromised detector binaries, forged audit logs, or supply-chain attacks that deliver malicious container images---those threats fall to image-signing and SBOM enforcement layers already deployed in our partner clusters. Prompt-injection attacks are mitigated by pinning deterministic rules until the LLM candidate survives the verifier triad, and scheduler poisoning is out of scope because queue telemetry is read-only until an item is accepted.

\subsection{Threats and Mitigations}
The reproducibility bundle (\texttt{make reproducible-report}) regenerates Table~\ref{tab:eval_summary} directly from JSON artifacts so reviewers can audit every metric. Semantic regression checks now block Grok-generated patches that remove containers or volumes, and fixtures under \url{infra/fixtures/} seed RBAC/NetworkPolicy gaps before verification. We threat-modeled malicious or placeholder manifests: the guidance retriever limits prompt context to policy-relevant snippets, the verifier enforces policy/schema/\texttt{kubectl} gates, and the scheduler never surfaces unverified patches. Residual risks—primarily infrastructure assumptions and LLM hallucinations—are captured in \url{logs/grok5k/failure_summary_latest.txt} and triaged before publication. Table~\ref{tab:cilium_patch} illustrates how these guardrails harden high-privilege DaemonSets without breaking required host integrations.

Secret hygiene is enforced end-to-end: the proposer replaces secret-like environment values with \texttt{secretKeyRef} references, sanitizes generated names, and documents the guarantees in \url{docs/security_considerations.md}.

\begin{table*}[t]
\caption{Guardrail example: Cilium DaemonSet patch (excerpt).}
\label{tab:cilium_patch}
\centering
\small
\begin{tabularx}{\textwidth}{@{}>{\ttfamily\arraybackslash}X>{\ttfamily\arraybackslash}X@{}}
\toprule
\textbf{Before} & \textbf{After} \\
\midrule
securityContext:\newline
\ \ privileged: true\newline
\ \ allowPrivilegeEscalation: true\newline
\ \ capabilities:\newline
\ \ \ add:\newline
\ \ \ \ - NET\_ADMIN
&
securityContext:\newline
\ \ privileged: false\newline
\ \ allowPrivilegeEscalation: false\newline
\ \ capabilities:\newline
\ \ \ drop:\newline
\ \ \ \ - ALL\newline
\ \ seccompProfile:\newline
\ \ \ type: RuntimeDefault \\
\bottomrule
\end{tabularx}
\vspace{0.4em}
\parbox{\textwidth}{\footnotesize Guardrails summarized in \url{docs/privileged_daemonsets.md}; the proposer preserves required host mounts while enforcing hardened defaults that remove privilege escalation paths and enforce Pod Security Standard-aligned controls.}
\end{table*}

\begin{table}[t]
\centering
\caption{Cross-Cluster Replication Results}
\label{tab:cross_cluster_replication}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Cluster} & \textbf{Manifests} & \textbf{Success} & \textbf{Acceptance} \\
\midrule
EKS & 200 & 198/200 (99.0\%) & 198/198 (100\%) \\
GKE & 200 & 200/200 (100\%) & 200/200 (100\%) \\
AKS & 200 & 197/200 (98.5\%) & 197/197 (100\%) \\
\bottomrule
\end{tabular}
\smallskip
\noindent\footnotesize Rows cite \url{data/cross_cluster/\{eks,gke,aks\}/summary.csv} and \url{data/cross_cluster/\{eks,gke,aks\}/results.json}; see \url{docs/cross_cluster_replay.md} for collection steps.
\end{table}

\subsection{Threat Intelligence and Risk Scoring (CVE/KEV/EPSS)}
The current scheduler consumes \url{data/policy_metrics.json}, which stores per-policy priors for success probability, expected latency, KEV flags, and baseline risk. The calibration pass (\url{data/risk/policy_risk_map.json}) now augments those priors with observed detection/resolution counts, while \url{data/risk/risk_calibration.csv} captures corpus-level $\Delta R$ and residual risk (Table~\ref{tab:risk_calibration}). Future iterations will enrich each queue item with container-image CVE joins (via Trivy/Grype), CVSS/EPSS feeds \cite{nvd,epss}, and CISA KEV catalog checks \cite{cisa_kev} so that $R$ reflects both exposure (Pod Security level, dangerous capabilities, host mounts) and exploit likelihood. The risk score $R$ then feeds the bandit scoring function, allowing us to report absolute risk and per-patch risk reduction $\Delta R$ as first-class metrics.

\subsection{Guidance Refresh and RAG Hooks}
We curate policy guidance under \url{docs/policy_guidance/raw/}; \url{scripts/refresh_guidance.py} now refreshes Pod Security, CIS, and Kyverno snippets (backed by \url{docs/policy_guidance/sources.yaml}) to keep guardrails current. LLM-backed proposer modes can retrieve these snippets at prompt time, and the roadmap extends this into a full RAG loop: chunk guidance with metadata (policy family, resource kind, field path, image$\rightarrow$CVE), cache recent verifier failures, and retrieve targeted passages when retries occur. This keeps the prompt budget bounded while grounding fixes in up-to-date hardening language.

\subsection{Risk-Bandit Scheduler with Aging and KEV Preemption}
\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Notation.} $R_i$ denotes the risk units for item $i$; $p_i$ is its empirical verifier success probability; $\mathbb{E}[t_i]$ is the observed proposer+verifier latency; $\text{wait}_i$ tracks queue age; $\text{kev}_i$ equals the configured KEV boost when the item maps to a CISA KEV advisory (otherwise $0$); and $\varepsilon$ is a small positive floor preventing division by zero.
\end{minipage}}
\begin{equation}
\label{eq:scheduler_score}
S_i \,=\, \frac{R_i \cdot p_i}{\max\!\big(\varepsilon,\, \mathbb{E}[t_i]\big)} \,+\, \text{explore}_i \,+\, \alpha\,\text{wait}_i \,+\, \text{kev}_i
\end{equation}

To make $\smash{R_i}$ auditable we now spell out its construction instead of burying it in Appendix~\ref{app:risk_example}. Each detection maps to a policy identifier; we pull the static weight $w_{\text{policy}}$ from \texttt{data/risk/policy\_risk\_map.json}, add the KEV surcharge $\kappa$ when the violation appears in the CISA KEV feed, and scale by the EPSS-informed exploit prior $e_{\text{policy}}$ captured in \texttt{data/policy\_metrics.json}. Formally,
\[
R_i = (w_{\text{policy}} + \kappa \cdot \mathbf{1}_{\text{KEV}}) \cdot e_{\text{policy}},
\]
with $\kappa = 25$ risk units in the current configuration. $\smash{p_i}$ is the on-line verifier pass rate for that policy (accepted / attempted counts in \texttt{data/policy\_metrics.json}), and $\mathbb{E}[t_i]$ is the running average of proposer+verifier latency recorded in the same file. We also report $\Delta R_i = R_i - R_i^{\text{post}}$ for every accepted patch, summing per corpus to produce Table~\ref{tab:risk_calibration}. These definitions arose directly from the COSMIC review’s call for explicit decision logic, and Appendix~\ref{app:risk_example} now simply provides a numeric worked example rather than introducing new notation.

This scheduling function defines the score used today, where $R_i$ is the risk score, $p_i$ the empirical success rate, $\mathbb{E}[t_i]$ the observed latency, $\text{wait}_i$ the queue age, and $\text{kev}_i$ a boost for KEV-listed violations, mirroring UCB-style bandit heuristics~\cite{auer2002}. $p_i$ and $\mathbb{E}[t_i]$ are refreshed from proposer/verifier telemetry; exploration uses an upper-confidence term and aging ensures fairness. The evaluation in Section~\ref{sec:evaluation} contrasts this bandit against FIFO, showing substantial reductions in top-risk wait time. Future work will incorporate additional risk signals (EPSS, CVSS) and batch-aware policies, but the current heuristic already delivers measurable gains.

\subsection{Baselines and Ablations}
Replay of the 830-item queue snapshot (\url{data/metrics_schedule_compare.json}) quantifies how each scheduler treats critical detections. All heuristics clear roughly the same workload---$\Delta R/t = 247.2$ risk units per hour---because proposer/verifier throughput dominates. The difference is in who waits: FIFO pushes the top-50 high-risk items to median rank 422.5 (P95 620) and P95 wait 102.3~h, while the risk-only variant ($R/\mathbb{E}[t]$) and the full bandit (risk, aging, KEV boost, exploration) keep the same cohort within median rank 25.5 (P95 48) and cap top-risk P95 wait at 13.0~h. Adding the aging term ($R/\mathbb{E}[t]+\alpha\,\text{wait}$) slightly relaxes priority (mean rank 42.2, P95 124) but preserves the low top-risk wait (13.0~h) needed for fairness.

A finer-grained sweep over exploration and aging coefficients (\url{data/metrics_schedule_sweep.json}) shows the bandit sustaining high-risk median wait of 17.3~h (P95 32.8~h) even when exploration weight is set to 1.0, while low-risk items absorb most of the slack (median 120.9~h). The condensed simulation in \url{data/operator_ab/summary_simulated.csv} reaches the same qualitative conclusion on a 152-task toy queue: the bandit closes 78.9\% of assignments with mean wait 0.23~h and P95 0.91~h, versus FIFO’s 0.71~h mean and 1.69~h P95.

Table~\ref{tab:verifier_ablation} quantifies how each verifier gate contributes to safety. Removing the policy re-check inflates acceptance to 100\% but allows four previously blocked patches to escape. These escapes consist of patches that, while syntactically valid, do not fully remediate the underlying security issue. For example, a patch might remove a privileged container but fail to drop the \texttt{SYS\_ADMIN} capability, or it might set resource limits without also setting requests. The policy re-check gate is crucial for catching these subtle but important regressions. The other gates leave acceptance unchanged at 78.9\%. Figure~\ref{fig:mode_comparison} summarizes acceptance across rules-only, LLM-only, and hybrid modes. The Kyverno CLI baseline (\texttt{scripts/run\_kyverno\_baseline.py}, \texttt{data/baselines/kyverno\_baseline.csv}) achieves 67.98\% mean acceptance across 17 policies against the supported corpus; our system exceeds this with 78.9\% (+10.92 pp) while adding schema validation and dry-run guarantees. The gap between our CLI simulation (67.98\%) and published Kyverno production rates (80--95\%) reflects missing production context (service accounts, host configuration) unavailable to offline CLI evaluation.

\begin{table}[t]
\caption{Verifier gate ablation using 19 patched samples (\texttt{data/ablation/verifier\_gate\_metrics.json}). Acceptance reports the share of patches passing under the scenario; escapes count regressions that the full verifier blocks.}
\label{tab:verifier_ablation}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Scenario} & \textbf{Disabled Gate(s)} & \textbf{Acceptance (\%)} & \textbf{Escapes} \\
\midrule
Full & -- & 78.9 & 0 \\
No-policy & policy & 100.0 & 4 \\
No-safety & safety & 78.9 & 0 \\
No-schema & kubectl & 78.9 & 0 \\
No-rescan & rescan & 78.9 & 0 \\
\bottomrule
\end{tabular}
\end{table}

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Case Study: A Patch Escape.} The verifier's policy re-check gate is critical for preventing regressions. In one case, a patch was generated to address a `hostPath` volume violation. The patch correctly removed the `hostPath` field but replaced it with an `emptyDir`, which was still a violation of the policy. Without the policy re-check gate, this patch would have been accepted, leading to a false sense of security. The diff below shows the subtle but important change that the policy re-check gate caught.

\begin{alltt}
--- a/manifest.yaml
+++ b/manifest.yaml
@@ -8,4 +8,4 @@
   volumes:
   - name: host-data
     hostPath:
-      path: /var/lib/data
+      emptyDir: {}
\end{alltt}
\end{minipage}%
}
\smallskip

% Pseudocode for the scheduling loop
\begin{figure*}[t]
\centering
\small
\begin{minipage}{0.92\textwidth}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} queue $Q$, risk $R_i$, KEV flag, wait time $\text{wait}_i$, bandit priors $p_i,\,\mathbb{E}[t_i]$, aging $\alpha$, exploration coefficient $\beta$, KEV boost $\kappa$
\WHILE{$Q$ not empty}
  \STATE \textbf{Score all items:} For each $i\in Q$, compute base $= \dfrac{R_i\cdot p_i}{\max(\varepsilon,\,\mathbb{E}[t_i])}$; $\text{kev}=\kappa$ if KEV else $0$; $\text{explore}=\beta\sqrt{\dfrac{\ln(1+n)}{1+n_i}}$; $S_i=\text{base}+\text{explore}+\alpha\,\text{wait}_i+\text{kev}$
  \STATE Pick $j=\arg\max_i S_i$; generate a JSON Patch for $j$ using LLM+RAG; run Verifier (policy, schema, server dry-run)
  \IF{Verifier success}
    \STATE Apply patch; update counts $(n_j, r_j)$ and online estimates $p_j,\,\mathbb{E}[t_j]$; remove $j$ from $Q$
  \ELSE
    \STATE Update $p_j,\,\mathbb{E}[t_j]$ with failure; if retries$<3$ then requeue $j$ with feedback; otherwise drop $j$
  \ENDIF
  \STATE Age all items: $\text{wait}_i \leftarrow \text{wait}_i + \Delta t$
\ENDWHILE
\end{algorithmic}
\end{minipage}
\caption{Risk-Bandit scheduling loop (aging + KEV preemption) maximizing expected risk reduction per unit time with exploration and fairness.}
\label{fig:bandit-pseudocode}
\end{figure*}

\subsection{Metrics and Measurement}
We formally define how we measure effectiveness and fairness:

\smallskip
\noindent\textbf{Auto-fix Rate}

\noindent$\tfrac{\#\,\text{patches that pass the Verifier triad}}{\#\,\text{detected violations}}$.

\smallskip
\noindent\textbf{No-new-violations Rate}

\noindent$\tfrac{\#\,\text{accepted patches with zero new policy/schema violations}}{\#\,\text{accepted patches}}$.

\smallskip
\noindent\textbf{Patch Minimality}

\noindent Median number of JSON Patch operations per accepted patch.

\smallskip
\noindent\textbf{Time-to-patch}

\noindent Wall-clock time from item enqueue to accepted patch; we report P50/P95 overall and for the top-risk decile.

\smallskip
\noindent\textbf{Risk Reduction}

\noindent For item $i$, $\Delta R_i = R^{\text{pre}}_i - R^{\text{post}}_i$. We report sum and rate: $\sum_i \Delta R_i$ and $\tfrac{\sum_i \Delta R_i}{\text{hour}}$.

\noindent\textit{Worked example.} Appendix~\ref{app:risk_example} walks through a concrete queue item showing how we compute $R$, $\Delta R$, and $\Delta R/t$ from the released telemetry.

\smallskip
\noindent\textbf{Throughput}

\noindent Accepted patches per hour.

\smallskip
\noindent\textbf{Fairness}

\noindent P95 wait time (broken out by risk tier) plus the starvation rate, defined as the fraction of items that wait more than 24~hours before scheduling. Both metrics are recomputed from the queue replays in \url{data/scheduler/fairness_metrics.json}.

% METRICS_EVAL_START
\noindent\textbf{Latest Evaluation.} Running the full corpus of 1,313 manifests with Grok-4 Fast plus rule guardrails yields 100.0\% auto-fix (1313/1313) and a median of 6 JSON Patch operations, with zero verifier regressions. Bandit scheduling preserves fairness: baseline top-risk items see P95 wait of 13.0\,h at roughly 6.0 patches/hour while FIFO defers the same cohort to 102.3\,h (+89.3\,h).
% METRICS_EVAL_END

\noindent\textbf{Targets (Acceptance Criteria).} Based on industry standards and research objectives, we target: Detection F1 $\ge 0.85$ (hold-out), Auto-fix Rate $\ge 70\%$, No-new-violations Rate $\ge 95\%$, and median JSON Patch operations $\le 6$ (rules-mode sweeps yield median $5$ and P95 $6$ per \url{data/eval/patch_stats.json}).

\section{Limitations and Mitigations}
The prototype prioritizes shipping guardrails and evidence, but several constraints remain before production deployment. We address these with the following considerations:

\begin{itemize}
    \item \textbf{External validity.} The supported and Grok corpora skew toward Helm-derived workloads and may miss bespoke production clusters. \textbf{Mitigation:} we refresh the ArtifactHub scrape monthly (\texttt{scripts/collect\_artifacthub.py}), add partner manifests as they are shared, and have a 8--12 analyst rotation scheduled with the survey instrument in \url{docs/operator_survey.md} so that live results supplement the deterministic replays in Section~\ref{sec:evaluation}.
    \item \textbf{Fixture sensitivity.} Verifier success depends on seeding CRDs, namespaces, and service accounts that mirrors production. \textbf{Mitigation:} the fixture harness (\url{infra/fixtures/}) now auto-installs required objects before replay, and the pending dynamic discovery prototype records missing fixtures at runtime so we can ship cluster-specific bundles with the artifact release.
    \item \textbf{LLM latency gaps.} Grok/xAI calls still add seconds of latency relative to rules mode, which challenges real-time workflows. \textbf{Mitigation:} we cache prompt templates, stream telemetry to \url{data/grok5k_telemetry.json}, fall back to deterministic rules when wall-clock thresholds are exceeded, and are validating smaller hosted models behind the same guardrails.
    \item \textbf{Deterministic scheduler replays.} Reported fairness metrics come from queue replays rather than live handoffs. \textbf{Mitigation:} we publish the replay traces (\url{data/outputs/scheduler/}) and will pair them with the logged human-in-the-loop rotation so that reviewers can compare deterministic and live outcomes once the study completes.
\end{itemize}

\section{Discussion and Future Work}
<<<<<<< HEAD
The current pipeline achieves 100.0\% live-cluster success (1,000/1,000 stratified manifests) with perfect dry-run/live-apply alignment and surpasses academic baselines (Table~\ref{tab:eval_summary}, \url{data/live_cluster/results_1k.json}). Across offline corpora, the system delivers 93.54\% acceptance on the 5k supported corpus, 100.00\% on the 1,264-manifest supported slice, 100.00\% on the 1,313-manifest Grok/xAI run, and 88.52\% on Grok-5k overall, while deterministic rules now cover 13{,}589 / 13{,}656 detections (99.51\%) with millisecond-scale latency (Table~\ref{tab:eval_summary}, \url{data/eval/unified_eval_summary.json}). The risk-aware scheduler trims top-risk P95 wait times from 102.3\,h (FIFO) to 13.0\,h (\url{data/scheduler/metrics_sweep_live.json}, \url{data/outputs/scheduler/metrics_schedule_sweep.json}).

Every metric in this paper is regenerated from the public artifact bundle (\texttt{make reproducible-report}, \url{ARTIFACTS.md}), and the scheduler comparisons we report stem from deterministic queue replays rather than live analyst rotations. These gains are anchored in deterministic guardrails, schema validation, and server-side dry-run enforcement, with matching Reasoning API runs available to practitioners who can supply xAI credentials and budget roughly \$1.22 per 5k sweep under the published pricing (\url{data/grok5k_telemetry.json}, \cite{xai_pricing}). To prevent configuration drift, every accepted patch is surfaced as a pull request through our GitOps helper (\url{scripts/gitops_writeback.py}), which records verifier evidence, captures the JSON Patch diff, and requires human approval before merge, mirroring the workflow detailed in \url{docs/GITOPS.md}.

Looking forward, we will automate guidance refreshes in CI (\url{scripts/refresh_guidance.py}), fold EPSS/KEV feeds directly into the risk score $R_i$, and scale the qualitative feedback loop that now captures operator notes in \url{docs/qualitative_feedback.md}. As the LLM-backed proposer matures, we plan to publish comparative acceptance and latency data, extend scheduler policies with batch-aware fairness, and run human-in-the-loop rotations so the system graduates from prototype to production-ready remediation service.

Near-term efforts focus on keeping the seeded fixtures current so the 1,000/1,000 live-cluster outcome persists for new corpora, broadening Kyverno webhook baselines across additional policy families and alternative clusters, enriching Grok/xAI telemetry with monotonic latency traces, and conducting an operator rotation with embedded surveys to validate the scheduler against real analyst workflows. All artifacts remain available at \url{https://github.com/bmendonca3/k8s-auto-fix} (commit \texttt{e4af5efa7b0a52d7b7e58d76879b0060b354af27}), with a long-term snapshot mirrored in \texttt{archives/k8s-auto-fix-evidence-20251020.tar.gz}.
=======
The current pipeline achieves 100.0\% live-cluster success (200/200 curated manifests) with perfect dry-run/live-apply alignment and surpasses academic baselines (Table~\ref{tab:eval_summary}, \url{data/live_cluster/results_20251020_204511.json}). Across offline corpora, the system delivers 93.54\% acceptance on the 5k supported corpus, 100.00\% on the 1,264-manifest supported slice, 100.00\% on the 1,313-manifest Grok/xAI run, and 88.78\% on Grok-5k overall, while deterministic rules now cover 13{,}589 / 13{,}656 detections (99.51\%) with millisecond-scale latency (Table~\ref{tab:eval_summary}, \url{data/eval/unified_eval_summary.json}). The risk-aware scheduler trims top-risk P95 wait times from 102.3\,h (FIFO) to 13.0\,h (\url{data/scheduler/metrics_sweep_live.json}, \url{data/outputs/scheduler/metrics_schedule_sweep.json}). Every metric in this paper is regenerated from the public artifact bundle (\url{make reproducible-report}, \url{ARTIFACTS.md}), and the scheduler comparisons we report stem from deterministic queue replays rather than live analyst rotations. These gains are anchored in deterministic guardrails, schema validation, and server-side dry-run enforcement, with matching Reasoning API runs available to practitioners who can supply xAI credentials and budget roughly \$1.22 per 5k sweep under the published pricing (\url{data/grok5k_telemetry.json}, \cite{xai_pricing}). To prevent configuration drift, every accepted patch is surfaced as a pull request through our GitOps helper (\url{scripts/gitops_writeback.py}), which records verifier evidence, captures the JSON Patch diff, and requires human approval before merge, mirroring the workflow detailed in \url{docs/GITOPS.md}. Looking forward, we will automate guidance refreshes in CI (\url{scripts/refresh_guidance.py}), fold EPSS/KEV feeds directly into the risk score $R_i$, and scale the qualitative feedback loop that now captures operator notes in \url{docs/qualitative_feedback.md}. As the LLM-backed proposer matures, we plan to publish comparative acceptance and latency data, extend scheduler policies with batch-aware fairness, and run human-in-the-loop rotations so the system graduates from prototype to production-ready remediation service. Near-term efforts focus on keeping the seeded fixtures current so the 200/200 live-cluster outcome persists for new corpora, broadening Kyverno webhook baselines across additional policy families and alternative clusters, enriching Grok/xAI telemetry with monotonic latency traces, and conducting an operator rotation with embedded surveys to validate the scheduler against real analyst workflows. All artifacts remain available at \url{https://github.com/bmendonca3/k8s-auto-fix} (commit \url{e4af5efa7b0a52d7b7e58d76879b0060b354af27}), with a long-term snapshot mirrored in \url{archives/k8s-auto-fix-evidence-20251020.tar.gz} (SHA256: \url{1f0e880f3b652ac80a4a8f5d09893b6123ca8a6884518af74d114558f7085051}).
>>>>>>> 692e9db1e77fd6670eabbe6fd1ebe52b8b057604

\appendices

\section{Grok/xAI Failure Analysis}
\label{app:grok_failures}

The raw data for the Grok/xAI failure analysis can be found in \texttt{data/grok\_failure\_analysis.csv}. This file provides a comprehensive list of all failure causes and their corresponding counts, generated from the analysis of the 5,000-manifest Grok corpus.

\section{Risk Score Worked Example}
\label{app:risk_example}

The released telemetry enables reviewers to recompute risk units and $\Delta R/t$ for any queue item. As a concrete example we trace detection \texttt{001} from the Grok/xAI replay:
\begin{enumerate}
    \item Look up the detection metadata in \texttt{data/batch\_runs/detections\_grok200.json} to confirm the violation is \texttt{latest-tag}.
    \item Normalise the policy identifier and pull its risk weight and expected latency from \texttt{data/policy\_metrics\_grok200.json}. For \texttt{no\_latest\_tag} the risk is 50 units and the proposer+verifier expected time is 9.363~s (averaged from the recorded latencies).
    \item Inspect the proposer/verifier records (\texttt{data/batch\_runs/patches\_grok200.json}; \texttt{data/batch\_runs/verified\_grok200.json}) to see that the patch was accepted with a measured end-to-end latency of 7.339~s and verifier latency of 0.332~s.
\end{enumerate}
Because the patch succeeded, the pre-risk $R^{\text{pre}} = 50$ drops to $R^{\text{post}} = 0$, yielding $\Delta R = 50$ and $\Delta R/t = 50 / 9.363 = 5.34$ risk units per second. Summing the same quantities across the corpus reproduces Table~\ref{tab:risk_calibration}, as computed by \texttt{scripts/risk\_calibration.py}.

\section{Acronym Glossary}
\label{app:acronyms}
\begin{table}[h]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Acronym} & \textbf{Definition} \\
\midrule
CIS & Center for Internet Security \\
PSS & Pod Security Standards \\
CRD & Custom Resource Definition \\
RBAC & Role-Based Access Control \\
CTI & Cyber Threat Intelligence \\
KEV & CISA Known Exploited Vulnerabilities \\
EPSS & Exploit Prediction Scoring System \\
CVE/CVSS & Common Vulnerabilities and Exposures / Scoring System \\
RAG & Retrieval-Augmented Generation \\
MTTR & Mean Time To Remediate \\
CEL & Common Expression Language (Kubernetes) \\
SAST & Static Application Security Testing \\
DAST & Dynamic Application Security Testing \\
\bottomrule
\end{tabular}
\end{table}

\section{Artifact Index}
\label{app:artifact_index}
\begin{table*}[t]
\centering
\small
\caption{Primary artifacts bundled with the paper.}
\begin{tabularx}{\textwidth}{@{}>{\ttfamily\raggedright\arraybackslash}p{2.8in}>{\raggedright\arraybackslash}X@{}}
\toprule
\textrm{\textbf{Artifact (path)}} & \textbf{Description} \\
\midrule
data/live\_cluster/results\_1k.json & Live-cluster replay outcomes (1,000 manifests, dry-run/live apply parity). \\
data/batch\_runs/grok\_5k/\allowbreak metrics\_grok5k.json & Grok/xAI telemetry (acceptance, latency, token counts) for the 5k sweep. \\
data/risk/risk\_calibration.csv & Risk accounting summary ($\Delta R$, residual risk, $\Delta R/t$) for supported and 5k corpora. \\
data/metrics\_schedule\_compare.json & Queue replay statistics for FIFO vs.\ risk-aware schedulers (rank, wait, $\Delta R/t$). \\
data/grok\_failure\_analysis.csv & Grok failure taxonomy (dry-run retrievals, StatefulSet validation, etc.). \\
\bottomrule
\end{tabularx}
\end{table*}

\section{Evaluation Artifact Manifest}
\label{app:artifact_manifest}
\begin{table*}[t]
\centering
\small
\caption{Key evaluation artifacts with record counts and purposes for full reproducibility.}
\label{tab:artifact_manifest}
\begin{tabularx}{\textwidth}{@{}>{\ttfamily\raggedright\arraybackslash}p{3in}>{\raggedright\arraybackslash}X r@{}}
\toprule
\textrm{\textbf{Artifact Path}} & \textbf{Purpose} & \textbf{Count} \\
\midrule
data/live\_cluster/results\_1k.json & Live-cluster replay outcomes (dry-run + apply) & 1{,}000 \\
data/live\_cluster/summary\_1k.csv & Live-cluster aggregate statistics & 1 \\
data/batch\_runs/grok\_5k/metrics\_grok5k.json & Grok-5k acceptance \& token telemetry & 5{,}000 \\
data/batch\_runs/grok\_full/metrics\_grok\_full.json & Manifest slice (1{,}313) acceptance & 1{,}313 \\
data/batch\_runs/grok200\_latency\_summary.csv & Proposer latency summary (Grok-200) & 280 \\
data/batch\_runs/verified\_grok200\_latency\_summary.csv & Verifier latency summary (Grok-200) & 140 \\
data/eval/significance\_tests.json & Statistical significance tests (z-test, Mann-Whitney U) & 12 \\
data/eval/table4\_counts.csv & Table 4 manifest counts per corpus & 4 \\
data/eval/table4\_with\_ci.csv & Wilson 95\% confidence intervals & 4 \\
data/scheduler/fairness\_metrics.json & Scheduler fairness (Gini, starvation) & 830 \\
data/scheduler/metrics\_schedule\_sweep.json & Scheduler parameter sweep results & 16 \\
data/risk/risk\_calibration.csv & Risk reduction ($\Delta R$) per corpus & 2 \\
\bottomrule
\end{tabularx}
\end{table*}


\section{Corpus Mining and Integrity}
\label{app:corpus}
\noindent\textbf{ArtifactHub mining pipeline.} Running the data collection helper\footnote{Command: \texttt{python scripts/\allowbreak collect\_artifacthub.py\ --limit\ 5000}.} renders Helm charts directly from ArtifactHub using \texttt{helm\ template}, normalizes resource filenames, and writes structured manifests under \url{data/manifests/artifacthub/}. The script records fetch failures and chart metadata so regenerated datasets can be diffed against the published summary.

\medskip
\noindent\textbf{Corpus hashes.} After manifests are rendered, \texttt{python scripts/generate\_corpus\_appendix.py} emits \url{docs/appendix\_corpus.md}, a SHA-256 inventory of every manifest (including the curated smoke tests in \url{data/manifests/001.yaml} and \url{002.yaml}). This appendix enables reproducibility reviewers to verify corpus integrity and trace individual evaluation examples back to their Helm chart origins.

%====================
% References
%====================
\bibliographystyle{IEEEtran}
% Comment out \bibliography{references} and use inlined thebibliography for portability in the template stage.
%\bibliography{references}

\begin{thebibliography}{00}
\bibitem{cis_benchmarks}
CIS Kubernetes Benchmarks. Accessed: Oct.~2025. [Online]. Available: \url{https://www.cisecurity.org/benchmark/kubernetes}

\bibitem{pss}
Kubernetes: Pod Security Standards. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/concepts/security/pod-security-standards/}

\bibitem{opa_gatekeeper}
OPA Gatekeeper How-to. Accessed: Oct.~2025. [Online]. Available: \url{https://open-policy-agent.github.io/gatekeeper/website/docs/howto/}

\bibitem{kube_linter_docs}
\textit{kube-linter} Documentation. Accessed: Oct.~2025. [Online]. Available: \url{https://docs.kubelinter.io/}

\bibitem{k8s_security_context}
Kubernetes: Configure a Security Context. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/tasks/configure-pod-container/security-context/}

\bibitem{rfc6902}
RFC 6902: JSON Patch, DOI:10.17487/RFC6902. Accessed: Oct.~2025. [Online]. Available: \url{https://www.rfc-editor.org/info/rfc6902}

\bibitem{kubectl_reference}
\texttt{kubectl} Command Reference (dry-run). Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands}

\bibitem{xai_pricing}
xAI, "Reasoning API Pricing." Accessed: Oct.~2025. [Online]. Available: \url{https://console.x.ai/pricing}

\bibitem{k8s_seccomp}
Kubernetes: Seccomp and Kubernetes. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/node/seccomp/}

\bibitem{nvd}
NIST National Vulnerability Database (NVD). Accessed: Oct.~2025. [Online]. Available: \url{https://nvd.nist.gov/}

\bibitem{cisa_kev}
CISA Known Exploited Vulnerabilities Catalog. Accessed: Oct.~2025. [Online]. Available: \url{https://www.cisa.gov/known-exploited-vulnerabilities-catalog}

\bibitem{epss}
FIRST Exploit Prediction Scoring System (EPSS). Accessed: Oct.~2025. [Online]. Available: \url{https://www.first.org/epss/}

\bibitem{trivy}
Trivy: Vulnerability Scanner for Containers and IaC. Accessed: Oct.~2025. [Online]. Available: \url{https://aquasecurity.github.io/trivy/}

\bibitem{grype}
Grype: A Vulnerability Scanner for Container Images and Filesystems. Accessed: Oct.~2025. [Online]. Available: \url{https://github.com/anchore/grype}

\bibitem{swe_bench_verified}
SWE-bench Verified (background on closed-loop code repair evaluation). Accessed: Oct.~2025. [Online]. Available: \url{https://openai.com/index/introducing-swe-bench-verified/}

\bibitem{llmsecconfig}
Z. Ye, T. H. M. Le, and M. A. Babar, "LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations," in \emph{2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)}, 2025.

\bibitem{malul2024}
E. Malul, Y. Meidan, D. Mimran, Y. Elovici, and A. Shabtai, "GenKubeSec: LLM-Based Kubernetes Misconfiguration Detection, Localization, Reasoning, and Remediation," \emph{arXiv preprint arXiv:2405.19954}, 2024.

\bibitem{kubellm}
M. De Jesus, P. Sylvester, W. Clifford, A. Perez, and P. Lama, "LLM-Based Multi-Agent Framework For Troubleshooting Distributed Systems," in \emph{Proc. of the 2025 IEEE Cloud Summit}, 2025 (author's version).

\bibitem{kyverno_docs}
Kyverno Project, "Kyverno Documentation," Accessed: Oct.~2025. [Online]. Available: \url{https://kyverno.io/docs/}

\bibitem{borg}
A. Verma \emph{et al.}, "Large-scale Cluster Management at Google with Borg," in \emph{Proc. EuroSys}, 2015; supplemental SRE updates accessed Oct.~2025. [Online]. Available: \url{https://research.google/pubs/pub43438/}

\bibitem{artifacthub}
ArtifactHub Documentation. Accessed: Oct.~2025. [Online]. Available: \url{https://artifacthub.io/docs/}

\bibitem{auer2002}
P. Auer, N. Cesa-Bianchi, and P. Fischer, "Finite-time Analysis of the Multiarmed Bandit Problem," \emph{Machine Learning}, vol.~47, no.~2, pp.~235--256, 2002.

\bibitem{joseph2016}
M. Joseph, M. Kearns, J. H. Morgenstern, and A. Roth, "Fairness in Learning: Classic and Contextual Bandits," in \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, 2016.

\end{thebibliography}

%====================
% Author biographies
%====================
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{brian_mendonca_photo.png}}]{Brian Mendonca}
is an M.S.\ student at the Georgia Institute of Technology (2024--2026) focusing on secure DevOps, policy-driven remediation, and human-centered tooling for developer productivity. 

Prior to graduate study, he worked as an Aerospace Quality Engineer at BAE Systems (2024--2025) and at Tube Specialties Inc.\ (2025--present), where he led Lean/Six Sigma continuous improvement, nonconformance management, and 8D root-cause investigations supporting AS9100 compliance and on-time delivery. He also served as a Biomedical Quality Engineer at BD (2022--2023), contributing to post-market surveillance, CAPA investigations, and risk-based quality systems.

He received the B.E.\ in Mechanical Engineering (summa cum laude, GPA 3.99) from Arizona State University in 2021. His research interests include secure configuration management for cloud-native systems, program analysis for infrastructure-as-code, and data-informed quality engineering.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{vijay_madisetti_photo.png}}]{Vijay K. Madisetti}
is Professor of Cybersecurity and Privacy at the Georgia Institute of Technology. He earned his Ph.D.\ in Electrical Engineering and Computer Sciences from the University of California at Berkeley.

Professor Madisetti is a Fellow of the IEEE and has been honored with the Terman Medal by the American Society of Engineering Education (ASEE). He has authored several widely referenced textbooks on topics including cloud computing, data analytics, blockchain, and microservices, and has extensive experience in secure system architectures and privacy-preserving technologies.
\end{IEEEbiography}

\EOD

\end{document}
