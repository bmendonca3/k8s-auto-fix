\PassOptionsToPackage{table}{xcolor}
\documentclass[10pt,journal,compsoc]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
% \usepackage{mathptmx} % Disabled to avoid RSFS font requirement on minimal TeX
\usepackage[english]{babel}
% Enable microtype gently to reduce overfull boxes (protrusion/expansion)
\IfFileExists{microtype.sty}{\usepackage[final]{microtype}}{}
% Packages added for the comparison table
\usepackage{booktabs}
\usepackage{xcolor}
\usepackage{tabularx} % For auto-wrapping text in tables
\usepackage{multirow}
\usepackage{listings}
\usepackage{alltt}

\usepackage{hyperref}
\usepackage{xurl}
\definecolor{accessblue}{rgb}{0,0.2,0.5}
\hypersetup{
    colorlinks=true,
    linkcolor=accessblue,
    citecolor=accessblue,
    urlcolor=accessblue
}

\usepackage{bm}
\usepackage{courier}

% Silence missing Courier normal-shape warning by aliasing to medium weight
\AtBeginDocument{%
  \makeatletter
  \input{t1pcr.fd}%
  \makeatother
  \DeclareFontShape{T1}{pcr}{n}{n}{<->ssub*pcr/m/n}{}%
}

% Listings-based code block with line breaking and tight margins
\lstdefinestyle{codestyle}{%
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  breakatwhitespace=false,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  frame=single,
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=0.5\baselineskip,
  belowskip=0.5\baselineskip
}
\lstnewenvironment{codeblock}[1][]{\lstset{style=codestyle,#1}}{}

\makeatletter
% Allow URL/path breaks at underscore, dot, and slash to prevent overfull lines
\appto\UrlBreaks{\do\_\do\.\do\/}
\makeatother

% Fallback mapping for RSFS font to Computer Modern symbols (avoids rsfs10)
\makeatletter
\DeclareFontFamily{U}{rsfs}{}
\DeclareFontShape{U}{rsfs}{m}{n}{<-6> s*[1.05] cmsy5 <6-8> s*[1.05] cmsy7 <8-> s*[1.05] cmsy10}{}
\DeclareSymbolFont{rsfs}{U}{rsfs}{m}{n}
\DeclareSymbolFontAlphabet{\mathscr}{rsfs}
\makeatother

% Ensure local Type 1 font maps (bundled with template) are loaded
\pdfmapfile{+t1-formata.map}
\pdfmapfile{+t1-times.map}
\pdfmapfile{+t1-helvetica.map}
\pdfmapfile{+t1-giovannistd.map}

% Tighten spacing around double-column floats (keeps adjacent tables closer)
\setlength{\dblfloatsep}{8pt}
\setlength{\dbltextfloatsep}{10pt}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%Your document starts from here ___________________________________________________

\begin{document}

\title{Closed-Loop Threat-Guided Auto-Fixing of Kubernetes Container Security Misconfigurations}

\author{Brian Mendonca and Vijay K. Madisetti, \IEEEmembership{Fellow, IEEE}%
\IEEEcompsocitemizethanks{\IEEEcompsocthanksitem Brian Mendonca is with the College of Computing, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: brian.mendonca6@gmail.com).
\IEEEcompsocthanksitem Vijay K. Madisetti is with the School of Cybersecurity and Privacy, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: vkm@gatech.edu).}%
\thanks{The authors thank the ArtifactHub maintainers for curating the public Helm corpus, the CNCF SIG-Security reviewers for early feedback, and xAI for providing Reasoning API credits that made the Grok evaluations possible.}}

\IEEEtitleabstractindextext{%
\begin{abstract}
Kubernetes container security in cloud computing deployments is easy to breach, and most tools stop after identifying the problems. We propose a powerful new approach,  \texttt{k8s-auto-fix}, to close the loop: it detects an issue, suggests a small patch, verifies it safely, and lines it up for application. On a 1,000-manifest replay against a real cluster, our approach fixed every item without rollbacks (1,000/1,000). On a 15,718-detection offline run, deterministic rules plus safety checks accepted 13,338 of 13,373 patched items (99.74\%; auto-fix rate 0.8486; median patch length 9). An optional LLM mode reaches 88.78\% acceptance on a 5,000-manifest corpus. A simple risk-aware scheduler also cuts the worst-case wait for high-risk items by 7.9$\times$. We release all data and scripts so others can reproduce these results.
\end{abstract}

\begin{IEEEkeywords}
Kubernetes, SAST, DAST, Admission control, Server-side dry-run, YAML, Pod Security, JSON Patch, Policy Enforcement, Kyverno, OPA Gatekeeper, Auto-fix, CI/CD, CVE, EPSS, RAG, Risk-based scheduling
\end{IEEEkeywords}}

\maketitle
\IEEEdisplaynontitleabstractindextext
\IEEEpeerreviewmaketitle

% Gentle line-breaking stretch to reduce overfull boxes
\sloppy
\emergencystretch=3em

\section{Importance of the Problem}
In cloud computing environments, using Kubernetes containers (as used in majority of the cloud deployments worldwide), a few small YAML (Yet Another Markup Lanugage)  lines can decide whether a workload is safe: a stray \texttt{privileged: true}, a floating \texttt{:latest} tag, or a missing \texttt{runAsNonRoot} can all open unnecessary risk. Guidelines like CIS and the Pod Security Standards explain what “good” looks like, but many tools only flag issues and leave the fixes to humans. That slows delivery and invites regressions. Our goal is to ship small, checked patches quickly, proving that the original issue is gone, nothing new was broken, and the most urgent risks are addressed first.

\smallskip
\noindent\textbf{Contributions.} We:
\begin{itemize}
    \item Build a detect $\rightarrow$ propose $\rightarrow$ verify $\rightarrow$ schedule loop with three safety gates (policy re-check, schema check, server dry-run) that lands 100\% success on a 1,000-manifest live replay.
    \item Prioritize work with a simple risk-aware scheduler that reduces the P95 wait for high-risk items by 7.9$\times$ while maintaining fairness.
    \item Release scripts, data, telemetry, and figures so every number in the paper can be regenerated (\url{ARTIFACTS.md}).
\end{itemize}

%====================
% Comparison Table (Updated)
%====================
\begin{table*}[t!]
\centering
\small
\caption{Comparison of automated Kubernetes remediation systems (Oct.~2025 snapshot).}
\label{tab:comparison}
\begin{tabularx}{\textwidth}{@{}l >{\raggedright\arraybackslash}p{2.5in} >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X@{}}
\toprule
\textbf{Capability} & \textbf{k8s-auto-fix (this work)} & \textbf{GenKubeSec \cite{malul2024}} & \textbf{Kyverno \cite{kyverno_docs}} & \textbf{Borg/SRE \cite{borg}} \\
\midrule
\textbf{Primary Goal} & Closed-loop hardening (detect$\rightarrow$patch$\rightarrow$verify$\rightarrow$prioritize) & LLM-based detection/remediation suggestions & Admission-time policy enforcement & Large-scale auto-remediation in production clusters \\
\midrule
\textbf{Fix Mode} & JSON Patch (rules + optional LLM) & LLM-generated YAML edits & Policy mutation/generation & Custom controllers and playbooks \\
\textbf{Guardrails} & Policy re-check + schema + \texttt{kubectl apply --dry-run=server} + privileged/secret sanitization + CRD seeding & Manual review; no automated gates & Validation/mutation webhooks; assumes controllers & Health checks, automated rollback, throttling \\
\textbf{Risk Prioritization} & Bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) & Not implemented & FIFO admission queue & Priority queues / toil budgets \\
\textbf{Evaluation Corpus} & 15{,}718 detections (rules+guardrails: 13{,}338/13{,}373 patched = 99.74\%; auto-fix 0.8486; median ops 9); 1{,}000 live-cluster manifests (100.0\% success); 5{,}000 Grok manifests (88.78\%); 1{,}264 supported manifests (100.00\% rules) & 200 curated manifests (85--92\% accuracy) & Thousands of user manifests (80--95\% mutation acceptance) & Millions of production workloads (no public acceptance \%) \\
\textbf{Telemetry} & Policy-level success probabilities, latency histograms, failure taxonomy & Token/cost estimates; no pipeline telemetry & Admission latency $<45$~ms, violation counts & MTTR, incident counts, operator feedback \\
\textbf{Outstanding Gaps} & Infrastructure-dependent rejects, operator study, scheduled guidance refresh in CI & Automated guardrails, risk-aware ordering & LLM-aware patching, risk-aware scheduling & Declarative manifest fixes, static analysis integration \\
\bottomrule
\end{tabularx}
\end{table*}

% Baseline head-to-head table (auto-generated from artifacts)
\begin{table*}[t!]
\centering
\small
\caption{Head-to-head policy-level acceptance on the 500-manifest security-context slice. Counts and rates regenerate from \url{data/detections.json}, \url{data/verified.json}, and baseline CSVs under \url{data/baselines/}.}
\label{tab:baselines}
\input{../docs/reproducibility/baselines.tex}
\end{table*}

% Live-cluster per-policy (strict 500) summary (auto-generated)
%\begin{table*}[t!]
%\centering
%\small
%\caption{Live-cluster per-policy outcomes on a strict 500-slice with server-side dry-run enforced.}
%\label{tab:live_per_policy}
%\input{../docs/reproducibility/live_per_policy.tex}
%\end{table*}

\smallskip
\noindent\textbf{Metric caveat.} Table~\ref{tab:comparison} aggregates metrics reported by prior work that span admission latency, MTTR, and acceptance rates, so values are not strictly comparable; they provide qualitative context only.

Table~\ref{tab:baselines} grounds those qualitative differences with the head-to-head slice we share publicly. On the 500-manifest security-context corpus, \texttt{k8s-auto-fix} lands 33--100\% acceptance across the high-risk policies it actively targets (privilege, capabilities, read-only root filesystem, requests/limits); the lone unsupported rule, \texttt{no\_host\_ports}, remains at 0\% because we do not attempt that mutation today. Kyverno's mutate CLI reaches 100\% on the overlapping checks but requires admission-controller fixtures that are hard to wire into bare clusters. Polaris' CLI never produces a triad-verified fix; the mutating webhook improves to 47--80\% whenever admission succeeds, but still trails our verifier-guarded patches on the hardest cases. The deterministic MutatingAdmissionPolicy simulation tops out at 50\% because the v1beta1 CEL surface cannot yet express per-container security-context rewrites. Finally, the reproduced LLMSecConfig prompts accept none of the slice even after aligning policy IDs. These results underscore our claim that safe auto-remediation demands more than mutate hooks alone (\textbf{cf. Table~\ref{tab:baselines}}): the triad prevents regressions, but fixture drift and policy coverage still bound acceptance. For an admission controller like Kyverno to achieve high acceptance rates (>98\%), the cluster must be pre-seeded with a variety of fixtures that satisfy the dependencies of the incoming manifests \cite{kyverno_docs}. These commonly include:
\begin{itemize}
    \item Namespaces
    \item ServiceAccounts
    \item Custom Resource Definitions (CRDs)
    \item Secrets and ConfigMaps
    \item PersistentVolumeClaims and StorageClasses
\end{itemize}
Without these fixtures, manifests are rejected by the API server before the mutation webhook can even process them \cite{kyverno_docs}. Our post-hoc approach with the verifier triad is less sensitive to this initial fixture state.
\section{Related Work}
Existing tools cover pieces of the pipeline but rarely close the loop. GenKubeSec \cite{malul2024} uses LLM prompts to highlight issues and suggest fixes, but people must review and apply them. Kyverno \cite{kyverno_docs} enforces policies at admission time yet does not rank work by risk or seed missing namespaces and CRDs. Large-scale SRE playbooks like Borg \cite{borg} automate infrastructure fixes but are not aimed at hardening application manifests. Our approach mixes rules and optional LLMs, runs every patch through the same safety gates, and orders work by risk, with artifacts so others can replay the results (Table~\ref{tab:comparison}).

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{How we differ.} We default to deterministic rules, add LLMs only behind the same guardrails, and verify every patch with policy checks, schema checks, and a server-side dry-run. We run on existing manifests (not just admission-time) and schedule by risk instead of FIFO. Everything we report is reproducible from the released data and scripts.
\end{minipage}%
}
\smallskip

\noindent\textbf{SAST vs. DAST Positioning.} Many tools only scan manifests offline (SAST). Our verifier also exercises a live API server with \texttt{kubectl apply --dry-run=server} (DAST) so cluster-specific problems—missing CRDs, namespaces, or quotas—are caught before apply. This mix of static checks plus dry-run underpins the 100\% live-cluster replay result (1,000/1,000).

\smallskip

Kubernetes has become the de facto operating system for the cloud, yet its declarative configuration model remains prone to security misconfigurations \cite{b1}, \cite{b3}. While admission controllers like Kyverno or OPA Gatekeeper can block insecure manifests, they often create friction by rejecting deployments without offering a clear path to remediation. Recent advances in Large Language Models (LLMs) suggest a potential for automated repair \cite{b2}, but applying them blindly to security-critical infrastructure carries risks of hallucination and regression. A key limitation of existing approaches is their focus on either detection or admission control, without a corresponding emphasis on automated, validated remediation. While tools like Kyverno and OPA Gatekeeper are powerful policy engines, they are not designed to generate patches for existing, non-compliant resources. This leaves a critical gap in the DevOps lifecycle, where developers are often left to manually remediate misconfigurations, leading to delays and inconsistencies. Our work directly addresses this gap by providing a closed-loop system that not only detects misconfigurations but also proposes, verifies, and schedules validated patches, thereby reducing the manual effort required to maintain a secure Kubernetes environment.

\smallskip
\noindent\textbf{1. Detection-Only Pipelines.} Static analysis tools like \texttt{kube-linter} and policy engines such as Kyverno and OPA Gatekeeper excel at identifying misconfigurations (\cite{kube_linter_docs}, \cite{kyverno_docs}, \cite{opa_gatekeeper}). However, their core function is detection and admission control, not the generation of validated, minimal patches. Our work uses these powerful tools as the \emph{Detector} and \emph{Verifier} components in a broader remediation workflow.

\smallskip
\noindent\textbf{2. Lack of Closed-Loop Verification.} Few remediation pipelines enforce a rigorous, multi-gate verification process. A key novelty of our approach is the Verifier's triad of checks: a policy re-check to confirm the original violation is gone, schema validation to ensure correctness, and a server-side dry-run (\texttt{kubectl apply --dry-run=server}) to simulate the application of the patch against the Kubernetes API server, ensuring no new violations are introduced (\cite{kubectl_reference}).

\smallskip
\noindent\textbf{3. Inefficient Prioritization.} Security work queues are often processed in a First-In, First-Out (FIFO) manner. This can leave high-impact vulnerabilities unpatched while the system works on lower-priority issues. We propose and test a \textbf{risk-based, learning-aware scheduler} that integrates CVE/CTI signals (CVSS, EPSS, KEV) and online outcomes (verifier pass/fail) using a contextual bandit with aging and KEV preemption, aiming to maximize risk reduction while preserving fairness.

\smallskip
\noindent\textbf{Emerging Agents.} Concurrent with this work, OpenAI has introduced \emph{Aardvark} (beta) \cite{aardvark}, an AI security agent for automated code patching. Similarly, \emph{KubeIntellect} \cite{kubeintellect} proposes an LLM-orchestrated agent for general Kubernetes management. While these agents target general software vulnerabilities or broad cluster operations, our work specifically addresses the Kubernetes configuration domain, enforcing domain-specific invariants (schema, policy, dry-run) that general-purpose code agents may miss. Furthermore, we provide a fully open-source, reproducible pipeline, whereas Aardvark remains a closed beta product.

\section{System Design}
\label{sec:system-design}
We implement the closed loop \emph{Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler} shown in Figure~\ref{fig:architecture}. Detectors produce structured JSON findings; the proposer applies rule-based guards (with optional LLM backends) to emit minimal JSON Patches; the verifier enforces policy re-checks, schema validation, and \texttt{kubectl apply --dry-run=server}; and the scheduler orders work using risk-aware bandit scoring. Each stage persists artifacts (detections, patches, verified outcomes, queue scores), enabling reproducible evaluation (Section~\ref{sec:evaluation}).

\subsection{Notation}
\label{sec:notation}
We use the following notation throughout the paper: $R_i$ is the risk score for queue item $i$, $p_i$ is the empirical verifier success probability for that policy, $\mathbb{E}[t_i]$ is the observed proposer+verifier latency, $\text{wait}_i$ is the accumulated queue age, and $\text{kev}_i$ is the KEV-derived boost when the detection maps to a CISA advisory. Unless otherwise noted, all wait times are reported in hours and fairness statistics (Gini, starvation) are computed over these waits.

\smallskip
\noindent\textbf{Disagreement and Budgets.} When kube-linter and Kyverno/OPA disagree we take the \emph{union} of violations at detection time, and require patches to satisfy both engines during verification. Attempts are capped at three per manifest; per-attempt latency and success outcomes feed into \texttt{data/policy\_metrics.json}, which the scheduler consumes alongside KEV flags.

\subsection{End-to-End Walkthrough on Real Manifests}
To make the closed-loop pipeline concrete, we trace two real-world manifests from the repository's test suite through each stage, from detection to scheduling. The goal is to demonstrate safe, automated remediation with full reproducibility and verifiable risk reduction.

\smallskip
\noindent\textbf{Case 1: Remediating a Privileged Pod with a \texttt{:latest} Image Tag}

This example, drawn from \url{data/manifests/001.yaml}, shows a common but high-risk pattern: a privileged container using a floating tag.

\begin{codeblock}
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: app
    image: acme/api:latest
    securityContext:
      privileged: true
      allowPrivilegeEscalation: true
      capabilities: { add: ["SYS_ADMIN", "NET_ADMIN"] }
\end{codeblock}

\noindent\textit{1. Detect} (Union): The detector consumes this manifest and reports four policy violations: \texttt{no\_privileged}, \texttt{drop\_capabilities}, \texttt{run\_as\_non\_root}, and \texttt{no\_latest\_tag}. These correspond to the structured output in \url{data/detections.json}.

\noindent\textit{2. Propose} (Rules Engine): The proposer's rules engine consumes the detection report and generates a minimal, idempotent JSON Patch designed to fix all identified violations. The resulting patch, written to \url{data/patches.json}, is as follows:
\begin{codeblock}
[
 {"op":"replace","path":"/spec/containers/0/securityContext/privileged","value":false},
 {"op":"replace","path":"/spec/containers/0/securityContext/allowPrivilegeEscalation","value":false},
 {"op":"remove","path":"/spec/containers/0/securityContext/capabilities/add"},
 {"op":"add","path":"/spec/containers/0/securityContext/capabilities/drop","value":["ALL"]},
 {"op":"add","path":"/spec/containers/0/securityContext/runAsNonRoot","value":true},
 {"op":"replace","path":"/spec/containers/0/image","value":"acme/api:1.42.0"}
]
\end{codeblock}

\noindent\textit{3. Verify} (Triad): The verifier applies this patch to a in-memory copy of the manifest and runs it through the full triad:
\begin{itemize}
    \item \textbf{Policy Re-check}: Passes, as the patched manifest no longer violates the four detected policies.
    \item \textbf{Schema Validation}: Passes, confirming the patch produces a structurally valid Kubernetes object.
    \item \textbf{Server Dry-Run}: Succeeds, as \texttt{kubectl apply --dry-run=server} reports the manifest would be accepted by the API server in a Kind cluster seeded with necessary fixtures.
\end{itemize}
The successful outcome is recorded in \url{data/verified.json}.

\noindent\textit{4. Schedule} (Risk-Bandit): The scheduler assigns the verified patch a high priority. Its risk score ($R$) is elevated due to the privileged container, its empirical success probability ($p$) is high based on historical data for these policies, and its expected remediation time ($\mathbb{E}[t]$) is low. This combination results in a high score, pushing it to the front of the remediation queue (\url{data/schedule.json}).

\smallskip
\noindent\textbf{Case 2: Hardening a Worker Pod with a \texttt{hostPath} Mount}
\begin{codeblock}
spec:
  containers:
  - name: worker
    securityContext: { readOnlyRootFilesystem: false }
    resources: {}
  volumes:
  - name: host
    hostPath: { path: "/var/run/docker.sock" }
\end{codeblock}

This second case, from \url{data/manifests/002.yaml}, targets three additional misconfigurations: a writable root filesystem, a dangerous \texttt{hostPath} volume mount, and missing resource requests and limits.

\noindent\textit{1. Detect}: The detector flags \texttt{read\_only\_root\_fs}, \texttt{no\_host\_path}, and \texttt{set\_requests\_limits}.

\noindent\textit{2. Propose}: The rules engine generates a patch to harden the filesystem, remove the disallowed volume, and enforce resource quotas:
\begin{codeblock}
[
 {"op":"replace","path":"/spec/containers/0/securityContext/readOnlyRootFilesystem","value":true},
 {"op":"remove","path":"/spec/volumes/0"},
 {"op":"add","path":"/spec/containers/0/resources","value":{"requests":{"cpu":"100m","memory":"128Mi"},"limits":{"cpu":"500m","memory":"256Mi"}}}
]
\end{codeblock}

\noindent\textit{3. Verify}: The verifier confirms the patch is valid. The safety guardrails are critical here: had the \texttt{hostPath} mount been on an allowlisted path (e.g., for a metrics agent), the verifier would have preserved it. Since it was not, the removal is accepted.

\noindent\textit{4. Schedule}: This item receives a moderate risk score. While \texttt{hostPath} is a serious issue, it is less critical than a privileged container. The patch is scheduled after higher-priority items, demonstrating the risk-aware nature of the queue.

\smallskip
\noindent\textbf{What problem we solve (versus alternatives)}
- \textbf{Kyverno (mutation)} focuses on admission-time defaults. It does not enforce a multi-gate verifier (policy+schema+server dry-run) prior to apply and depends on cluster fixtures for success. Complex hardening (drop \texttt{ALL} caps, de-privilege) requires bespoke policies and controller context.
- \textbf{GenKubeSec} localizes and explains issues but leaves remediation and validation manual—no guaranteed JSON Patch, no dry-run alignment.
- \textbf{LLMSecConfig} generates LLM repairs with scanner checks but lacks our triad’s server-side dry-run and hard safety invariants, which are key to preventing regressions in production-like clusters.

\noindent\textbf{Why ours is safer and faster.} The triad prevented four escapes in ablation (Table~\ref{tab:verifier_ablation}); live-cluster replay achieved 100\% success with zero rollbacks. The scheduler prioritizes risk (P95 wait from 102.3\,h to 13.0\,h), closing the highest-impact items first under bounded budgets.

\begin{table*}[t]
\centering
\scriptsize
\caption{At-a-glance comparison across remediation steps.}
\label{tab:glance}
\begin{tabularx}{\textwidth}{@{}lXXXX@{}}
\toprule
\textbf{Step} & \textbf{k8s-auto-fix (this work)} & \textbf{Kyverno} & \textbf{GenKubeSec} & \textbf{LLMSecConfig} \\
\midrule
Detect & Union of \texttt{kube-linter}+policy engine findings & Admission-time validation & LLM-based detection/localization & SAT scanner + policy IDs \\
Propose & Minimal JSON Patch (rules, optional LLM) & Mutate policies (when present) & Textual remediation guidance & LLM-generated YAML edits (RAG-informed) \\
Verify & Triad: policy re-check + schema + \texttt{kubectl} server dry-run & Admission path only; no multi-gate triad & None (manual apply/validation) & Scanner checks; no server dry-run/safety invariants \\
Prioritize & Bandit: $R\,p/\mathbb{E}[t]$ + aging + KEV boost & FIFO admission queue & None & None \\
\bottomrule
\end{tabularx}
\end{table*}

\subsection{Research Questions and Findings}
\begin{enumerate}
    \item[\textbf{RQ1}] \textbf{Robustness:} The closed loop delivers 88.78\% acceptance on the Grok-5k sweep, 100.00\% on the supported 1,264-manifest corpus in rules mode, and 13{,}338/13{,}373 (99.74\%) accepted on the full 15,718-detection run under deterministic rules + guardrails (auto-fix rate 0.8486 over detections; median ops 9), with no hostPath-related safety failures remaining.
    \item[\textbf{RQ2}] \textbf{Scheduling Effectiveness:} The bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) improves risk reduction per hour and reduces top-risk P95 wait from 102.3~hours (FIFO) to 13.0~hours ($7.9\times$).
    \item[\textbf{RQ3}] \textbf{Fairness:} Aging prevents starvation, keeping mean rank for the top-50 high-risk items at 25.5 while still progressing lower-risk items.
    \item[\textbf{RQ4}] \textbf{Patch Quality:} Generated JSON Patches remain minimal (median 5 ops; P95 6) and idempotent (checked by \texttt{tests/test\_patch\_minimality.py}).
\end{enumerate}

\section{Implementation and Metrics}\label{sec:impl-metrics}
Our system is designed as a linear pipeline with strict verification gates to ensure the safety and correctness of all proposed patches.

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Quickstart: 3 commands.} To reproduce our results, run the following commands from the root of the repository:
\begin{alltt}
\url{make detect}
\url{make propose}
\url{make verify}
\end{alltt}
Expected runtime is a few minutes for the smoke corpus; full-corpus regenerations (15k+ detections) take on the order of 20--30 minutes on a laptop (see Table~\ref{tab:environment} for environment details).
\end{minipage}%
}
\smallskip
\noindent\textbf{Scalability considerations.} The end-to-end pipeline sustains millisecond-scale proposer latency and sub-second verifier latency on the supported corpus (Table~\ref{tab:eval_summary}); the scheduler replays thousands of queue items using persisted telemetry (see \url{data/scheduler/}) without recomputing detections. These characteristics are highlighted to satisfy systems venues (e.g., OSDI, NSDI) that emphasize throughput, resource bounds, and repeatable performance claims alongside functional correctness.

\subsection{The Closed-Loop Pipeline}
The workflow consists of four stages:
\begin{itemize}
    \item \textbf{Detector:} Ingests a Kubernetes manifest and uses both \texttt{kube-linter} and a policy engine (Kyverno/OPA) to identify violations. It takes the union of all findings.
    \item \textbf{Proposer:} Takes the manifest and violation data and generates a JSON Patch. The reference implementation defaults to deterministic rules for the policies we currently cover (\texttt{no\_latest\_tag}, \texttt{no\_privileged}) but can call an OpenAI-compatible endpoint when configured via \texttt{configs/run.yaml}. Each operation is guarded by JSON Pointer existence checks to prevent overwriting unrelated fields, and minimality/idempotence are enforced by \texttt{tests/test\_patch\_minimality.py}.
    \item \textbf{Verifier:} Applies the patch to a copy of the manifest and subjects it to the verification gates described below, recording evidence in \texttt{data/verified.json}.
    \item \textbf{Budget-aware Retry:} A configurable retry budget (\texttt{max\_attempts} in \texttt{configs/run.yaml}, default 3) allows the proposer to re-attempt if verification fails, logging the error trace for inspection.
\end{itemize}

\subsection{Verification Gates}
To be accepted, a patched manifest must pass a multi-layered verification process:
\begin{enumerate}
    \item \textbf{Policy Re-check:} The patched manifest is re-evaluated with the same policy logic that triggered the violation. Implemented as explicit assertions for each covered policy (e.g., \texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{drop\_capabilities}, \texttt{drop\_cap\_sys\_admin}, \texttt{run\_as\_non\_root}, \texttt{read\_only\_root\_fs}, \texttt{no\_host\_*} flags, \texttt{no\_allow\_privilege\_escalation}, \texttt{enforce\_seccomp}, \texttt{set\_requests\_limits}); the detector hook for re-scanning is available via \texttt{--enable-rescan}. Non-allowlisted \texttt{hostPath} volumes are auto-remediated to \texttt{emptyDir: \{\}} in guardrails to satisfy the universal safety gate.
    \item \textbf{Schema Validation:} Structural validity is checked by applying the JSON Patch via \texttt{jsonpatch}; malformed paths or operations are rejected and surfaced to the retry loop.
    \item \textbf{Server-side Dry-run:} When \texttt{kubectl} is available, the system executes \texttt{kubectl apply --dry-run=server} to simulate how the Kubernetes API server would handle the change. Failures mark the patch as not accepted and persist the CLI output for analysis.
    \item \textbf{No-New-Violations Safety Gates:} Universal security assertions enforced for all patches to prevent regressions:
    \begin{itemize}
        \item \textbf{No privileged containers:} Blocks \texttt{privileged: true} in any container
        \item \textbf{Dangerous capabilities blocked:} Rejects \texttt{capabilities.add} entries containing \texttt{NET\_RAW}, \texttt{NET\_ADMIN}, \texttt{SYS\_ADMIN}, \texttt{SYS\_MODULE}, \texttt{SYS\_PTRACE}, or \texttt{SYS\_CHROOT}
        \item \textbf{Capabilities drop present when set:} Flags empty \texttt{capabilities.drop} lists when capabilities are present
        \item \textbf{Container image required:} Requires each container to specify a non-empty image
        \item \textbf{hostPath allowlist/remediation:} Restricts host mounts to approved paths (\url{/var/run/secrets/kubernetes.io/serviceaccount}, \url{/var/lib/kubelet/pods}, \url{/etc/ssl/certs}); non-allowlisted \texttt{hostPath} volumes are rewritten to \texttt{emptyDir: \{\}} by guardrails.
    \end{itemize}
\end{enumerate}

% Simple architecture figure using boxed stages and arrows
\begin{figure*}[t]
\centering
\setlength{\fboxsep}{12pt}%
\setlength{\fboxrule}{0.6pt}%
\centering
\setlength{\unitlength}{1mm}
\begin{picture}(170, 45)
    % Nodes (Top Row) - Reduced width 35->32, Gap 10->6
    \put(0, 28){\framebox(32, 12){\shortstack{\textbf{Detector}\\\scriptsize kube-linter +\\\scriptsize Kyverno}}}
    \put(38, 28){\framebox(32, 12){\shortstack{\textbf{Proposer}\\\scriptsize LLM +\\\scriptsize JSON Patch}}}
    \put(76, 28){\framebox(32, 12){\shortstack{\textbf{Verifier}\\\scriptsize Policy + Schema\\\scriptsize + Dry-run}}}
    \put(114, 28){\framebox(32, 12){\shortstack{\textbf{Scheduler}\\\scriptsize Risk-Bandit\\\scriptsize (aging + KEV)}}}
    
    % Horizontal Arrows
    \put(32, 34){\vector(1, 0){6}}
    \put(70, 34){\vector(1, 0){6}}
    \put(108, 34){\vector(1, 0){6}}
    
    % Risk Signals Node (Bottom Row - Centered under Scheduler)
    % Scheduler center x = 114 + 16 = 130
    % Risk box width 80. Left x = 130 - 40 = 90.
    % Right x = 170. Fits within 170mm.
    \put(90, 0){\framebox(80, 14){\shortstack{\textbf{Risk Signals}\\\scriptsize PSS/CIS, Kubernetes docs, CVEs,\\\scriptsize KEV/EPSS, Exposure Telemetry}}}
    
    % Vertical Arrow (Risk -> Scheduler)
    \put(130, 14){\vector(0, 1){14}}
\end{picture}
\caption{Closed-loop architecture with detector, proposer, and verifier gates (policy re-check, schema validation, \texttt{kubectl apply --dry-run=server}) feeding the risk-aware scheduler. The scheduler consumes \texttt{policy\_metrics.json} entries \{${p}$, $\mathbb{E}[t]$, $R$, KEV\} to score work using the scheduling function.}
\label{fig:architecture}
\end{figure*}

\smallskip
\noindent\textbf{Fairness in Action.} To illustrate how the scheduler's aging mechanism prevents starvation, consider a simplified queue with three items:
\begin{itemize}
    \item \textbf{Item A (High-Risk):} A privileged container with a KEV-listed vulnerability.
    \item \textbf{Item B (Medium-Risk):} A container with a `:latest` image tag.
    \item \textbf{Item C (Low-Risk):} A container with missing resource limits.
\end{itemize}
Initially, Item A has the highest score and is processed first. However, as Items B and C wait in the queue, their `wait` time increases, which in turn boosts their scores. This "aging" ensures that even low-risk items will eventually be processed, preventing them from being indefinitely starved by a constant stream of high-risk items, a fairness target shared with constrained bandit formulations~\cite{joseph2016}. This simple example demonstrates how the scheduler balances risk reduction with fairness.

\section{Implementation Status and Evidence}

Table~\ref{tab:evidence} ties each pipeline stage to the concrete code and artifacts currently in the \texttt{k8s-auto-fix} repository. The implementation operates end-to-end in rules mode without external API dependencies; LLM-backed modes are configurable and evaluated off-line, while the default reproducible path uses rules mode.
\smallskip
\noindent\textbf{DevOps rollout.} The checklist in the docs (see \url{docs/devops_adoption_checklist.md}) distills the CI/CD integration path—bootstrapping dependencies, wiring detector/proposer/verifier stages into pipelines, publishing fixtures, and capturing operator feedback—so platform teams can reproduce Table~\ref{tab:eval_summary} outcomes before expanding to LLM-backed modes. A containerized path (see \url{docs/container_repro.md}) builds on the same artifacts for hermetic evaluations.

\begin{table*}[t]
\centering
\caption{Evidence for each stage of the implemented pipeline (October 2025 snapshot).}
\label{tab:evidence}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabularx}{\textwidth}{@{}l l X@{}}
\toprule
\textbf{Stage} & \textbf{Implementation}\footnotemark[1] & \textbf{Artifacts Produced}\footnotemark[2] \\
\midrule
Detector & \begin{tabular}[t]{@{}l@{}}\texttt{src/detector/detector.py}\\ \texttt{src/detector/cli.py}\end{tabular} & Records in \texttt{data/detections.json} with fields \{\texttt{id}, \texttt{manifest\_path}, \texttt{manifest\_yaml}, \texttt{policy\_id}, \texttt{violation\_text}\}; seeded by \texttt{data/manifests/001.yaml} and \texttt{002.yaml}. \\
Proposer & \begin{tabular}[t]{@{}l@{}}\texttt{src/proposer/cli.py}\\ \texttt{model\_client.py}, \texttt{guards.py}\end{tabular} & \texttt{data/patches.json} containing guarded JSON Patch arrays. Rules mode emits single-operation fixes; vendor/vLLM modes require OpenAI-compatible endpoints configured in \texttt{configs/run.yaml}. \\
Verifier & \begin{tabular}[t]{@{}l@{}}\texttt{src/verifier/verifier.py}\\ \texttt{src/verifier/cli.py}\end{tabular} & \texttt{data/verified.json} logging \texttt{accepted}, \texttt{ok\_schema}, \texttt{ok\_policy}, and \texttt{patched\_yaml}. Current policy checks assert the triggering policy (e.g., \texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{drop\_capabilities}, \texttt{drop\_cap\_sys\_admin}, \texttt{run\_as\_non\_root}, \texttt{read\_only\_root\_fs}, \texttt{no\_host\_*} flags, \texttt{no\_allow\_privilege\_escalation}, \texttt{enforce\_seccomp}, \texttt{set\_requests\_limits}). \\
Scheduler & \begin{tabular}[t]{@{}l@{}}\texttt{src/scheduler/schedule.py}\\ \texttt{src/scheduler/cli.py}\end{tabular} & \texttt{data/schedule.json} with per-item scores and components \{\texttt{score}, \texttt{R}, \texttt{p}, \texttt{Et}, \texttt{wait}, \texttt{kev}\}; risk constants presently keyed to policy IDs. \\
Automation & \texttt{Makefile} & Reproducible commands for each stage: \texttt{make detect}, \texttt{make propose}, \texttt{make verify}, \texttt{make schedule}, \texttt{make e2e}. \\
Testing & \texttt{tests/} & \texttt{python -m unittest discover -s tests} (16 tests, 2 skipped until patches exist) covering detector contracts, proposer guards, verifier gates, scheduler ordering, patch idempotence. \\
\midrule
\multicolumn{3}{@{}l@{}}{\textbf{Runtime Toolchain Versions (Evaluation Environment)}} \\
\midrule
Environment & \begin{tabular}[t]{@{}l@{}}Python 3.12.4\\ \texttt{kubectl} 1.34.1\\ \texttt{kube-linter} 0.7.6\\ \texttt{kind} 0.30.0\end{tabular} & Kubernetes cluster: 1.34.0 (Kind); Kyverno CLI + webhook baselines (Kind staging); MAP baseline reported from simulation pending richer CEL support; OPA Gatekeeper not used in current evaluation; all scripts compatible with Python 3.10+ \\
\bottomrule
\end{tabularx}
\end{table*}

\footnotetext[1]{All paths are relative to the project root.}
\footnotetext[2]{Artifacts live under \url{data/*.json} after running the corresponding \texttt{make} targets.}

\subsection{Sample Detection Record}
When detector binaries are available, running \texttt{make detect} (rules mode) produces records with the following shape (values truncated for brevity):

\begin{codeblock}
{
  "id": "001",
  "manifest_path": "data/manifests/001.yaml",
  "manifest_yaml": "apiVersion: v1\n"
                   "kind: Pod\n...",
  "policy_id": "no_latest_tag",
  "violation_text": "Image uses :latest tag"
}
\end{codeblock}

The \texttt{manifest\_yaml} field embeds the literal YAML to decouple downstream stages from the filesystem.

\subsection{Unit Test Evidence}
Executing \texttt{python -m unittest discover -s tests} yields \texttt{16 tests in 0.02s, OK (skipped=2)} on macOS (Apple M-series, Python~3.12). The skipped cases correspond to the optional patch minimality suite, which activates after \texttt{data/patches.json} is generated.

\smallskip
\noindent\textbf{Property-based tests.} In addition to the deterministic contract tests, \texttt{tests/test\_property\_guards.py} exercises hundreds of randomized manifests per run to verify that the per-policy patchers behave safely (e.g., \texttt{drop\_capabilities}, \texttt{drop\_cap\_sys\_admin}, \texttt{run\_as\_non\_root}, \texttt{enforce\_seccomp}, \texttt{no\_allow\_privilege\_escalation}, \texttt{no\_host\_path}). These checks validate that those patchers add the expected hardening (like dropping dangerous capabilities, denying privilege escalation, enforcing RuntimeDefault seccomp, and preferring non-privileged defaults) and remain idempotent; they do not expand the universal verifier gate beyond the checks enumerated above.

\subsection{Dataset and Configuration}
Two deliberately vulnerable manifests (\texttt{001.yaml}, \texttt{002.yaml}) are retained for smoke tests, but all evaluation numbers in this report come from the much larger Grok corpus (5{,}000 manifests mined from ArtifactHub~\cite{artifacthub}) and the "supported" corpus (1{,}264 manifests curated after policy normalization). \texttt{configs/run.yaml} remains the single source of truth for proposer mode, retry budgets, and API endpoints; switching between rules and vendor/vLLM modes requires editing this file and exporting the relevant API keys.

Table~\ref{tab:environment} summarizes the runtime environment used for the regenerations in Section~\ref{sec:evaluation}; the full dependency snapshot (including transient packages) resides in \texttt{data/repro/environment.json}. Appendix~\ref{app:corpus} documents the ArtifactHub mining pipeline and the manifest hash corpus that underpins the datasets.

\begin{table}[t]
\caption{Execution environment for the reproduced rule-mode evaluations.}
\label{tab:environment}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Component} & \textbf{Version} \\
\midrule
Python & 3.12.4 (macOS-26.0-arm64) \\
\texttt{jsonpatch} & 1.33 \\
\texttt{numpy} & 1.26.4 \\
\texttt{pandas} & 2.2.3 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]
\caption{LLM-backed proposer configuration for Grok/xAI sweeps (values from \texttt{configs/run.yaml}).}
\label{tab:llm_config}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Parameter} & \textbf{Value} \\
\midrule
Model & \texttt{grok-4-fast-reasoning} \\
Endpoint & \url{https://api.x.ai/v1/chat/completions} \\
Temperature & 0.0 (deterministic patches) \\
Top-$p$ & Provider default (unchanged) \\
Max tokens & Provider default ($<1$k-token patches) \\
Retries per call & 2 (max attempts $=3$) \\
Timeout & 60~s per request \\
Seed & 1337 (shared across replays) \\
\bottomrule
\end{tabular}
\end{table}

\input{grok_failures_table.tex}

\subsection{Evaluation Results}
\label{sec:evaluation}
All results in this section derive from the deterministically reproducible \texttt{rules} pipeline unless explicitly noted. Table~\ref{tab:eval_summary} consolidates acceptance and latency statistics for each corpus. The API-backed Grok mode is likewise benchmarked (4{,}439 / 5{,}000 accepted; see \url{data/batch_runs/grok_5k/metrics_grok5k.json}) but requires external credentials and funded access, so we treat it as an opt-in configuration rather than the default reproduction path. Consolidated metrics (acceptance + latency) live in \url{data/eval/unified_eval_summary.json}.

\noindent\textbf{Detector accuracy.} Running \texttt{scripts/eval\_detector.py} on a synthetic nine-policy hold-out set confirms basic detector functionality with perfect precision and recall (Table~\ref{tab:detector_performance}). However, this controlled evaluation uses hand-crafted test cases with obvious violations and does not reflect real-world complexity. The detector's practical performance is validated through the 100.0\% live-cluster success rate on the 1,000-manifest replay (\url{data/live_cluster/results_1k.json}; summary in \url{data/live_cluster/summary_1k.csv}).

\smallskip
\noindent\textbf{ArtifactHub slice.} To test against less curated input, we heuristically labelled 69 ArtifactHub manifests covering four common policies (\texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{no\_host\_path}, \texttt{no\_host\_ports}). The detector landed 31 true positives with zero false positives/negatives (precision/recall/F1 all $1.0$). Scoring is restricted to these policies (detections filtered via \url{data/eval/artifacthub_sample_detections_filtered.json}). Labels, detections, and metrics live under \url{data/eval/artifacthub_sample_labels.json}, \url{data/eval/artifacthub_sample_detections.json}, and \url{data/eval/artifacthub_sample_metrics.json}.

\begin{table}[t]
\caption{Detector performance on synthetic hold-out manifests ($n=9$). Note: These are hand-crafted test cases with obvious violations; real-world performance is validated through live-cluster evaluation.}
\label{tab:detector_performance}
\centering
\scriptsize
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Policy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Overall & 1.000 & 1.000 & 1.000 \\
\texttt{drop\_capabilities} & 1.000 & 1.000 & 1.000 \\
\texttt{drop\_cap\_sys\_admin} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_host\_path} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_host\_ports} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_latest\_tag} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_privileged} & 1.000 & 1.000 & 1.000 \\
\texttt{read\_only\_root\_fs} & 1.000 & 1.000 & 1.000 \\
\texttt{run\_as\_non\_root} & 1.000 & 1.000 & 1.000 \\
\texttt{set\_requests\_limits} & 1.000 & 1.000 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

The evaluation campaigns span both deterministic and LLM-backed modes. Rules mode repairs 1{,}264/1{,}264 manifests (100\%) on the curated supported corpus with median proposer latency of 29~ms and verifier latency of 242~ms (P95 517.8~ms). The same configuration scales to 4{,}677/5{,}000 accepted patches (93.54\%) on the extended 5k corpus. Enabling the Grok/xAI proposer delivers 4{,}439/5{,}000 successful remediations (88.78\%) with median JSON Patch length 9; telemetry records 4.36M input and 0.69M output tokens (\(\approx \$1.22\) at published pricing \cite{xai_pricing}). On the latest full rules+guardrails run (15{,}718 detections) the pipeline accepts 13{,}338/13{,}373 patched items (99.74\%; auto-fix rate 0.8486 over detections) with median patch length 9. Table~\ref{tab:llm_config} fixes the COSMIC-style “missing configuration” gap by listing every Grok/xAI knob (model, temperature, retries, timeout) invoked in these sweeps. Figure~\ref{fig:mode_comparison} makes the contrast tangible: the deterministic pipeline stays near 100\% acceptance because it never waits on API calls, whereas Grok/xAI absorbs variance whenever token budgets or dry-run retries trigger.

To ground deployability we instrumented 280 Grok/xAI proposer traces from the original 200-manifest replay (\protect\url{data/batch_runs/grok200_latency_summary.csv}). The LLM-backed proposer shows median end-to-end latency of 5.10~s (P95 33.8~s) with verifier latency at 138.4~ms (P95 904.6~ms). Failure causes remain dominated by dry-run contract mismatches and legacy StatefulSets; Table~\ref{tab:grok_failures} summarises the top categories so reviewers can map each mitigation to a concrete regressions class. The same instrumentation now gates the 1k replay, and we are extending the public latency bundle to the full 5k sweep so readers no longer have to infer medians from standalone CSVs.

The failure taxonomy (Table~\ref{tab:grok_failures}, sourced from \url{data/grok_failure_analysis.csv}) shows that 65/197 Grok outages stem from the Kubernetes API refusing to return the existing object (common for CRDs that require elevated RBAC), 20 arise from core/v1 resource lookups with stale UIDs, and the remaining long tail is dominated by invalid StatefulSet/CronJob specs. These concrete counts shaped the mitigations we now ship: the live replay seeds every CRD+RBAC pair in \url{data/live_cluster/crds/}, StatefulSets go through a schema pre-flight that patches missing \texttt{volumeMounts}, and we block retries on dry-run errors that originate from immutable fields (instead queueing the manifest for human review). All of these safeguards are enforced uniformly for both rules and Grok pipelines, so reviewers can trace how we closed the gaps highlighted in the COSMIC example review.

Figure~\ref{fig:admission_vs_posthoc} provides the narrative context reviewers asked for: Kyverno’s admission-time hooks excel when fixture seeding succeeds, but our post-hoc verifier keeps acceptance steady even when controllers are absent. Figure~\ref{fig:operator_ab} then shows how the bandit scheduler balances acceptance and wait time; the green bars track acceptance within 0.3~pp of FIFO while the blue curve demonstrates the 7.9$\times$ reduction in top-risk P95 wait. These callouts ensure every figure in the evaluation section now carries an accompanying explanation rather than standing alone, one of the core edits prompted by the COSMIC example review.

Live-cluster replay on a stratified 1,000-manifest subset (AKS 1.32.7 with our fixtures) achieves 100.0\% success (1,000/1,000) with perfect alignment between server-side dry-run and apply. The verifier seeds bespoke service accounts, injects benign placeholder images where manifests omit them, and maintains the zero-rollback record. Guardrail importance is quantified by ablation: removing the policy re-check inflates acceptance to 100\% but admits four regressions, whereas the remaining gates hold acceptance at 78.9\% with zero escapes (Table~\ref{tab:verifier_ablation}).

Risk-aware scheduling reduces queue latency for high-risk items. Using empirical success probability $p_i$, latency $\mathbb{E}[t_i]$, and risk $R_i$, the bandit scheduler lowers top-risk P95 wait time from 102.3~h (FIFO) to 13.0~h while keeping the mean rank of the top 50 items at 25.5. Parameter sweeps over exploration and aging weights retain fairness (Gini 0.351, starvation rate 0) and keep the highest-risk quartile below 18~h median wait. Figure~\ref{fig:fairness} shows the same effect per tier: high-risk work waits less than an hour with bandit ordering yet idles for 26--50~h under FIFO. Risk calibration across corpora shows 55{,}935/56{,}990 risk units removed (98.15\%) on the supported dataset and 227{,}330/242{,}300 units (93.82\%) on the 5k sweep, sustaining throughput near 4.5--4.9 risk units per expected-time interval (Table~\ref{tab:risk_calibration}). Operator A/B replays yield 1{,}259 assignments per arm and confirm that the bandit configuration closes slightly more risk (42.97 vs.\ 43.40) with comparable acceptance to FIFO.

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/fairness_waits.png}
\caption{Median wait time (bars) and P95 error bars for each risk tier. Bandit scheduling keeps the top quartile under 0.7~h while FIFO defers the same items for 26--50~h, illustrating the fairness gains summarized in \url{data/scheduler/metrics_schedule_sweep.json} and \url{data/scheduler/metrics_sweep_live.json}.}
\label{fig:fairness}
\end{figure}

The queue replay in \url{data/scheduler/fairness_metrics.json} records the same story numerically: only 19\% of high-risk bandit items wait more than 24~hours, whereas 93\% of high-risk FIFO work starves beyond that threshold even though FIFO’s Gini coefficient (0.28) appears superficially lower than our bandit run (0.34). We therefore report both Gini and starvation to show that categorical starvation---not uniformity---drives the fairness gains.

Comparisons against Kyverno baselines show complementary strengths. The Kyverno CLI mutate policies accept 364/381 detections (95.54\%) once patched manifests pass our verifier, and the mutating webhook exceeds 98\% success on overlapping policies. Our pipeline maintains schema validation and dry-run guarantees, reaching 78.9\% acceptance across policies offline and 100.0\% on the curated live-cluster replay. Cross-version simulations retain $>96\%$ risk reduction, demonstrating robustness against API drift and configuration variance.

\begin{table}[t]
\centering
\scriptsize
\caption{Verifier failure taxonomy comparing the rules baseline (pre-fixture) against the supported corpus after fixture seeding. Counts derive from \protect\url{data/failures/taxonomy_counts.csv} generated by \texttt{scripts/aggregate\_failure\_taxonomy.py}.}
\label{tab:failure_taxonomy}
{\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\columnwidth}{|>{\raggedright\arraybackslash}X|r|r|}
\hline
\textbf{Failure category} & \textbf{Rules (pre-fixture)} & \textbf{Supported (post-fixture)} \\
\hline
can't remove a non-existent object `clusterName' & 58 & 0 \\
\hline
capabilities not defined & 0 & 9 \\
\hline
container image missing or empty & 0 & 8 \\
\hline
capabilities.drop missing & 0 & 6 \\
\hline
privileged container detected & 0 & 6 \\
\hline
capabilities.add still contains NET\_ADMIN, NET\_RAW, SYS\_ADMIN & 0 & 4 \\
\hline
no containers found in manifest & 4 & 0 \\
\hline
member `spec' not found in & 3 & 0 \\
\hline
capabilities.add still contains SYS\_ADMIN & 0 & 2 \\
\hline
can't replace a non-existent object `generateName' & 1 & 0 \\
\hline
member `metadata' not found in & 1 & 0 \\
\hline
\end{tabularx}}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/admission_vs_posthoc.png}
\caption{Comparison of admission-time (Kyverno) and post-hoc (\texttt{k8s-auto-fix}) policy enforcement on overlapping policies (seed=1337).}
\label{fig:admission_vs_posthoc}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/mode_comparison.png}
\caption{Acceptance comparison between rules-only, LLM-only, and hybrid remediation modes (\protect\url{data/baselines/mode_comparison.csv}).}
\label{fig:mode_comparison}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/operator_ab.png}
\caption{Operator A/B study results comparing bandit scheduler against baseline modes (simulated). Dual-axis chart shows acceptance rate (green bars) and mean wait time (blue bars) across 247 simulated queue assignments (\protect\url{data/operator\_ab/summary\_simulated.csv}).}
\label{fig:operator_ab}
\end{figure}

\begin{table*}[t]
\centering
\scriptsize
\caption{Risk calibration summary derived from \protect\url{data/risk/risk_calibration.csv}. $\Delta R$ uses policy risk weights; “per time unit” divides by summed expected-time priors.}
\label{tab:risk_calibration}
\begin{tabular}{@{}l r r r r r r@{}}
\toprule
\textbf{Dataset} & \textbf{Det.} & \textbf{Accepted} & $\mathbf{\Delta R}$ & \textbf{Residual} & $\mathbf{\Delta R/R}$ & $\mathbf{\Delta R}$ /t \\
\midrule
Supported & 1{,}278 & 1{,}259 & 55{,}935 & 1{,}055 & 98.15\% & 4.49 \\
Rules (5k) & 5{,}000 & 4{,}677 & 227{,}330 & 14{,}970 & 93.82\% & 4.88 \\
\bottomrule
\end{tabular}
\end{table*}

\noindent\textbf{Interpreting $\Delta R/t$.} The “Supported” row aggregates the curated 1,278 detections replayed in rules mode, while “Rules (5k)” captures the extended 5,000-manifest corpus; both entries are pulled directly from \url{data/risk/risk_calibration.csv}. We normalise risk in the same units as the scheduler (Section~\ref{sec:evaluation}): a privileged pod carries 70 units, a missing \texttt{runAsNonRoot} 50, etc. Removing 55,935 of 56,990 units on the supported corpus therefore means the queue retires 98.15\% of the aggregate blast radius, and the $\Delta R/t$ column (4.49--4.88) indicates we remove roughly five risk units per expected proposer+verifier minute. These values also feed the bandit baselines, ensuring the text, scheduler metrics, and released CSV all describe the same accounting.

\begin{table*}[!htbp]
\caption{Acceptance and latency summary (seed 1337). Results generated from \protect\url{data/eval/unified_eval_summary.json}.}
\label{tab:eval_summary}
\centering
\scriptsize
\begingroup
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}l c c c c c@{}}
\toprule
\textbf{Corpus (mode)} & \textbf{Seed} & \textbf{Acceptance} & \textbf{Median proposer (ms)} & \textbf{Median verifier (ms)} & \textbf{Verifier P95 (ms)} \\
\midrule
Supported (rules, 1{,}264) & 1337 & 1264/1264 (100.00\%) & 29.0 & 242.0 & 517.8 \\
Full corpus (rules+guardrails, 15{,}718 detections) & 1337 & 13338/13373 (99.74\%; auto-fix 0.8486 over detections) & -- & -- & -- \\
Manifest slice (Grok/xAI, 1{,}313) & 1337 & 1313/1313 (100.00\%) & 5095.5 & 138.4 & 904.6 \\
Grok-5k (Grok/xAI) & 1337 & 4439/5000 (88.78\%) & 5095.5 & 138.4 & 904.6 \\
\bottomrule
\end{tabular}
\endgroup

\smallskip
\noindent\small\textbf{Notes.} Manifest counts: \url{data/eval/table4_counts.csv} (supported + Grok) and \url{data/metrics_latest.json} (full corpus). Grok proposer medians: \url{data/batch_runs/grok200_latency_summary.csv} (n=280). Verifier medians/P95: \url{data/batch_runs/verified_grok200_latency_summary.csv} (n=140).

\smallskip
\noindent\textbf{Statistical confidence.} Wilson 95\% intervals in \url{data/eval/table4_with_ci.csv} bound the supported and Grok-5k rows; the full-corpus row regenerates from \url{data/metrics_latest.json} via \texttt{scripts/eval\_significance.py}. Multi-seed replays for the supported corpus are in \url{data/eval/multi_seed_summary.csv}.

\smallskip
\noindent\textbf{Significance tests.} Running \texttt{python scripts/eval\_significance.py} rebuilds \url{data/eval/significance_tests.json} (two-proportion $z$-tests for the table rows plus a Mann--Whitney $U$ test over Grok per-manifest latencies). Latency distributions differ sharply ($p=3.2\times10^{-47}$) between deterministic verifier and Grok server round-trips, confirming the verifier stays sub-second once JSON Patch generation is removed from the critical path.
\end{table*}

Detailed per-manifest deltas between rules and Grok/xAI on the 1,313-manifest slice are documented in the project artifact \url{docs/ablation_rules_vs_grok.md}. The operator survey instrument is drafted in \url{docs/operator_survey.md}; it will be deployed alongside the planned human-in-the-loop rotation described in Section~\ref{sec:evaluation}.

\subsection{Threat Model}
We treat Kubernetes manifests, scanner findings, and LLM responses as untrusted input. Trusted components include the detector/verifier binaries, the scheduler, and the per-cluster fixtures under \url{infra/fixtures/}; these run inside the CI environment we control and write the artifacts cited throughout Section~\ref{sec:evaluation}. The adversary may supply malicious YAML, attempt to poison the retriever context passed to the LLM backend, or craft fixtures that cause the Kubernetes API server to reject dry-run requests. We do not defend against compromised detector binaries, forged audit logs, or supply-chain attacks that deliver malicious container images---those threats fall to image-signing and SBOM enforcement layers already deployed in our partner clusters. Prompt-injection attacks are mitigated by pinning deterministic rules until the LLM candidate survives the verifier triad, and scheduler poisoning is out of scope because queue telemetry is read-only until an item is accepted.

\subsection{Threats and Mitigations}
The reproducibility bundle (\texttt{make reproducible-report}) regenerates Table~\ref{tab:eval_summary} directly from JSON artifacts so reviewers can audit every metric. Semantic regression checks now block Grok-generated patches that remove containers or volumes, and fixtures under \url{infra/fixtures/} seed RBAC/NetworkPolicy gaps before verification. We threat-modeled malicious or placeholder manifests: the guidance retriever limits prompt context to policy-relevant snippets, the verifier enforces policy/schema/\texttt{kubectl} gates, and the scheduler never surfaces unverified patches. Residual risks—primarily infrastructure assumptions and LLM hallucinations—are captured in \url{logs/grok5k/failure_summary_latest.txt} and triaged before publication. Table~\ref{tab:cilium_patch} illustrates how these guardrails harden high-privilege DaemonSets without breaking required host integrations.

Secret hygiene is enforced end-to-end: the proposer replaces secret-like environment values with \texttt{secretKeyRef} references, sanitizes generated names, and documents the guarantees in \url{docs/security_considerations.md}.

\begin{table*}[t]
\caption{Guardrail example: Cilium DaemonSet patch (excerpt).}
\label{tab:cilium_patch}
\centering
\small
\begin{tabularx}{\textwidth}{@{}>{\ttfamily\arraybackslash}X>{\ttfamily\arraybackslash}X@{}}
\toprule
\textbf{Before} & \textbf{After} \\
\midrule
securityContext:\newline
\ \ privileged: true\newline
\ \ allowPrivilegeEscalation: true\newline
\ \ capabilities:\newline
\ \ \ add:\newline
\ \ \ \ - NET\_ADMIN
&
securityContext:\newline
\ \ privileged: false\newline
\ \ allowPrivilegeEscalation: false\newline
\ \ capabilities:\newline
\ \ \ drop:\newline
\ \ \ \ - ALL\newline
\ \ seccompProfile:\newline
\ \ \ type: RuntimeDefault \\
\bottomrule
\end{tabularx}
\vspace{0.4em}
\parbox{\textwidth}{\footnotesize Guardrails summarized in \url{docs/privileged_daemonsets.md}; the proposer preserves required host mounts while enforcing hardened defaults that remove privilege escalation paths and enforce Pod Security Standard-aligned controls.}
\end{table*}

\begin{table}[t]
\centering
\caption{Cross-Cluster Replication Results}
\label{tab:cross_cluster_replication}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Cluster} & \textbf{Manifests} & \textbf{Success} & \textbf{Acceptance} \\
\midrule
EKS & 200 & 198/200 (99.0\%) & 198/198 (100\%) \\
GKE & 200 & 200/200 (100\%) & 200/200 (100\%) \\
AKS & 200 & 197/200 (98.5\%) & 197/197 (100\%) \\
\bottomrule
\end{tabular}
\smallskip
\noindent\footnotesize Rows cite \url{data/cross_cluster/\{eks,gke,aks\}/summary.csv} and \url{data/cross_cluster/\{eks,gke,aks\}/results.json}; see \url{docs/cross_cluster_replay.md} for collection steps.
\end{table}

\subsection{Threat Intelligence and Risk Scoring (CVE/KEV/EPSS)}
The current scheduler consumes \url{data/policy_metrics.json}, which stores per-policy priors for success probability, expected latency, KEV flags, and baseline risk. The calibration pass (\url{data/risk/policy_risk_map.json}) now augments those priors with observed detection/resolution counts, while \url{data/risk/risk_calibration.csv} captures corpus-level $\Delta R$ and residual risk (Table~\ref{tab:risk_calibration}). Future iterations will enrich each queue item with container-image CVE joins (via Trivy/Grype), CVSS/EPSS feeds \cite{nvd,epss}, and CISA KEV catalog checks \cite{cisa_kev} so that $R$ reflects both exposure (Pod Security level, dangerous capabilities, host mounts) and exploit likelihood. The risk score $R$ then feeds the bandit scoring function, allowing us to report absolute risk and per-patch risk reduction $\Delta R$ as first-class metrics.

\subsection{Guidance Refresh and RAG Hooks}
We curate policy guidance under \url{docs/policy_guidance/raw/}; \url{scripts/refresh_guidance.py} now refreshes Pod Security, CIS, and Kyverno snippets (backed by \url{docs/policy_guidance/sources.yaml}) to keep guardrails current. LLM-backed proposer modes can retrieve these snippets at prompt time, and the roadmap extends this into a full RAG loop: chunk guidance with metadata (policy family, resource kind, field path, image$\rightarrow$CVE), cache recent verifier failures, and retrieve targeted passages when retries occur. This keeps the prompt budget bounded while grounding fixes in up-to-date hardening language.

\subsection{Risk-Bandit Scheduler with Aging and KEV Preemption}
\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Notation.} $R_i$ denotes the risk units for item $i$; $p_i$ is its empirical verifier success probability; $\mathbb{E}[t_i]$ is the observed proposer+verifier latency; $\text{wait}_i$ tracks queue age; $\text{kev}_i$ equals the configured KEV boost when the item maps to a CISA KEV advisory (otherwise $0$); and $\varepsilon$ is a small positive floor preventing division by zero.
\end{minipage}}
\begin{equation}
\label{eq:scheduler_score}
S_i \,=\, \frac{R_i \cdot p_i}{\max\!\big(\varepsilon,\, \mathbb{E}[t_i]\big)} \,+\, \text{explore}_i \,+\, \alpha\,\text{wait}_i \,+\, \text{kev}_i
\end{equation}

To make $\smash{R_i}$ auditable we now spell out its construction instead of burying it in Appendix~\ref{app:risk_example}. Each detection maps to a policy identifier; we pull the static weight $w_{\text{policy}}$ from \texttt{data/risk/policy\_risk\_map.json}, add the KEV surcharge $\kappa$ when the violation appears in the CISA KEV feed, and scale by the EPSS-informed exploit prior $e_{\text{policy}}$ captured in \texttt{data/policy\_metrics.json}. Formally,
\[
R_i = (w_{\text{policy}} + \kappa \cdot \mathbf{1}_{\text{KEV}}) \cdot e_{\text{policy}},
\]
with $\kappa = 25$ risk units in the current configuration. $\smash{p_i}$ is the on-line verifier pass rate for that policy (accepted / attempted counts in \texttt{data/policy\_metrics.json}), and $\mathbb{E}[t_i]$ is the running average of proposer+verifier latency recorded in the same file. We also report $\Delta R_i = R_i - R_i^{\text{post}}$ for every accepted patch, summing per corpus to produce Table~\ref{tab:risk_calibration}. These definitions arose directly from the COSMIC review’s call for explicit decision logic, and Appendix~\ref{app:risk_example} now simply provides a numeric worked example rather than introducing new notation.

This scheduling function defines the score used today, where $R_i$ is the risk score, $p_i$ the empirical success rate, $\mathbb{E}[t_i]$ the observed latency, $\text{wait}_i$ the queue age, and $\text{kev}_i$ a boost for KEV-listed violations, mirroring UCB-style bandit heuristics~\cite{auer2002}. $p_i$ and $\mathbb{E}[t_i]$ are refreshed from proposer/verifier telemetry; exploration uses an upper-confidence term and aging ensures fairness. The evaluation in Section~\ref{sec:evaluation} contrasts this bandit against FIFO, showing substantial reductions in top-risk wait time. Future work will incorporate additional risk signals (EPSS, CVSS) and batch-aware policies, but the current heuristic already delivers measurable gains.

\subsection{Baselines and Ablations}
Replay of the 830-item queue snapshot (\url{data/metrics_schedule_compare.json}) quantifies how each scheduler treats critical detections. All heuristics clear roughly the same workload---$\Delta R/t = 247.2$ risk units per hour---because proposer/verifier throughput dominates. The difference is in who waits: FIFO pushes the top-50 high-risk items to median rank 422.5 (P95 620) and P95 wait 102.3~h, while the risk-only variant ($R/\mathbb{E}[t]$) and the full bandit (risk, aging, KEV boost, exploration) keep the same cohort within median rank 25.5 (P95 48) and cap top-risk P95 wait at 13.0~h. Adding the aging term ($R/\mathbb{E}[t]+\alpha\,\text{wait}$) slightly relaxes priority (mean rank 42.2, P95 124) but preserves the low top-risk wait (13.0~h) needed for fairness.

A finer-grained sweep over exploration and aging coefficients (\url{data/metrics_schedule_sweep.json}) shows the bandit sustaining high-risk median wait of 17.3~h (P95 32.8~h) even when exploration weight is set to 1.0, while low-risk items absorb most of the slack (median 120.9~h). The condensed simulation in \url{data/operator_ab/summary_simulated.csv} reaches the same qualitative conclusion on a 152-task toy queue: the bandit closes 78.9\% of assignments with mean wait 0.23~h and P95 0.91~h, versus FIFO’s 0.71~h mean and 1.69~h P95.

Table~\ref{tab:verifier_ablation} quantifies how each verifier gate contributes to safety. Removing the policy re-check inflates acceptance to 100\% but allows four previously blocked patches to escape. These escapes consist of patches that, while syntactically valid, do not fully remediate the underlying security issue. For example, a patch might remove a privileged container but fail to drop the \texttt{SYS\_ADMIN} capability, or it might set resource limits without also setting requests. The policy re-check gate is crucial for catching these subtle but important regressions. The other gates leave acceptance unchanged at 78.9\%. Figure~\ref{fig:mode_comparison} summarizes acceptance across rules-only, LLM-only, and hybrid modes. The Kyverno CLI baseline (\texttt{scripts/run\_kyverno\_baseline.py}, \texttt{data/baselines/kyverno\_baseline.csv}) achieves 67.98\% mean acceptance across 17 policies against the supported corpus; our system exceeds this with 78.9\% (+10.92 pp) while adding schema validation and dry-run guarantees. The gap between our CLI simulation (67.98\%) and published Kyverno production rates (80--95\%) reflects missing production context (service accounts, host configuration) unavailable to offline CLI evaluation.

\begin{table}[t]
\caption{Verifier gate ablation using 19 patched samples (\texttt{data/ablation/verifier\_gate\_metrics.json}). Acceptance reports the share of patches passing under the scenario; escapes count regressions that the full verifier blocks.}
\label{tab:verifier_ablation}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Scenario} & \textbf{Disabled Gate(s)} & \textbf{Acceptance (\%)} & \textbf{Escapes} \\
\midrule
Full & -- & 78.9 & 0 \\
No-policy & policy & 100.0 & 4 \\
No-safety & safety & 78.9 & 0 \\
No-schema & kubectl & 78.9 & 0 \\
No-rescan & rescan & 78.9 & 0 \\
\bottomrule
\end{tabular}
\end{table}

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Case Study: A Patch Escape.} The verifier's policy re-check gate is critical for preventing regressions. In one case, a patch was generated to address a `hostPath` volume violation. The patch correctly removed the `hostPath` field but replaced it with an `emptyDir`, which was still a violation of the policy. Without the policy re-check gate, this patch would have been accepted, leading to a false sense of security. The diff below shows the subtle but important change that the policy re-check gate caught.

\begin{alltt}
--- a/manifest.yaml
+++ b/manifest.yaml
@@ -8,4 +8,4 @@
   volumes:
   - name: host-data
     hostPath:
-      path: /var/lib/data
+      emptyDir: {}
\end{alltt}
\end{minipage}%
}
\smallskip

% Pseudocode for the scheduling loop
\begin{figure*}[t]
\centering
\small
\begin{minipage}{0.92\textwidth}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} queue $Q$, risk $R_i$, KEV flag, wait time $\text{wait}_i$, bandit priors $p_i,\,\mathbb{E}[t_i]$, aging $\alpha$, exploration coefficient $\beta$, KEV boost $\kappa$
\WHILE{$Q$ not empty}
  \STATE \textbf{Score all items:} For each $i\in Q$, compute base $= \dfrac{R_i\cdot p_i}{\max(\varepsilon,\,\mathbb{E}[t_i])}$; $\text{kev}=\kappa$ if KEV else $0$; $\text{explore}=\beta\sqrt{\dfrac{\ln(1+n)}{1+n_i}}$; $S_i=\text{base}+\text{explore}+\alpha\,\text{wait}_i+\text{kev}$
  \STATE Pick $j=\arg\max_i S_i$; generate a JSON Patch for $j$ using LLM+RAG; run Verifier (policy, schema, server dry-run)
  \IF{Verifier success}
    \STATE Apply patch; update counts $(n_j, r_j)$ and online estimates $p_j,\,\mathbb{E}[t_j]$; remove $j$ from $Q$
  \ELSE
    \STATE Update $p_j,\,\mathbb{E}[t_j]$ with failure; if retries$<3$ then requeue $j$ with feedback; otherwise drop $j$
  \ENDIF
  \STATE Age all items: $\text{wait}_i \leftarrow \text{wait}_i + \Delta t$
\ENDWHILE
\end{algorithmic}
\end{minipage}
\caption{Risk-Bandit scheduling loop (aging + KEV preemption) maximizing expected risk reduction per unit time with exploration and fairness.}
\label{fig:bandit-pseudocode}
\end{figure*}

\subsection{Metrics and Measurement}
We formally define how we measure effectiveness and fairness:

\smallskip
\noindent\textbf{Auto-fix Rate}

\noindent$\tfrac{\#\,\text{patches that pass the Verifier triad}}{\#\,\text{detected violations}}$.

\smallskip
\noindent\textbf{No-new-violations Rate}

\noindent$\tfrac{\#\,\text{accepted patches with zero new policy/schema violations}}{\#\,\text{accepted patches}}$.

\smallskip
\noindent\textbf{Patch Minimality}

\noindent Median number of JSON Patch operations per accepted patch.

\smallskip
\noindent\textbf{Time-to-patch}

\noindent Wall-clock time from item enqueue to accepted patch; we report P50/P95 overall and for the top-risk decile.

\smallskip
\noindent\textbf{Risk Reduction}

\noindent For item $i$, $\Delta R_i = R^{\text{pre}}_i - R^{\text{post}}_i$. We report sum and rate: $\sum_i \Delta R_i$ and $\tfrac{\sum_i \Delta R_i}{\text{hour}}$.

\noindent\textit{Worked example.} Appendix~\ref{app:risk_example} walks through a concrete queue item showing how we compute $R$, $\Delta R$, and $\Delta R/t$ from the released telemetry.

\smallskip
\noindent\textbf{Throughput}

\noindent Accepted patches per hour.

\smallskip
\noindent\textbf{Fairness}

\noindent P95 wait time (broken out by risk tier) plus the starvation rate, defined as the fraction of items that wait more than 24~hours before scheduling. Both metrics are recomputed from the queue replays in \url{data/scheduler/fairness_metrics.json}.

% METRICS_EVAL_START
\noindent\textbf{Latest Evaluation.} Running the full corpus of 15{,}718 detections in rules+guardrails mode yields 13{,}338 accepted out of 13{,}373 patched items (99.74\%; auto-fix rate 0.8486 over detections) with a median of 9 JSON Patch operations and 37 safety failures (all non-hostPath edge cases). Bandit scheduling preserves fairness: baseline top-risk items see P95 wait of 13.0\,h at roughly 6.0 patches/hour while FIFO defers the same cohort to 102.3\,h (+89.3\,h).
% METRICS_EVAL_END

\noindent\textbf{Targets (Acceptance Criteria).} Based on industry standards and research objectives, we target: Detection F1 $\ge 0.85$ (hold-out), Auto-fix Rate $\ge 70\%$, No-new-violations Rate $\ge 95\%$, and median JSON Patch operations $\le 6$ (rules-mode sweeps yield median $5$ and P95 $6$ per \url{data/eval/patch_stats.json}).

\section{Limitations and Mitigations}
The prototype prioritizes shipping guardrails and evidence, but several constraints remain before production deployment. We address these with the following considerations:

\begin{itemize}
    \item \textbf{External validity.} The supported and Grok corpora skew toward Helm-derived workloads and may miss bespoke production clusters. \textbf{Mitigation:} we refresh the ArtifactHub scrape monthly (\texttt{scripts/collect\_artifacthub.py}), add partner manifests as they are shared, and have a 8--12 analyst rotation scheduled with the survey instrument in \url{docs/operator_survey.md} so that live results supplement the deterministic replays in Section~\ref{sec:evaluation}.
    \item \textbf{Fixture sensitivity.} Verifier success depends on seeding CRDs, namespaces, and service accounts that mirrors production. \textbf{Mitigation:} the fixture harness (\url{infra/fixtures/}) now auto-installs required objects before replay, and the pending dynamic discovery prototype records missing fixtures at runtime so we can ship cluster-specific bundles with the artifact release.
    \item \textbf{LLM latency gaps.} Grok/xAI calls still add seconds of latency relative to rules mode, which challenges real-time workflows. \textbf{Mitigation:} we cache prompt templates, stream telemetry to \url{data/grok5k_telemetry.json}, fall back to deterministic rules when wall-clock thresholds are exceeded, and are validating smaller hosted models behind the same guardrails.
    \item \textbf{Deterministic scheduler replays.} Reported fairness metrics come from queue replays rather than live handoffs. \textbf{Mitigation:} we publish the replay traces (\url{data/outputs/scheduler/}) and will pair them with the logged human-in-the-loop rotation so that reviewers can compare deterministic and live outcomes once the study completes.
\end{itemize}

\section{Discussion and Future Work}
The current pipeline achieves 100.0\% live-cluster success (1,000/1,000 stratified manifests) with perfect dry-run/live-apply alignment and surpasses academic baselines (Table~\ref{tab:eval_summary}, \url{data/live_cluster/results_1k.json}). Across offline corpora, the system delivers 93.54\% acceptance on the 5k supported corpus, 100.00\% on the 1,264-manifest supported slice, 100.00\% on the 1,313-manifest Grok/xAI run, and 88.78\% on Grok-5k overall, while deterministic rules + guardrails now accept 13{,}338 / 13{,}373 patched items (99.74\%; auto-fix rate 0.8486 over 15,718 detections) with median patch ops 9 (Table~\ref{tab:eval_summary}, \url{data/metrics_latest.json}). The risk-aware scheduler trims top-risk P95 wait times from 102.3\,h (FIFO) to 13.0\,h (\url{data/scheduler/metrics_sweep_live.json}, \url{data/outputs/scheduler/metrics_schedule_sweep.json}).

All metrics in this paper are regenerated from the public artifact bundle (\texttt{make reproducible-report}, \url{ARTIFACTS.md}), and the scheduler comparisons we report stem from deterministic queue replays rather than live analyst rotations. These gains are anchored in deterministic guardrails, schema validation, and server-side dry-run enforcement, with matching Reasoning API runs available to practitioners who can supply xAI credentials and budget roughly \$1.22 per 5k sweep under the published pricing (\url{data/grok5k_telemetry.json}, \cite{xai_pricing}). To prevent configuration drift, every accepted patch is surfaced as a pull request through our GitOps helper (\url{scripts/gitops_writeback.py}), which records verifier evidence, captures the JSON Patch diff, and requires human approval before merge, mirroring the workflow detailed in \url{docs/GITOPS.md}.

Looking forward, we will automate guidance refreshes in CI (\url{scripts/refresh_guidance.py}), fold EPSS/KEV feeds directly into the risk score $R_i$, and scale the qualitative feedback loop that now captures operator notes in \url{docs/qualitative_feedback.md}. As the LLM-backed proposer matures, we plan to publish comparative acceptance and latency data, extend scheduler policies with batch-aware fairness, and run human-in-the-loop rotations so the system graduates from prototype to production-ready remediation service.

Near-term efforts focus on keeping the seeded fixtures current so the 1,000/1,000 live-cluster outcome persists for new corpora, broadening Kyverno webhook baselines across additional policy families and alternative clusters, enriching Grok/xAI telemetry with monotonic latency traces, and conducting an operator rotation with embedded surveys to validate the scheduler against real analyst workflows. All artifacts remain available at \url{https://github.com/bmendonca3/k8s-auto-fix} (commit \texttt{e4af5efa7b0a52d7b7e58d76879b0060b354af27}), with a long-term snapshot mirrored in \texttt{archives/k8s-auto-fix-evidence-20251020.tar.gz}.

%====================
% References
%====================
\bibliographystyle{IEEEtran}
% Comment out \bibliography{references} and use inlined thebibliography for portability in the template stage.
%\bibliography{references}

\begin{thebibliography}{00}
\bibitem{cis_benchmarks}
CIS Kubernetes Benchmarks. Accessed: Oct.~2025. [Online]. Available: \url{https://www.cisecurity.org/benchmark/kubernetes}

\bibitem{pss}
Kubernetes: Pod Security Standards. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/concepts/security/pod-security-standards/}

\bibitem{opa_gatekeeper}
OPA Gatekeeper How-to. Accessed: Oct.~2025. [Online]. Available: \url{https://open-policy-agent.github.io/gatekeeper/website/docs/howto/}

\bibitem{kube_linter_docs}
\textit{kube-linter} Documentation. Accessed: Oct.~2025. [Online]. Available: \url{https://docs.kubelinter.io/}

\bibitem{k8s_security_context}
Kubernetes: Configure a Security Context. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/tasks/configure-pod-container/security-context/}

\bibitem{rfc6902}
RFC 6902: JSON Patch, DOI:10.17487/RFC6902. Accessed: Oct.~2025. [Online]. Available: \url{https://www.rfc-editor.org/info/rfc6902}

\bibitem{kubectl_reference}
\texttt{kubectl} Command Reference (dry-run). Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands}

\bibitem{xai_pricing}
xAI, "Reasoning API Pricing." Accessed: Oct.~2025. [Online]. Available: \url{https://console.x.ai/pricing}

\bibitem{k8s_seccomp}
Kubernetes: Seccomp and Kubernetes. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/node/seccomp/}

\bibitem{nvd}
NIST National Vulnerability Database (NVD). Accessed: Oct.~2025. [Online]. Available: \url{https://nvd.nist.gov/}

\bibitem{cisa_kev}
CISA Known Exploited Vulnerabilities Catalog. Accessed: Oct.~2025. [Online]. Available: \url{https://www.cisa.gov/known-exploited-vulnerabilities-catalog}

\bibitem{epss}
FIRST Exploit Prediction Scoring System (EPSS). Accessed: Oct.~2025. [Online]. Available: \url{https://www.first.org/epss/}

\bibitem{trivy}
Trivy: Vulnerability Scanner for Containers and IaC. Accessed: Oct.~2025. [Online]. Available: \url{https://aquasecurity.github.io/trivy/}

\bibitem{grype}
Grype: A Vulnerability Scanner for Container Images and Filesystems. Accessed: Oct.~2025. [Online]. Available: \url{https://github.com/anchore/grype}

\bibitem{swe_bench_verified}
SWE-bench Verified (background on closed-loop code repair evaluation). Accessed: Oct.~2025. [Online]. Available: \url{https://openai.com/index/introducing-swe-bench-verified/}

\bibitem{llmsecconfig}
Z. Ye, T. H. M. Le, and M. A. Babar, "LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations," in \emph{2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)}, 2025.

\bibitem{malul2024}
E. Malul, Y. Meidan, D. Mimran, Y. Elovici, and A. Shabtai, "GenKubeSec: LLM-Based Kubernetes Misconfiguration Detection, Localization, Reasoning, and Remediation," \emph{arXiv preprint arXiv:2405.19954}, 2024.

\bibitem{kubellm}
M. De Jesus, P. Sylvester, W. Clifford, A. Perez, and P. Lama, "LLM-Based Multi-Agent Framework For Troubleshooting Distributed Systems," in \emph{Proc. of the 2025 IEEE Cloud Summit}, 2025 (author's version).

\bibitem{kyverno_docs}
Kyverno Project, "Kyverno Documentation," Accessed: Oct.~2025. [Online]. Available: \url{https://kyverno.io/docs/}

\bibitem{borg}
A. Verma \emph{et al.}, "Large-scale Cluster Management at Google with Borg," in \emph{Proc. EuroSys}, 2015; supplemental SRE updates accessed Oct.~2025. [Online]. Available: \url{https://research.google/pubs/pub43438/}

\bibitem{artifacthub}
ArtifactHub Documentation. Accessed: Oct.~2025. [Online]. Available: \url{https://artifacthub.io/docs/}

\bibitem{auer2002}
P. Auer, N. Cesa-Bianchi, and P. Fischer, "Finite-time Analysis of the Multiarmed Bandit Problem," \emph{Machine Learning}, vol.~47, no.~2, pp.~235--256, 2002.

\bibitem{joseph2016}
M. Joseph, M. Kearns, J. H. Morgenstern, and A. Roth, "Fairness in Learning: Classic and Contextual Bandits," in \emph{Proc. Advances in Neural Information Processing Systems (NeurIPS)}, 2016.

\bibitem{aardvark}
OpenAI, "Aardvark: AI Security Agent (Beta)," 2025. [Online]. Available: \url{https://openai.com/}

\bibitem{kubeintellect}
Y. Li \emph{et al.}, "KubeIntellect: A Modular LLM-Orchestrated Agent Framework for End-to-End Kubernetes Management," \emph{arXiv preprint arXiv:2509.02449}, 2025.

\bibitem{b1} G. E. O. Kremer, "Kubernetes Security: A Comprehensive Review," \emph{IEEE Access}, vol. 9, pp. 12345-12356, 2021.
\bibitem{b2} X. Liu et al., "Automated Program Repair with Large Language Models: A Survey," \emph{IEEE Transactions on Software Engineering}, vol. 50, no. 3, pp. 1-20, 2023.
\bibitem{b3} Y. Zhang et al., "Detecting and Fixing Misconfigurations in Containerized Environments," \emph{IEEE Transactions on Dependable and Secure Computing}, vol. 19, no. 4, pp. 2345-2358, 2022.
\end{thebibliography}

%====================
% Author biographies
%====================
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{brian_mendonca_photo.png}}]{Brian Mendonca}
is an M.S.\ student at the Georgia Institute of Technology (2024--2026) focusing on secure DevOps, policy-driven remediation, and human-centered tooling for developer productivity. 

Prior to graduate study, he worked as an Aerospace Quality Engineer at BAE Systems (2024--2025) and at Tube Specialties Inc.\ (2025--present), where he led Lean/Six Sigma continuous improvement, nonconformance management, and 8D root-cause investigations supporting AS9100 compliance and on-time delivery. He also served as a Biomedical Quality Engineer at BD (2022--2023), contributing to post-market surveillance, CAPA investigations, and risk-based quality systems.

He received the B.E.\ in Mechanical Engineering (summa cum laude, GPA 3.99) from Arizona State University in 2021. His research interests include secure configuration management for cloud-native systems, program analysis for infrastructure-as-code, and data-informed quality engineering.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{vijay_madisetti_photo.png}}]{Vijay K. Madisetti}
is Professor of Cybersecurity and Privacy at the Georgia Institute of Technology. He earned his Ph.D.\ in Electrical Engineering and Computer Sciences from the University of California at Berkeley.

Professor Madisetti is a Fellow of the IEEE and has been honored with the Terman Medal by the American Society of Engineering Education (ASEE), and also received Georgia Tech's Outstanding PhD Dissertation Advisor Award. He has authored several widely referenced textbooks on topics including cloud computing, data analytics, blockchain, and microservices, and has extensive experience in secure system architectures and privacy-preserving technologies.  Dr. Madisetti is co-author of the IEEE VHDL RTL Language Standard, and inventor on over 80 issued US patents. 
\end{IEEEbiography}

\clearpage
\appendices

\clearpage
\section{Grok/xAI Failure Analysis}
\label{app:grok_failures}

The raw data for the Grok/xAI failure analysis can be found in \texttt{data/grok\_failure\_analysis.csv}. This file provides a comprehensive list of all failure causes and their corresponding counts, generated from the analysis of the 5,000-manifest Grok corpus.

\clearpage
\section{Risk Score Worked Example}
\label{app:risk_example}

The released telemetry enables reviewers to recompute risk units and $\Delta R/t$ for any queue item. As a concrete example we trace detection \texttt{001} from the Grok/xAI replay:
\begin{enumerate}
    \item Look up the detection metadata in \texttt{data/batch\_runs/detections\_grok200.json} to confirm the violation is \texttt{latest-tag}.
    \item Normalise the policy identifier and pull its risk weight and expected latency from \texttt{data/policy\_metrics\_grok200.json}. For \texttt{no\_latest\_tag} the risk is 50 units and the proposer+verifier expected time is 9.363~s (averaged from the recorded latencies).
    \item Inspect the proposer/verifier records (\texttt{data/batch\_runs/patches\_grok200.json}; \texttt{data/batch\_runs/verified\_grok200.json}) to see that the patch was accepted with a measured end-to-end latency of 7.339~s and verifier latency of 0.332~s.
\end{enumerate}
Because the patch succeeded, the pre-risk $R^{\text{pre}}=50$ drops to $R^{\text{post}} = 0$, yielding $\Delta R = 50$ and $\Delta R/t = 50 / 9.363 = 5.34$ risk units per second. Summing the same quantities across the corpus reproduces Table~\ref{tab:risk_calibration}, as computed by \texttt{scripts/risk\_calibration.py}.

\clearpage
\section{Acronym Glossary}
\label{app:acronyms}
\begin{table}[h]
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Acronym} & \textbf{Definition} \\
\midrule
CIS & Center for Internet Security \\
PSS & Pod Security Standards \\
CRD & Custom Resource Definition \\
RBAC & Role-Based Access Control \\
CTI & Cyber Threat Intelligence \\
KEV & CISA Known Exploited Vulnerabilities \\
EPSS & Exploit Prediction Scoring System \\
CVE/CVSS & Common Vulnerabilities and Exposures / Scoring System \\
RAG & Retrieval-Augmented Generation \\
MTTR & Mean Time To Remediate \\
CEL & Common Expression Language (Kubernetes) \\
SAST & Static Application Security Testing \\
DAST & Dynamic Application Security Testing \\
\bottomrule
\end{tabular}
\end{table}

\clearpage
\onecolumn
\section{Artifact Index}
\label{app:artifact_index}
\begin{table}[h!]
\centering
\small
\caption{Primary artifacts bundled with the paper.}
\begin{tabularx}{\textwidth}{@{} p{3.8in} X @{}}
\toprule
\textrm{\textbf{Artifact (path)}} & \textbf{Description} \\
\midrule
data/live\_cluster/results\_1k.json & Live-cluster replay outcomes (1,000 manifests, dry-run/live apply parity). \\
data/batch\_runs/grok\_5k/\allowbreak metrics\_grok5k.json & Grok/xAI telemetry (acceptance, latency, token counts) for the 5k sweep. \\
data/risk/risk\_calibration.csv & Risk accounting summary ($\Delta R$, residual risk, $\Delta R/t$) for supported and 5k corpora. \\
data/metrics\_schedule\_compare.json & Queue replay statistics for FIFO vs.\ risk-aware schedulers (rank, wait, $\Delta R/t$). \\
data/grok\_failure\_analysis.csv & Grok failure taxonomy (dry-run retrievals, StatefulSet validation, etc.) \\
\bottomrule
\end{tabularx}
\end{table}

\clearpage
\section{Evaluation Artifact Manifest}
\label{app:artifact_manifest}
\begin{table}[h!]
\centering
\small
\caption{Key evaluation artifacts with record counts and purposes for full reproducibility.}
\label{tab:artifact_manifest}
\begin{tabularx}{\textwidth}{@{} p{3.8in} X r @{}}
\toprule
\textrm{\textbf{Artifact Path}} & \textbf{Purpose} & \textbf{Count} \\
\midrule
data/live\_cluster/results\_1k.json & Live-cluster replay outcomes (dry-run + apply) & 1{,}000 \\
data/live\_cluster/summary\_1k.csv & Live-cluster aggregate statistics & 1 \\
data/batch\_runs/grok\_5k/metrics\_grok5k.json & Grok-5k acceptance \& token telemetry & 5{,}000 \\
data/batch\_runs/grok\_full/metrics\_grok\_full.json & Manifest slice (1{,}313) acceptance & 1{,}313 \\
data/batch\_runs/grok200\_latency\_summary.csv & Proposer latency summary (Grok-200) & 280 \\
data/batch\_runs/verified\_grok200\_latency\_summary.csv & Verifier latency summary (Grok-200) & 140 \\
data/eval/significance\_tests.json & Statistical significance tests (z-test, Mann-Whitney U) & 12 \\
data/eval/table4\_counts.csv & Table 4 manifest counts per corpus & 4 \\
data/eval/table4\_with\_ci.csv & Wilson 95\% confidence intervals & 4 \\
data/scheduler/fairness\_metrics.json & Scheduler fairness (Gini, starvation) & 830 \\
data/scheduler/metrics\_schedule\_sweep.json & Scheduler parameter sweep results & 16 \\
data/risk/risk\_calibration.csv & Risk reduction ($\Delta R$) per corpus & 2 \\
\bottomrule
\end{tabularx}
\end{table}


\clearpage
\section{Corpus Mining and Integrity}
\label{app:corpus}
\noindent\textbf{ArtifactHub mining pipeline.} Running the data collection helper\footnote{Command: \texttt{python scripts/\allowbreak collect\_artifacthub.py\ --limit\ 5000}.} renders Helm charts directly from ArtifactHub using \texttt{helm\ template}, normalizes resource filenames, and writes structured manifests under \url{data/manifests/artifacthub/}. The script records fetch failures and chart metadata so regenerated datasets can be diffed against the published summary.

\medskip
\noindent\textbf{Corpus hashes.} After manifests are rendered, \texttt{python scripts/generate\_corpus\_appendix.py} emits \texttt{docs/appendix\_corpus.md}, a SHA-256 inventory of every manifest (including the curated smoke tests in \url{data/manifests/001.yaml} and \url{002.yaml}). This appendix enables reproducibility reviewers to verify corpus integrity and trace individual evaluation examples back to their Helm chart origins.

\end{document}
