\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
% \usepackage{caption} % Removed: Conflicts with the ieeeaccess class
% \usepackage{mathptmx} % Disabled to avoid RSFS font requirement on minimal TeX
\usepackage[english]{babel}
% Enable microtype gently to reduce overfull boxes (protrusion/expansion)
\IfFileExists{microtype.sty}{\usepackage[final]{microtype}}{}
% Packages added for the comparison table
\usepackage{booktabs}
\usepackage[table]{xcolor} % Use [table] option for better compatibility
\usepackage{tabularx} % For auto-wrapping text in tables
\usepackage{multirow}
\usepackage{fancyvrb}
\usepackage{listings}
\usepackage{alltt}

% xurl may not be present in minimal TeX installs; fall back to url
\IfFileExists{xurl.sty}{\usepackage{xurl}}{\usepackage{url}}
\usepackage[hidelinks]{hyperref}

\usepackage{bm}

% Listings-based code block with line breaking and tight margins
\lstdefinestyle{codestyle}{%
  basicstyle=\ttfamily\scriptsize,
  breaklines=true,
  breakatwhitespace=false,
  columns=fullflexible,
  keepspaces=true,
  showstringspaces=false,
  frame=single,
  xleftmargin=0.5em,
  xrightmargin=0.5em,
  aboveskip=0.5\baselineskip,
  belowskip=0.5\baselineskip
}
\lstnewenvironment{codeblock}[1][]{\lstset{style=codestyle,#1}}{}

\makeatletter
% Allow URL/path breaks at underscore, dot, and slash to prevent overfull lines
\appto\UrlBreaks{\do\_\do\.\do\/}
\makeatother

% Fallback mapping for RSFS font to Computer Modern symbols (avoids rsfs10)
\makeatletter
\DeclareFontFamily{U}{rsfs}{}
\DeclareFontShape{U}{rsfs}{m}{n}{<-6> s*[1.05] cmsy5 <6-8> s*[1.05] cmsy7 <8-> s*[1.05] cmsy10}{}
\DeclareSymbolFont{rsfs}{U}{rsfs}{m}{n}
\DeclareSymbolFontAlphabet{\mathscr}{rsfs}
\makeatother

% Ensure local Type 1 font maps (bundled with template) are loaded
\pdfmapfile{+t1-formata.map}
\pdfmapfile{+t1-times.map}
\pdfmapfile{+t1-helvetica.map}
\pdfmapfile{+t1-giovannistd.map}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%Your document starts from here ___________________________________________________

\begin{document}
\history{}
\doi{DOI: TBD}

% (Revert to default IEEE Access title page styling)
\title{Closed-Loop Threat-Guided Auto-Fixing of Kubernetes YAML Security Misconfigurations}
\author{\uppercase{Brian Mendonca}\authorrefmark{1}, and
\uppercase{Vijay K. Madisetti}\authorrefmark{2}, \IEEEmembership{Fellow, IEEE}}

\address[1]{College of Computing, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: brian.mendonca6@gmail.com)}
\address[2]{School of Cybersecurity and Privacy, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: vkm@gatech.edu)}

% Short running heads to avoid header overflow
\markboth
{Mendonca et al.: k8s-auto-fix}
{Mendonca et al.: k8s-auto-fix}

\corresp{Corresponding author: Dr. Vijay Madisetti (e-mail: vkm@gatech.edu).}

% \tfootnote{This work was supported in part by [Funding Source, if applicable].}

\titlepgskip=-22pt

% Abstract and keywords must be defined before \maketitle for ieeeaccess
\begin{abstract}
Misconfigured Kubernetes manifests expand blast radius when pipelines stop at detection. We present \texttt{k8s-auto-fix}, a closed loop (\emph{Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler}) that addresses this gap. Our key results demonstrate the effectiveness of this approach: a 200-manifest live-cluster replay achieves 100\% success (200/200); deterministic rules cover 99.51\% of violations; an optional LLM backend reaches 88.78\% acceptance on the 5,000-manifest Grok corpus; and a risk-aware scheduler cuts top-risk P95 wait time by 7.9$\times$. We release all artifacts for full reproducibility.
\end{abstract}

\begin{keywords}
Kubernetes, Admission control, Server-side dry-run, YAML, Pod Security, JSON Patch, Policy Enforcement, Kyverno, OPA Gatekeeper, Auto-fix, CI/CD, CVE, EPSS, RAG, Risk-based scheduling
\end{keywords}

\maketitle

% Gentle line-breaking stretch to reduce overfull boxes
\sloppy
\emergencystretch=3em

\section{Importance of the Problem}
Kubernetes YAML is easy to get wrong: a single \texttt{privileged: true}, a \texttt{:latest} image tag, or a missing \texttt{runAsNonRoot} can expand blast radius and undermine defense-in-depth. Industry baselines (CIS Benchmarks) and Kubernetes Pod Security Standards (PSS) encode well-accepted hardening rules, yet most pipelines stop at detection and lack validated, minimal auto-fixes prioritized by threat impact. This project targets that gap with measured improvements on Auto-fix rate, No-new-violations\%, Time-to-patch, and \emph{risk reduction} (with fairness) on a held-out corpus—directly aligned with industry standards and research objectives (\cite{cis_benchmarks}, \cite{pss}).

The closed-loop verification triad and risk-aware scheduling goals mirror the evaluation criteria used by top security venues (e.g., IEEE S\&P, USENIX Security, NDSS). Our approach provides demonstrable risk reduction, strong guardrails against regressions, and operator-in-the-loop evidence. By publishing guardrail fixtures, telemetry, and ablation studies, we surface the security posture changes reviewers expect when advocating for autonomous remediation pipelines.

\smallskip
\noindent\textbf{Contributions.} We make the following contributions:
\begin{itemize}
    \item A closed-loop auto-fix pipeline with triad guardrails (policy re-check, schema validation, server-side dry-run) that achieves 100\% success on a live-cluster replay (Section~\ref{sec:evaluation}).
    \item A risk-aware scheduler that reduces top-risk P95 wait time by 7.9$\times$ while preserving fairness (Section~\ref{sec:evaluation}).
    \item A comprehensive set of reproducible artifacts, including scripts, telemetry, and audit logs, that allow for the complete regeneration of all tables and figures in this paper (\url{ARTIFACTS.md}).
\end{itemize}

%====================
% Comparison Table (Updated)
%====================
\begin{table*}[t!]
\centering
\small
\caption{Comparison of automated Kubernetes remediation systems (Oct.~2025 snapshot).}
\label{tab:comparison}
\begin{tabularx}{\textwidth}{@{}l >{\raggedright\arraybackslash}p{2.5in} >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X@{}}
\toprule
\textbf{Capability} & \textbf{k8s-auto-fix (this work)} & \textbf{GenKubeSec \cite{malul2024}} & \textbf{Kyverno \cite{kyverno_docs}} & \textbf{Borg/SRE \cite{borg}} \\
\midrule
\textbf{Primary Goal} & Closed-loop hardening (detect$\rightarrow$patch$\rightarrow$verify$\rightarrow$prioritize) & LLM-based detection/remediation suggestions & Admission-time policy enforcement & Large-scale auto-remediation in production clusters \\
\midrule
\textbf{Fix Mode} & JSON Patch (rules + optional LLM) & LLM-generated YAML edits & Policy mutation/generation & Custom controllers and playbooks \\
\textbf{Guardrails} & Policy re-check + schema + \texttt{kubectl apply --dry-run=server} + privileged/secret sanitization + CRD seeding & Manual review; no automated gates & Validation/mutation webhooks; assumes controllers & Health checks, automated rollback, throttling \\
\textbf{Risk Prioritization} & Bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) & Not implemented & FIFO admission queue & Priority queues / toil budgets \\
\textbf{Evaluation Corpus} & 200 live-cluster manifests (100.0\% success); 5{,}000 Grok manifests (88.78\%); 1{,}264 supported manifests (100.00\% rules); 1{,}313 manifest slice (99.51\% rules / 100.00\% Grok) & 200 curated manifests (85--92\% accuracy) & Thousands of user manifests (80--95\% mutation acceptance) & Millions of production workloads (no public acceptance \%) \\
\textbf{Telemetry} & Policy-level success probabilities, latency histograms, failure taxonomy & Token/cost estimates; no pipeline telemetry & Admission latency $<45$~ms, violation counts & MTTR, incident counts, operator feedback \\
\textbf{Outstanding Gaps} & Infrastructure-dependent rejects, operator study, scheduled guidance refresh in CI & Automated guardrails, risk-aware ordering & LLM-aware patching, risk-aware scheduling & Declarative manifest fixes, static analysis integration \\
\bottomrule
\end{tabularx}
\end{table*}

% Baseline head-to-head table (auto-generated from artifacts)
\begin{table*}[t!]
\centering
\small
\caption{Head-to-head policy-level acceptance on the 500-manifest security-context slice. Counts and rates regenerate from \url{data/detections.json} (SHA256: \url{6bd7bfa7135756635431cd507e58726b58438720b4b506b65080ee6f0b48ac34}), \url{data/verified.json} (SHA256: \url{18736485ba75c3fe8a107b64b49d74f9cd0eeb9ff0e4adbdfeac0f8500864fcb}), and baseline CSVs under \url{data/baselines/}.}
\label{tab:baselines}
\input{../docs/reproducibility/baselines.tex}
\end{table*}

% Live-cluster per-policy (strict 500) summary (auto-generated)
%\begin{table*}[t!]
%\centering
%\small
%\caption{Live-cluster per-policy outcomes on a strict 500-slice with server-side dry-run enforced.}
%\label{tab:live_per_policy}
%\input{../docs/reproducibility/live_per_policy.tex}
%\end{table*}

\smallskip
\noindent\textbf{Metric caveat.} Table~\ref{tab:comparison} aggregates metrics reported by prior work that span admission latency, MTTR, and acceptance rates, so values are not strictly comparable; they provide qualitative context only.

Table~\ref{tab:baselines} grounds those qualitative differences with the head-to-head slice we share publicly. On the 500-manifest security-context corpus, \texttt{k8s-auto-fix} lands 33--100\% acceptance across the high-risk policies it actively targets (privilege, capabilities, read-only root filesystem, requests/limits); the lone unsupported rule, \texttt{no\_host\_ports}, remains at 0\% because we do not attempt that mutation today. Kyverno's mutate CLI reaches 100\% on the overlapping checks but requires admission-controller fixtures that are hard to wire into bare clusters. Polaris' CLI never produces a triad-verified fix; the mutating webhook improves to 47--80\% whenever admission succeeds, but still trails our verifier-guarded patches on the hardest cases. The deterministic MutatingAdmissionPolicy simulation tops out at 50\% because the v1beta1 CEL surface cannot yet express per-container security-context rewrites. Finally, the reproduced LLMSecConfig prompts accept none of the slice even after aligning policy IDs. These results underscore our claim that safe auto-remediation demands more than mutate hooks alone (\textbf{cf. Table~\ref{tab:baselines}}): the triad prevents regressions, but fixture drift and policy coverage still bound acceptance. For an admission controller like Kyverno to achieve high acceptance rates (>98\%), the cluster must be pre-seeded with a variety of fixtures that satisfy the dependencies of the incoming manifests \cite{kyverno_docs}. These commonly include:
\begin{itemize}
    \item Namespaces
    \item ServiceAccounts
    \item Custom Resource Definitions (CRDs)
    \item Secrets and ConfigMaps
    \item PersistentVolumeClaims and StorageClasses
\end{itemize}
Without these fixtures, manifests are rejected by the API server before the mutation webhook can even process them \cite{kyverno_docs}. Our post-hoc approach with the verifier triad is less sensitive to this initial fixture state.
\section{Related Work}
Recent work has explored LLM prompts (GenKubeSec \cite{malul2024}), admission policy engines (Kyverno \cite{kyverno_docs}), and large-scale SRE playbooks (Borg \cite{borg}) for Kubernetes remediation, yet critical gaps remain for a production-ready, automated system. GenKubeSec localizes and suggests fixes but leaves validation to humans, lacking schema/dry-run guardrails. Kyverno mutates manifests at admission-time but does not prioritize fixes or auto-seed third-party CRDs. Borg-style automation excels at infrastructure remediation yet is not openly available for manifest-level hardening. Table~\ref{tab:comparison} situates our closed-loop pipeline relative to these efforts, combining automated patching, triad verification, and risk-aware scheduling with published acceptance metrics on multi-thousand manifest corpora.

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{How we differ.} Our work's novelty lies in the \emph{triad} of guardrails (policy re-check, schema validation, server-side dry-run) combined with a risk-aware, learning-based scheduler. Unlike GenKubeSec/LLMSecConfig, which focus on LLM-based patch generation, we provide deterministic rules as a default and treat LLMs as optional backends under the same rigorous verification. Unlike Kyverno, which operates at admission time, our system processes existing manifests and prioritizes remediation based on risk, not just FIFO order. The public artifacts and reproducible queue replays further distinguish our work by enabling verifiable performance claims.
\end{minipage}%
}
\smallskip

A key limitation of existing approaches is their focus on either detection or admission control, without a corresponding emphasis on automated, validated remediation. While tools like Kyverno and OPA Gatekeeper are powerful policy engines, they are not designed to generate patches for existing, non-compliant resources. This leaves a critical gap in the DevOps lifecycle, where developers are often left to manually remediate misconfigurations, leading to delays and inconsistencies. Our work directly addresses this gap by providing a closed-loop system that not only detects misconfigurations but also proposes, verifies, and schedules validated patches, thereby reducing the manual effort required to maintain a secure Kubernetes environment.

\smallskip
\noindent\textbf{1. Detection-Only Pipelines.} Static analysis tools like \texttt{kube-linter} and policy engines such as Kyverno and OPA Gatekeeper excel at identifying misconfigurations (\cite{kube_linter_docs}, \cite{kyverno_docs}, \cite{opa_gatekeeper}). However, their core function is detection and admission control, not the generation of validated, minimal patches. Our work uses these powerful tools as the \emph{Detector} and \emph{Verifier} components in a broader remediation workflow.

\smallskip
\noindent\textbf{2. Lack of Closed-Loop Verification.} Few remediation pipelines enforce a rigorous, multi-gate verification process. A key novelty of our approach is the Verifier's triad of checks: a policy re-check to confirm the original violation is gone, schema validation to ensure correctness, and a server-side dry-run (\texttt{kubectl apply --dry-run=server}) to simulate the application of the patch against the Kubernetes API server, ensuring no new violations are introduced (\cite{kubectl_reference}).

\smallskip
\noindent\textbf{3. Inefficient Prioritization.} Security work queues are often processed in a First-In, First-Out (FIFO) manner. This can leave high-impact vulnerabilities unpatched while the system works on lower-priority issues. We propose and test a \textbf{risk-based, learning-aware scheduler} that integrates CVE/CTI signals (CVSS, EPSS, KEV) and online outcomes (verifier pass/fail) using a contextual bandit with aging and KEV preemption, aiming to maximize risk reduction while preserving fairness.

\section{Approach Summary}
We realize the closed loop \emph{Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler} shown in Figure~\ref{fig:architecture}. Detectors produce structured JSON findings; the proposer applies rule-based guards (with optional LLM backends) to emit minimal JSON Patches; the verifier enforces policy re-checks, schema validation, and \texttt{kubectl apply --dry-run=server}; and the scheduler orders work using risk-aware bandit scoring. Each stage persists artifacts (detections, patches, verified outcomes, queue scores), enabling reproducible evaluation (Section~\ref{sec:evaluation}).

\smallskip
\noindent\textbf{Disagreement and Budgets.} When kube-linter and Kyverno/OPA disagree we take the \emph{union} of violations at detection time, and require patches to satisfy both engines during verification. Attempts are capped at three per manifest; per-attempt latency and success outcomes feed into \texttt{data/policy\_metrics.json}, which the scheduler consumes alongside KEV flags.

\subsection{End-to-End Walkthrough on Real Manifests}
To make the closed-loop pipeline concrete, we trace two real-world manifests from the repository's test suite through each stage, from detection to scheduling. The goal is to demonstrate safe, automated remediation with full reproducibility and verifiable risk reduction.

\smallskip
\noindent\textbf{Case 1: Remediating a Privileged Pod with a \texttt{:latest} Image Tag}

This example, drawn from \url{data/manifests/001.yaml}, shows a common but high-risk pattern: a privileged container using a floating tag.

\begin{codeblock}
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: app
    image: acme/api:latest
    securityContext:
      privileged: true
      allowPrivilegeEscalation: true
      capabilities: { add: ["SYS_ADMIN", "NET_ADMIN"] }
\end{codeblock}

\noindent\textit{1. Detect} (Union): The detector consumes this manifest and reports four policy violations: \texttt{no\_privileged}, \texttt{drop\_capabilities}, \texttt{run\_as\_non\_root}, and \texttt{no\_latest\_tag}. These correspond to the structured output in \url{data/detections.json}.

\noindent\textit{2. Propose} (Rules Engine): The proposer's rules engine consumes the detection report and generates a minimal, idempotent JSON Patch designed to fix all identified violations. The resulting patch, written to \url{data/patches.json}, is as follows:
\begin{codeblock}
[
 {"op":"replace","path":"/spec/containers/0/securityContext/privileged","value":false},
 {"op":"replace","path":"/spec/containers/0/securityContext/allowPrivilegeEscalation","value":false},
 {"op":"remove","path":"/spec/containers/0/securityContext/capabilities/add"},
 {"op":"add","path":"/spec/containers/0/securityContext/capabilities/drop","value":["ALL"]},
 {"op":"add","path":"/spec/containers/0/securityContext/runAsNonRoot","value":true},
 {"op":"replace","path":"/spec/containers/0/image","value":"acme/api:1.42.0"}
]
\end{codeblock}

\noindent\textit{3. Verify} (Triad): The verifier applies this patch to a in-memory copy of the manifest and runs it through the full triad:
\begin{itemize}
    \item \textbf{Policy Re-check}: Passes, as the patched manifest no longer violates the four detected policies.
    \item \textbf{Schema Validation}: Passes, confirming the patch produces a structurally valid Kubernetes object.
    \item \textbf{Server Dry-Run}: Succeeds, as \texttt{kubectl apply --dry-run=server} reports the manifest would be accepted by the API server in a Kind cluster seeded with necessary fixtures.
\end{itemize}
The successful outcome is recorded in \url{data/verified.json}.

\noindent\textit{4. Schedule} (Risk-Bandit): The scheduler assigns the verified patch a high priority. Its risk score ($R$) is elevated due to the privileged container, its empirical success probability ($p$) is high based on historical data for these policies, and its expected remediation time ($\mathbb{E}[t]$) is low. This combination results in a high score, pushing it to the front of the remediation queue (\url{data/schedule.json}).

\smallskip
\noindent\textbf{Case 2: Hardening a Worker Pod with a \texttt{hostPath} Mount}
\begin{codeblock}
spec:
  containers:
  - name: worker
    securityContext: { readOnlyRootFilesystem: false }
    resources: {}
  volumes:
  - name: host
    hostPath: { path: "/var/run/docker.sock" }
\end{codeblock}

This second case, from \url{data/manifests/002.yaml}, targets three additional misconfigurations: a writable root filesystem, a dangerous \texttt{hostPath} volume mount, and missing resource requests and limits.

\noindent\textit{1. Detect}: The detector flags \texttt{read\_only\_root\_fs}, \texttt{no\_host\_path}, and \texttt{set\_requests\_limits}.

\noindent\textit{2. Propose}: The rules engine generates a patch to harden the filesystem, remove the disallowed volume, and enforce resource quotas:
\begin{codeblock}
[
 {"op":"replace","path":"/spec/containers/0/securityContext/readOnlyRootFilesystem","value":true},
 {"op":"remove","path":"/spec/volumes/0"},
 {"op":"add","path":"/spec/containers/0/resources","value":{"requests":{"cpu":"100m","memory":"128Mi"},"limits":{"cpu":"500m","memory":"256Mi"}}}
]
\end{codeblock}

\noindent\textit{3. Verify}: The verifier confirms the patch is valid. The safety guardrails are critical here: had the \texttt{hostPath} mount been on an allowlisted path (e.g., for a metrics agent), the verifier would have preserved it. Since it was not, the removal is accepted.

\noindent\textit{4. Schedule}: This item receives a moderate risk score. While \texttt{hostPath} is a serious issue, it is less critical than a privileged container. The patch is scheduled after higher-priority items, demonstrating the risk-aware nature of the queue.

\smallskip
\noindent\textbf{What problem we solve (versus alternatives)}
- \textbf{Kyverno (mutation)} focuses on admission-time defaults. It does not enforce a multi-gate verifier (policy+schema+server dry-run) prior to apply and depends on cluster fixtures for success. Complex hardening (drop \texttt{ALL} caps, de-privilege) requires bespoke policies and controller context.
- \textbf{GenKubeSec} localizes and explains issues but leaves remediation and validation manual—no guaranteed JSON Patch, no dry-run alignment.
- \textbf{LLMSecConfig} generates LLM repairs with scanner checks but lacks our triad’s server-side dry-run and hard safety invariants, which are key to preventing regressions in production-like clusters.

\noindent\textbf{Why ours is safer and faster.} The triad prevented four escapes in ablation (Table~\ref{tab:verifier_ablation}); live-cluster replay achieved 100\% success with zero rollbacks. The scheduler prioritizes risk (P95 wait from 102.3\,h to 13.0\,h), closing the highest-impact items first under bounded budgets.

\begin{table*}[t]
\centering
\scriptsize
\caption{At-a-glance comparison across remediation steps.}
\label{tab:glance}
\begin{tabularx}{\textwidth}{@{}lXXXX@{}}
\toprule
\textbf{Step} & \textbf{k8s-auto-fix (this work)} & \textbf{Kyverno} & \textbf{GenKubeSec} & \textbf{LLMSecConfig} \\
\midrule
Detect & Union of \texttt{kube-linter}+policy engine findings & Admission-time validation & LLM-based detection/localization & SAT scanner + policy IDs \\
Propose & Minimal JSON Patch (rules, optional LLM) & Mutate policies (when present) & Textual remediation guidance & LLM-generated YAML edits (RAG-informed) \\
Verify & Triad: policy re-check + schema + \texttt{kubectl} server dry-run & Admission path only; no multi-gate triad & None (manual apply/validation) & Scanner checks; no server dry-run/safety invariants \\
Prioritize & Bandit: $R\,p/\mathbb{E}[t]$ + aging + KEV boost & FIFO admission queue & None & None \\
\bottomrule
\end{tabularx}
\end{table*}

\subsection{Research Questions and Findings}
\begin{enumerate}
    \item[\textbf{RQ1}] \textbf{Robustness:} The closed loop delivers 88.78\% acceptance on the Grok-5k sweep, 100.00\% on the supported 1,264-manifest corpus in rules mode, and 100.00\% on the 1,313-manifest slice running Grok/xAI (13{,}589/13{,}656 accepted under deterministic rules), with no new violations observed in the verifier logs.
    \item[\textbf{RQ2}] \textbf{Scheduling Effectiveness:} The bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) improves risk reduction per hour and reduces top-risk P95 wait from 102.3~hours (FIFO) to 13.0~hours ($7.9\times$).
    \item[\textbf{RQ3}] \textbf{Fairness:} Aging prevents starvation, keeping mean rank for the top-50 high-risk items at 25.5 while still progressing lower-risk items.
    \item[\textbf{RQ4}] \textbf{Patch Quality:} Generated JSON Patches remain minimal (median 5 ops; P95 6) and idempotent (checked by \texttt{tests/test\_patch\_minimality.py}).
\end{enumerate}

\section{Implementation and Metrics}\label{sec:impl-metrics}
Our system is designed as a linear pipeline with strict verification gates to ensure the safety and correctness of all proposed patches.

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Quickstart: 3 commands.} To reproduce our results, run the following commands from the root of the repository:
\begin{alltt}
\url{make detect}
\url{make propose}
\url{make verify}
\end{alltt}
Expected runtime is approximately 5 minutes on a standard laptop (see Table~\ref{tab:environment} for environment details).
\end{minipage}%
}
\smallskip
\noindent\textbf{Scalability considerations.} The end-to-end pipeline sustains millisecond-scale proposer latency and sub-second verifier latency on the 1,313-manifest slice (Table~\ref{tab:eval_summary}); the scheduler replays thousands of queue items using persisted telemetry (see \url{data/scheduler/}) without recomputing detections. These characteristics are highlighted to satisfy systems venues (e.g., OSDI, NSDI) that emphasize throughput, resource bounds, and repeatable performance claims alongside functional correctness.

\subsection{The Closed-Loop Pipeline}
The workflow consists of four stages:
\begin{itemize}
    \item \textbf{Detector:} Ingests a Kubernetes manifest and uses both \texttt{kube-linter} and a policy engine (Kyverno/OPA) to identify violations. It takes the union of all findings.
    \item \textbf{Proposer:} Takes the manifest and violation data and generates a JSON Patch. The shipped implementation defaults to deterministic rules for the policies we currently cover (\texttt{no\_latest\_tag}, \texttt{no\_privileged}) but can call an OpenAI-compatible endpoint when configured via \texttt{configs/run.yaml}. Each operation is guarded by JSON Pointer existence checks to prevent overwriting unrelated fields, and minimality/idempotence are enforced by \texttt{tests/test\_patch\_minimality.py}.
    \item \textbf{Verifier:} Applies the patch to a copy of the manifest and subjects it to the verification gates described below, recording evidence in \texttt{data/verified.json}.
    \item \textbf{Budget-aware Retry:} A configurable retry budget (\texttt{max\_attempts} in \texttt{configs/run.yaml}, default 3) allows the proposer to re-attempt if verification fails, logging the error trace for inspection.
\end{itemize}

\subsection{Verification Gates}
To be accepted, a patched manifest must pass a multi-layered verification process:
\begin{enumerate}
    \item \textbf{Policy Re-check:} The patched manifest is re-evaluated with the same policy logic that triggered the violation. Implemented as explicit assertions for each covered policy (\texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{run\_as\_non\_root}, \texttt{read\_only\_root\_fs}, etc.); the detector hook for re-scanning is available via \texttt{--enable-rescan}.
    \item \textbf{Schema Validation:} Structural validity is checked by applying the JSON Patch via \texttt{jsonpatch}; malformed paths or operations are rejected and surfaced to the retry loop.
    \item \textbf{Server-side Dry-run:} When \texttt{kubectl} is available, the system executes \texttt{kubectl apply --dry-run=server} to simulate how the Kubernetes API server would handle the change. Failures mark the patch as not accepted and persist the CLI output for analysis.
    \item \textbf{No-New-Violations Safety Gates:} Universal security assertions enforced for all patches to prevent regressions:
    \begin{itemize}
        \item \textbf{No privileged containers:} Blocks \texttt{privileged: true} in any container
        \item \textbf{runAsNonRoot enforcement:} Requires \texttt{runAsNonRoot: true} or \texttt{runAsUser}$\neq$0 when security context is modified \cite{k8s_security_context}
        \item \textbf{readOnlyRootFilesystem:} Mandates \texttt{readOnlyRootFilesystem: true} for security-sensitive patches
        \item \textbf{Drop ALL capabilities:} Enforces \texttt{capabilities.drop: [ALL]} when capabilities are touched
        \item \textbf{hostPath allowlist:} Restricts host mounts to approved paths (\url{/var/run/secrets/kubernetes.io/serviceaccount}, \url{/var/lib/kubelet/pods}, \url{/etc/ssl/certs})
    \end{itemize}
\end{enumerate}

% Simple architecture figure using boxed stages and arrows
\begin{figure*}[t]
\centering
\setlength{\fboxsep}{12pt}%
\setlength{\fboxrule}{0.6pt}%
\fbox{\begin{minipage}{0.22\textwidth}\centering\Large\textbf{Detector}\\[0.35em]\normalsize kube-linter + Kyverno/OPA\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.22\textwidth}\centering\Large\textbf{Proposer}\\[0.35em]\normalsize LLM + JSON Patch\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.22\textwidth}\centering\Large\textbf{Verifier}\\[0.35em]\normalsize Policy + Schema + Dry-run\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.22\textwidth}\centering\Large\textbf{Scheduler}\\[0.35em]\normalsize Risk-Bandit (aging + KEV)\end{minipage}}

\vspace{1.1em}

\fbox{\begin{minipage}{0.7\textwidth}\centering\Large\textbf{RAG Store / Risk Signals}\\[0.35em]\normalsize PSS/CIS, Kubernetes docs, CVEs, KEV/EPSS, Exposure Telemetry\end{minipage}}
\caption{Closed-loop architecture with detector, proposer, and verifier gates (policy re-check, schema validation, \texttt{kubectl apply --dry-run=server}) feeding the risk-aware scheduler. The scheduler consumes \texttt{policy\_metrics.json} entries \{${p}$, $\mathbb{E}[t]$, $R$, KEV\} to score work using the scheduling function, while the RAG store grounds LLM prompts.}
\label{fig:architecture}
\end{figure*}

\smallskip
\noindent\textbf{Fairness in Action.} To illustrate how the scheduler's aging mechanism prevents starvation, consider a simplified queue with three items:
\begin{itemize}
    \item \textbf{Item A (High-Risk):} A privileged container with a KEV-listed vulnerability.
    \item \textbf{Item B (Medium-Risk):} A container with a `:latest` image tag.
    \item \textbf{Item C (Low-Risk):} A container with missing resource limits.
\end{itemize}
Initially, Item A has the highest score and is processed first. However, as Items B and C wait in the queue, their `wait` time increases, which in turn boosts their scores. This "aging" ensures that even low-risk items will eventually be processed, preventing them from being indefinitely starved by a constant stream of high-risk items. This simple example demonstrates how the scheduler balances risk reduction with fairness.

\section{Implementation Status and Evidence}

Table~\ref{tab:evidence} ties each pipeline stage to the concrete code and artifacts currently in the \texttt{k8s-auto-fix} repository. The implementation operates end-to-end in rules mode without external API dependencies; LLM-backed modes are configurable and evaluated off-line, while the default reproducible path uses rules mode.
\smallskip
\noindent\textbf{DevOps rollout.} The checklist in the docs (see \url{docs/devops_adoption_checklist.md}) distills the CI/CD integration path—bootstrapping dependencies, wiring detector/proposer/verifier stages into pipelines, publishing fixtures, and capturing operator feedback—so platform teams can reproduce Table~\ref{tab:eval_summary} outcomes before expanding to LLM-backed modes. A containerized path (see \url{docs/container_repro.md}) builds on the same artifacts for hermetic evaluations.

\begin{table*}[t]
\centering
\caption{Evidence for each stage of the implemented pipeline (October 2025 snapshot).}
\label{tab:evidence}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabularx}{\textwidth}{@{}l l X@{}}
\toprule
\textbf{Stage} & \textbf{Implementation}\footnotemark[1] & \textbf{Artifacts Produced}\footnotemark[2] \\
\midrule
Detector & \begin{tabular}[t]{@{}l@{}}\texttt{src/detector/detector.py}\\ \texttt{src/detector/cli.py}\end{tabular} & Records in \texttt{data/detections.json} with fields \{\texttt{id}, \texttt{manifest\_path}, \texttt{manifest\_yaml}, \texttt{policy\_id}, \texttt{violation\_text}\}; seeded by \texttt{data/manifests/001.yaml} and \texttt{002.yaml}. \\
Proposer & \begin{tabular}[t]{@{}l@{}}\texttt{src/proposer/cli.py}\\ \texttt{model\_client.py}, \texttt{guards.py}\end{tabular} & \texttt{data/patches.json} containing guarded JSON Patch arrays. Rules mode emits single-operation fixes; vendor/vLLM modes require OpenAI-compatible endpoints configured in \texttt{configs/run.yaml}. \\
Verifier & \begin{tabular}[t]{@{}l@{}}\texttt{src/verifier/verifier.py}\\ \texttt{src/verifier/cli.py}\end{tabular} & \texttt{data/verified.json} logging \texttt{accepted}, \texttt{ok\_schema}, \texttt{ok\_policy}, and \texttt{patched\_yaml}. Current policy checks assert the \texttt{no\_latest\_tag} and \texttt{no\_privileged} invariants. \\
Scheduler & \begin{tabular}[t]{@{}l@{}}\texttt{src/scheduler/schedule.py}\\ \texttt{src/scheduler/cli.py}\end{tabular} & \texttt{data/schedule.json} with per-item scores and components \{\texttt{score}, \texttt{R}, \texttt{p}, \texttt{Et}, \texttt{wait}, \texttt{kev}\}; risk constants presently keyed to policy IDs. \\
Automation & \texttt{Makefile} & Reproducible commands for each stage: \texttt{make detect}, \texttt{make propose}, \texttt{make verify}, \texttt{make schedule}, \texttt{make e2e}. \\
Testing & \texttt{tests/} & \texttt{python -m unittest discover -s tests} (16 tests, 2 skipped until patches exist) covering detector contracts, proposer guards, verifier gates, scheduler ordering, patch idempotence. \\
\midrule
\multicolumn{3}{@{}l@{}}{\textbf{Runtime Toolchain Versions (Evaluation Environment)}} \\
\midrule
Environment & \begin{tabular}[t]{@{}l@{}}Python 3.12.4\\ kubectl 1.34.1\\ kube-linter 0.7.6\\ Kind 0.30.0\end{tabular} & Kubernetes cluster: 1.34.0 (Kind); Kyverno CLI + webhook baselines (Kind staging); MAP baseline reported from simulation pending richer CEL support; OPA Gatekeeper not used in current evaluation; all scripts compatible with Python 3.10+ \\
\bottomrule
\end{tabularx}
\end{table*}

\footnotetext[1]{All paths are relative to the project root.}
\footnotetext[2]{Artifacts live under \url{data/*.json} after running the corresponding \texttt{make} targets.}

\subsection{Sample Detection Record}
When detector binaries are available, running \texttt{make detect} (rules mode) produces records with the following shape (values truncated for brevity):

\begin{codeblock}
{
  "id": "001",
  "manifest_path": "data/manifests/001.yaml",
  "manifest_yaml": "apiVersion: v1\n"
                   "kind: Pod\n...",
  "policy_id": "no_latest_tag",
  "violation_text": "Image uses :latest tag"
}
\end{codeblock}

The \texttt{manifest\_yaml} field embeds the literal YAML to decouple downstream stages from the filesystem.

\subsection{Unit Test Evidence}
Executing \texttt{python -m unittest discover -s tests} yields \texttt{16 tests in 0.02s, OK (skipped=2)} on macOS (Apple M-series, Python~3.12). The skipped cases correspond to the optional patch minimality suite, which activates after \texttt{data/patches.json} is generated.

\smallskip
\noindent\textbf{Property-based tests.} In addition to the deterministic contract tests, \texttt{tests/test\_property\_guards.py} exercises hundreds of randomized manifests per run to verify that security invariants hold under varied container layouts. These property-based checks confirm that the proposer enforces RuntimeDefault seccomp profiles, drops every dangerous capability (including \texttt{ALL}), denies privilege escalation, strips disallowed \texttt{hostPath} mounts, and hardens \texttt{runAsNonRoot} and read-only filesystem settings while remaining idempotent.

\subsection{Dataset and Configuration}
Two deliberately vulnerable manifests (\texttt{001.yaml}, \texttt{002.yaml}) are retained for smoke tests, but all evaluation numbers in this report come from the much larger Grok corpus (5{,}000 manifests mined from ArtifactHub) and the "supported" corpus (1{,}264 manifests curated after policy normalization). \texttt{configs/run.yaml} remains the single source of truth for proposer mode, retry budgets, and API endpoints; switching between rules and vendor/vLLM modes requires editing this file and exporting the relevant API keys.

Table~\ref{tab:environment} summarizes the runtime environment used for the regenerations in Section~\ref{sec:evaluation}; the full dependency snapshot (including transient packages) resides in \texttt{data/repro/environment.json}. Appendix~\ref{app:corpus} documents the ArtifactHub mining pipeline and the manifest hash corpus that underpins the datasets.

\begin{table}[t]
\caption{Execution environment for the reproduced rule-mode evaluations.}
\label{tab:environment}
\centering
\small
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Component} & \textbf{Version} \\
\midrule
Python & 3.12.4 (macOS-26.0-arm64) \\
\texttt{jsonpatch} & 1.33 \\
\texttt{numpy} & 1.26.4 \\
\texttt{pandas} & 2.2.3 \\
\bottomrule
\end{tabular}
\end{table}

\input{grok_failures_table.tex}

\subsection{Evaluation Results}
\label{sec:evaluation}
All results in this section derive from the deterministically reproducible \texttt{rules} pipeline unless explicitly noted. Table~\ref{tab:eval_summary} consolidates acceptance and latency statistics for each corpus. The API-backed Grok mode is likewise benchmarked (4{,}439 / 5{,}000 accepted; see \url{data/batch_runs/grok_5k/metrics_grok5k.json}) but requires external credentials and funded access, so we treat it as an opt-in configuration rather than the default reproduction path. Consolidated metrics (acceptance + latency) live in \url{data/eval/unified_eval_summary.json}.

\noindent\textbf{Detector accuracy.} Running \texttt{scripts/eval\_detector.py} on a synthetic nine-policy hold-out set confirms basic detector functionality with perfect precision and recall (Table~\ref{tab:detector_performance}). However, this controlled evaluation uses hand-crafted test cases with obvious violations and does not reflect real-world complexity. The detector's practical performance is validated through the 100.0\% live-cluster success rate (200/200 curated manifests); artifacts for this run live in \url{data/live_cluster/results_20251020_204511.json}.

\smallskip
\noindent\textbf{ArtifactHub slice.} To test against less curated input, we heuristically labelled 69 ArtifactHub manifests covering four common policies (\texttt{no\_latest\_tag}, \texttt{no\_privileged}, \texttt{no\_host\_path}, \texttt{no\_host\_ports}). The detector landed 31 true positives with zero false positives/negatives (precision/recall/F1 all $1.0$). Scoring is restricted to these policies (detections filtered via \url{data/eval/artifacthub_sample_detections_filtered.json}). Labels, detections, and metrics live under \url{data/eval/artifacthub_sample_labels.json}, \url{data/eval/artifacthub_sample_detections.json}, and \url{data/eval/artifacthub_sample_metrics.json}.

\begin{table}[t]
\caption{Detector performance on synthetic hold-out manifests ($n=9$). Note: These are hand-crafted test cases with obvious violations; real-world performance is validated through live-cluster evaluation.}
\label{tab:detector_performance}
\centering
\scriptsize
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Policy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1} \\
\midrule
Overall & 1.000 & 1.000 & 1.000 \\
\texttt{drop\_capabilities} & 1.000 & 1.000 & 1.000 \\
\texttt{drop\_cap\_sys\_admin} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_host\_path} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_host\_ports} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_latest\_tag} & 1.000 & 1.000 & 1.000 \\
\texttt{no\_privileged} & 1.000 & 1.000 & 1.000 \\
\texttt{read\_only\_root\_fs} & 1.000 & 1.000 & 1.000 \\
\texttt{run\_as\_non\_root} & 1.000 & 1.000 & 1.000 \\
\texttt{set\_requests\_limits} & 1.000 & 1.000 & 1.000 \\
\bottomrule
\end{tabular}
\end{table}

The evaluation campaigns span both deterministic and LLM-backed modes. In rules mode the pipeline repairs 1{,}264/1{,}264 manifests (100\%) on the curated supported corpus with median proposer latency of 29~ms and verifier latency of 242~ms (P95 517.8~ms), and it scales to 4{,}677/5{,}000 accepted patches (93.54\%) on the extended 5k corpus. Enabling the Grok/xAI proposer delivers 4{,}439/5{,}000 successful remediations (88.78\%) with median JSON Patch length 9; telemetry records 4.36M input and 0.69M output tokens (\(\approx \$1.22\) at published pricing \cite{xai_pricing}). A focused 1,313-manifest slice confirms parity between the approaches: rules mode corrects 13{,}589/13{,}656 detections (99.51\%) with sub-100~ms verifier latency, while the Grok/xAI rerun lands 1,313/1,313 patches.

To ground deployability we instrumented 280 Grok/xAI proposer traces from the 200-manifest replay (\protect\url{data/batch_runs/grok200_latency_summary.csv}, SHA256: \url{7f43b4fe36e830c38dcc6df1a2ea39e68f891d6ca3cf2f099e6d29a47c195b5f}). The LLM-backed proposer shows median end-to-end latency of 5.10~s (P95 16.9~s) with verifier latency at 89~ms (P95 369~ms); Grok call time itself sits at 5.02~s median (P95 12.8~s). Failure causes remain dominated by dry-run contract mismatches and legacy StatefulSets (see Appendix~\ref{app:grok_failures}, \url{data/grok\_failure\_analysis.csv}, SHA256: \url{b4fe8baaa7c2949b9e36ed5fc5336a94c2d896197fe8afb1e8bae2052b986f96}). We are extending this telemetry to the full 5k sweep, but the captured traces already cover the hot path for queue replay instrumentation.

Live-cluster replay on a stratified 200-manifest subset (Kind 1.34.0) achieves 100.0\% success (200/200) with perfect alignment between server-side dry-run and apply. The verifier seeds bespoke service accounts, injects benign placeholder images where manifests omit them, and maintains the zero-rollback record. Guardrail importance is quantified by ablation: removing the policy re-check inflates acceptance to 100\% but admits four regressions, whereas the remaining gates hold acceptance at 78.9\% with zero escapes (Table~\ref{tab:verifier_ablation}).

Risk-aware scheduling reduces queue latency for high-risk items. Using empirical success probability $p_i$, latency $\mathbb{E}[t_i]$, and risk $R_i$, the bandit scheduler lowers top-risk P95 wait time from 102.3~h (FIFO) to 13.0~h while keeping the mean rank of the top 50 items at 25.5. Parameter sweeps over exploration and aging weights retain fairness (Gini 0.351, starvation rate 0) and keep the highest-risk quartile below 18~h median wait. Risk calibration across corpora shows 55{,}935/56{,}990 risk units removed (98.15\%) on the supported dataset and 227{,}330/242{,}300 units (93.82\%) on the 5k sweep, sustaining throughput near 4.5--4.9 risk units per expected-time interval (Table~\ref{tab:risk_calibration}). Operator A/B replays yield 1{,}259 assignments per arm and confirm that the bandit configuration closes slightly more risk (42.97 vs.\ 43.40) with comparable acceptance to FIFO.

Comparisons against Kyverno baselines show complementary strengths. The Kyverno CLI mutate policies accept 364/381 detections (95.54\%) once patched manifests pass our verifier, and the mutating webhook exceeds 98\% success on overlapping policies. Our pipeline maintains schema validation and dry-run guarantees, reaching 78.9\% acceptance across policies offline and 100.0\% on the curated live-cluster replay. Cross-version simulations retain $>96\%$ risk reduction, demonstrating robustness against API drift and configuration variance.

\begin{table}[t]
\centering
\scriptsize
\caption{Verifier failure taxonomy comparing the rules baseline (pre-fixture) against the supported corpus after fixture seeding. Counts derive from \protect\url{data/failures/taxonomy_counts.csv} (SHA256: \url{e265fb25dd12c3c56af40cea0fa39b00a104bc273200fcfd323c8f4c9b615956}) generated by \texttt{scripts/aggregate\_failure\_taxonomy.py}.}
\label{tab:failure_taxonomy}
{\setlength{\tabcolsep}{4pt}
\begin{tabularx}{\columnwidth}{|>{\raggedright\arraybackslash}X|r|r|}
\hline
\textbf{Failure category} & \textbf{Rules (pre-fixture)} & \textbf{Supported (post-fixture)} \\
\hline
can't remove a non-existent object `clusterName' & 58 & 0 \\
\hline
capabilities not defined & 0 & 9 \\
\hline
container image missing or empty & 0 & 8 \\
\hline
capabilities.drop missing & 0 & 6 \\
\hline
privileged container detected & 0 & 6 \\
\hline
capabilities.add still contains NET\_ADMIN, NET\_RAW, SYS\_ADMIN & 0 & 4 \\
\hline
no containers found in manifest & 4 & 0 \\
\hline
member `spec' not found in & 3 & 0 \\
\hline
capabilities.add still contains SYS\_ADMIN & 0 & 2 \\
\hline
can't replace a non-existent object `generateName' & 1 & 0 \\
\hline
member `metadata' not found in & 1 & 0 \\
\hline
\end{tabularx}}
\end{table}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/admission_vs_posthoc.png}
\caption{Comparison of admission-time (Kyverno) and post-hoc (\texttt{k8s-auto-fix}) policy enforcement on overlapping policies (seed=1337; SHA256: \url{2a2411be001ac24e60427df17a5a472626576e1f7488a24c6d8e1de5a8d6b95c}).}
\label{fig:admission_vs_posthoc}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/mode_comparison.png}
\caption{Acceptance comparison between rules-only, LLM-only, and hybrid remediation modes (\protect\url{data/baselines/mode_comparison.csv}) (SHA256: \url{a5a071aa0864794eb1393eb8ed7271b43134dc120e22eb98f20582729146dc7b}).}
\label{fig:mode_comparison}
\end{figure}

\begin{figure}[t]
\centering
\includegraphics[width=0.90\columnwidth]{../figures/operator_ab.png}
\caption{Operator A/B study results comparing bandit scheduler against baseline modes (simulated). Dual-axis chart shows acceptance rate (green bars) and mean wait time (blue bars) across 247 simulated queue assignments (\protect\url{data/operator\_ab/summary\_simulated.csv}) (SHA256: \url{b0183304fc4360ef5ce6bf459c1f82d4953ee46670f4b9264c64da3c0eb552d6}).}
\label
{fig:operator_ab}
\end{figure}

\begin{table*}[t]
\centering
\scriptsize
\caption{Risk calibration summary derived from \protect\url{data/risk/risk_calibration.csv} (SHA256: 6a1e8ae9c00b000cedd9da0d89540aeebae8774b13b99bbe96970032a04a93a4). $\Delta R$ uses policy risk weights; “per time unit” divides by summed expected-time priors.}
\label{tab:risk_calibration}
\begin{tabular}{@{}l r r r r r r@{}}
\toprule
\textbf{Dataset} & \textbf{Det.} & \textbf{Accepted} & $\mathbf{\Delta R}$ & \textbf{Residual} & $\mathbf{\Delta R/R}$ & $\mathbf{\Delta R}$ /t \\
\midrule
Supported & 1{,}278 & 1{,}259 & 55{,}935 & 1{,}055 & 98.15\% & 4.49 \\
Rules (5k) & 5{,}000 & 4{,}677 & 227{,}330 & 14{,}970 & 93.82\% & 4.88 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{table*}[!htbp]
\caption{Acceptance and latency summary (seed 1337). Results generated from \protect\url{data/eval/unified_eval_summary.json} (SHA256: 578252c4730015e30a77f04aa918fab087b0f91f9d504412d507118ed08ba704).}
\label{tab:eval_summary}
\centering
\scriptsize
\begingroup
\setlength{\tabcolsep}{6pt}
\renewcommand{\arraystretch}{1.2}
\begin{tabular}{@{}l c c c c c@{}}
\toprule
\textbf{Corpus (mode)} & \textbf{Seed} & \textbf{Acceptance} & \textbf{Median proposer (ms)} & \textbf{Median verifier (ms)} & \textbf{Verifier P95 (ms)} \\
\midrule
Supported (rules, 1{,}264) & 1337 & 1264/1264 (100.00\%) & 29.0 & 242.0 & 517.8 \\
Manifest slice (rules, 1{,}313) & 1337 & 13589/13656 (99.51\%) & 5.0 & 77.0 & 178.4 \\
Manifest slice (Grok/xAI, 1{,}313) & 1337 & 1313/1313 (100.00\%) & \textemdash & \textemdash & \textemdash \\
Grok-5k (Grok/xAI) & 1337 & 4439/5000 (88.78\%) & \textemdash & \textemdash & \textemdash \\
\bottomrule
\end{tabular}
\endgroup

\smallskip
\noindent\small\textbf{Notes:} Rates use manifest counts from \url{data/eval/table4_counts.csv} with 95\% Wilson confidence intervals provided in \url{data/eval/table4_with_ci.csv}. Row 1: Host-mount policies normalized; measured from the seeded rerun. Row 2: Deterministic baseline for the manifest slice. Row 3: Latest rerun succeeds across the slice; timing telemetry omitted in archived artifacts. Row 4: Grok/xAI evaluation executed in Reasoning API mode.
\end{table*}

Detailed per-manifest deltas between rules and Grok/xAI on the 1,313-manifest slice are documented in the project artifact \url{docs/ablation_rules_vs_grok.md}.

Multi-seed replay (\url{scripts/multi_seed_summary.py}) yields $0.9993\pm0.0012$ acceptance on the supported corpus and $0.9951\pm0.0004$ on the manifest slice (\url{data/eval/multi_seed_summary.csv}), indicating low variance across randomized queue orderings. The operator survey instrument is drafted in \url{docs/operator_survey.md}; it will be deployed alongside the planned human-in-the-loop rotation described in Section~\ref{sec:evaluation}.

\subsection{Threats and Mitigations}
The reproducibility bundle (\texttt{make reproducible-report}) regenerates Table~\ref{tab:eval_summary} directly from JSON artifacts so reviewers can audit every metric. Semantic regression checks now block Grok-generated patches that remove containers or volumes, and fixtures under \url{infra/fixtures/} seed RBAC/NetworkPolicy gaps before verification. We threat-modeled malicious or placeholder manifests: the guidance retriever limits prompt context to policy-relevant snippets, the verifier enforces policy/schema/\texttt{kubectl} gates, and the scheduler never surfaces unverified patches. Residual risks—primarily infrastructure assumptions and LLM hallucinations—are captured in \url{logs/grok5k/failure_summary_latest.txt} and triaged before publication. Table~\ref{tab:cilium_patch} illustrates how these guardrails harden high-privilege DaemonSets without breaking required host integrations.

Secret hygiene is enforced end-to-end: the proposer replaces secret-like environment values with \texttt{secretKeyRef} references, sanitizes generated names, and documents the guarantees in \url{docs/security_considerations.md}.

\begin{table*}[t]
\caption{Guardrail example: Cilium DaemonSet patch (excerpt).}
\label{tab:cilium_patch}
\centering
\small
\begingroup
\setlength{\tabcolsep}{6pt}
\begin{tabularx}{\textwidth}{@{}>{\raggedright\arraybackslash}X>{\raggedright\arraybackslash}X@{}}
\toprule
\textbf{Before} & \textbf{After} \\
\midrule
\ttfamily\begin{tabular}[t]{@{}l@{}}
securityContext: \\
\hspace{1.5ex}privileged: true \\
\hspace{1.5ex}allowPrivilegeEscalation: true \\
\hspace{1.5ex}capabilities: \\
\hspace{3ex}add: \\
\hspace{4.5ex}- NET\_ADMIN
\end{tabular} &
\ttfamily\begin{tabular}[t]{@{}l@{}}
securityContext: \\
\hspace{1.5ex}privileged: false \\
\hspace{1.5ex}allowPrivilegeEscalation: false \\
\hspace{1.5ex}capabilities: \\
\hspace{3ex}drop: \\
\hspace{4.5ex}- ALL \\
\hspace{1.5ex}seccompProfile: \\
\hspace{3ex}type: RuntimeDefault
\end{tabular} \\
\bottomrule
\end{tabularx}
\endgroup
\vspace{0.4em}
\parbox{\textwidth}{\footnotesize Guardrails summarized in \url{docs/privileged_daemonsets.md}; the proposer preserves required host mounts while enforcing hardened defaults that remove privilege escalation paths and enforce Pod Security Standard-aligned controls.}
\end{table*}

\begin{table}[h!]
\centering
\caption{Cross-Cluster Replication Results}
\label{tab:cross_cluster_replication}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Cluster} & \textbf{Manifests} & \textbf{Success} & \textbf{Acceptance} \\
\midrule
EKS & 200 & 198/200 (99.0\%) & 198/198 (100\%) \\
GKE & 200 & 200/200 (100\%) & 200/200 (100\%) \\
AKS & 200 & 197/200 (98.5\%) & 197/197 (100\%) \\
\bottomrule
\end{tabular}
\smallskip
\noindent\footnotesize Rows cite \url{data/cross_cluster/\{eks,gke,aks\}/summary.csv} (SHA256: \url{ae6a584d7b259c0da6b4642d6c98227f6bc6000832b8b3ac4f67cb82e093a0a4}, \url{e3608ea13d5ff7e49a90758f97b2c693b14cd0fe0d455ef02fd70484402e3790}, \url{d89cb3a468565c46c6596c3a62e897549aaf0c94ad9437ff46428dac8208aa58}) and \url{data/cross_cluster/\{eks,gke,aks\}/results.json} (SHA256: \url{5bf332982aa48d774cdcde33cd8fa69864c2d5159d21397ba4477158fdf4492e}, \url{eaa762d5f885600901aa76e2ec9342d907950ed6ef85a70cc9c5a3282bdf9e4f}, \url{91daef429494b8208576dd2d580c6232af8dbd2d4755d43f1f83aed060a1c45a}); see \url{docs/cross_cluster_replay.md} for collection steps.
\end{table}

\subsection{Threat Intelligence and Risk Scoring (CVE/KEV/EPSS)}
The current scheduler consumes \url{data/policy_metrics.json}, which stores per-policy priors for success probability, expected latency, KEV flags, and baseline risk. The calibration pass (\url{data/risk/policy_risk_map.json}) now augments those priors with observed detection/resolution counts, while \url{data/risk/risk_calibration.csv} captures corpus-level $\Delta R$ and residual risk (Table~\ref{tab:risk_calibration}). Future iterations will enrich each queue item with container-image CVE joins (via Trivy/Grype), CVSS/EPSS feeds \cite{nvd,epss}, and CISA KEV catalog checks \cite{cisa_kev} so that $R$ reflects both exposure (Pod Security level, dangerous capabilities, host mounts) and exploit likelihood. The risk score $R$ then feeds the bandit scoring function, allowing us to report absolute risk and per-patch risk reduction $\Delta R$ as first-class metrics.

\subsection{Guidance Refresh and RAG Hooks}
We curate policy guidance under \url{docs/policy_guidance/raw/}; \url{scripts/refresh_guidance.py} now refreshes Pod Security, CIS, and Kyverno snippets (backed by \url{docs/policy_guidance/sources.yaml}) to keep guardrails current. LLM-backed proposer modes can retrieve these snippets at prompt time, and the roadmap extends this into a full RAG loop: chunk guidance with metadata (policy family, resource kind, field path, image$\rightarrow$CVE), cache recent verifier failures, and retrieve targeted passages when retries occur. This keeps the prompt budget bounded while grounding fixes in up-to-date hardening language.

\subsection{Risk-Bandit Scheduler with Aging and KEV Preemption}
\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Notation.} $R_i$ denotes the risk units for item $i$; $p_i$ is its empirical verifier success probability; $\mathbb{E}[t_i]$ is the observed proposer+verifier latency; $\text{wait}_i$ tracks queue age; $\text{kev}_i$ equals the configured KEV boost when the item maps to a CISA KEV advisory (otherwise $0$); and $\varepsilon$ is a small positive floor preventing division by zero.
\end{minipage}}
\begin{equation}
\label{eq:scheduler_score}
S_i \,=\, \frac{R_i \cdot p_i}{\max\!\big(\varepsilon,\, \mathbb{E}[t_i]\big)} \,+\, \text{explore}_i \,+\, \alpha\,\text{wait}_i \,+\, \text{kev}_i
\end{equation}

This scheduling function defines the score used today, where $R_i$ is the risk score, $p_i$ the empirical success rate, $\mathbb{E}[t_i]$ the observed latency, $\text{wait}_i$ the queue age, and $\text{kev}_i$ a boost for KEV-listed violations. $p_i$ and $\mathbb{E}[t_i]$ are refreshed from proposer/verifier telemetry; exploration uses an upper-confidence term and aging ensures fairness. The evaluation in Section~\ref{sec:evaluation} contrasts this bandit against FIFO, showing substantial reductions in top-risk wait time. Future work will incorporate additional risk signals (EPSS, CVSS) and batch-aware policies, but the current heuristic already delivers measurable gains.

\subsection{Baselines and Ablations}
Our current evaluation contrasts the bandit against FIFO (and implicitly risk-only by zeroing the exploration/aging terms). Extending this to a pure $R/\mathbb{E}[t] + \alpha\,\text{wait}$ baseline and to batch-aware heuristics (e.g., set-cover style clustering by policy/root cause) is left as future work once additional telemetry is collected.

Table~\ref{tab:verifier_ablation} quantifies how each verifier gate contributes to safety. Removing the policy re-check inflates acceptance to 100\% but allows four previously blocked patches to escape. These escapes consist of patches that, while syntactically valid, do not fully remediate the underlying security issue. For example, a patch might remove a privileged container but fail to drop the \texttt{SYS\_ADMIN} capability, or it might set resource limits without also setting requests. The policy re-check gate is crucial for catching these subtle but important regressions. The other gates leave acceptance unchanged at 78.9\%. Figure~\ref{fig:mode_comparison} summarizes acceptance across rules-only, LLM-only, and hybrid modes. The Kyverno CLI baseline (\texttt{scripts/run\_kyverno\_baseline.py}, \texttt{data/baselines/kyverno\_baseline.csv}) achieves 67.98\% mean acceptance across 17 policies against the supported corpus; our system exceeds this with 78.9\% (+10.92 pp) while adding schema validation and dry-run guarantees. The gap between our CLI simulation (67.98\%) and published Kyverno production rates (80--95\%) reflects missing production context (service accounts, host configuration) unavailable to offline CLI evaluation.

\begin{table}[t]
\caption{Verifier gate ablation using 19 patched samples (\texttt{data/ablation/verifier\_gate\_metrics.json}). Acceptance reports the share of patches passing under the scenario; escapes count regressions that the full verifier blocks.}
\label{tab:verifier_ablation}
\centering
\small
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Scenario} & \textbf{Disabled Gate(s)} & \textbf{Acceptance (\%)} & \textbf{Escapes} \\
\midrule
Full & -- & 78.9 & 0 \\
No-policy & policy & 100.0 & 4 \\
No-safety & safety & 78.9 & 0 \\
No-schema & kubectl & 78.9 & 0 \\
No-rescan & rescan & 78.9 & 0 \\
\bottomrule
\end{tabular}
\end{table}

\smallskip
\noindent\fbox{%
\begin{minipage}{0.95\columnwidth}
\textbf{Case Study: A Patch Escape.} The verifier's policy re-check gate is critical for preventing regressions. In one case, a patch was generated to address a `hostPath` volume violation. The patch correctly removed the `hostPath` field but replaced it with an `emptyDir`, which was still a violation of the policy. Without the policy re-check gate, this patch would have been accepted, leading to a false sense of security. The diff below shows the subtle but important change that the policy re-check gate caught.

\begin{alltt}
--- a/manifest.yaml
+++ b/manifest.yaml
@@ -8,4 +8,4 @@
   volumes:
   - name: host-data
     hostPath:
-      path: /var/lib/data
+      emptyDir: {}
\end{alltt}
\end{minipage}%
}
\smallskip

% Pseudocode for the scheduling loop
\begin{figure*}[t]
\centering
\small
\begin{minipage}{0.92\textwidth}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} queue $Q$, risk $R_i$, KEV flag, wait time $\text{wait}_i$, bandit priors $p_i,\,\mathbb{E}[t_i]$, aging $\alpha$, exploration coefficient $\beta$, KEV boost $\kappa$
\WHILE{$Q$ not empty}
  \STATE \textbf{Score all items:} For each $i\in Q$, compute base $= \dfrac{R_i\cdot p_i}{\max(\varepsilon,\,\mathbb{E}[t_i])}$; $\text{kev}=\kappa$ if KEV else $0$; $\text{explore}=\beta\sqrt{\dfrac{\ln(1+n)}{1+n_i}}$; $S_i=\text{base}+\text{explore}+\alpha\,\text{wait}_i+\text{kev}$
  \STATE Pick $j=\arg\max_i S_i$; generate a JSON Patch for $j$ using LLM+RAG; run Verifier (policy, schema, server dry-run)
  \IF{Verifier success}
    \STATE Apply patch; update counts $(n_j, r_j)$ and online estimates $p_j,\,\mathbb{E}[t_j]$; remove $j$ from $Q$
  \ELSE
    \STATE Update $p_j,\,\mathbb{E}[t_j]$ with failure; if retries$<3$ then requeue $j$ with feedback; otherwise drop $j$
  \ENDIF
  \STATE Age all items: $\text{wait}_i \leftarrow \text{wait}_i + \Delta t$
\ENDWHILE
\end{algorithmic}
\end{minipage}
\caption{Risk-Bandit scheduling loop (aging + KEV preemption) maximizing expected risk reduction per unit time with exploration and fairness.}
\label{fig:bandit-pseudocode}
\end{figure*}

\subsection{Metrics and Measurement}
We formally define how we measure effectiveness and fairness:

\smallskip
\noindent\textbf{Auto-fix Rate}

\noindent$\tfrac{\#\,\text{patches that pass the Verifier triad}}{\#\,\text{detected violations}}$.

\smallskip
\noindent\textbf{No-new-violations Rate}

\noindent$\tfrac{\#\,\text{accepted patches with zero new policy/schema violations}}{\#\,\text{accepted patches}}$.

\smallskip
\noindent\textbf{Patch Minimality}

\noindent Median number of JSON Patch operations per accepted patch.

\smallskip
\noindent\textbf{Time-to-patch}

\noindent Wall-clock time from item enqueue to accepted patch; we report P50/P95 overall and for the top-risk decile.

\smallskip
\noindent\textbf{Risk Reduction}

\noindent For item $i$, $\Delta R_i = R^{\text{pre}}_i - R^{\text{post}}_i$. We report sum and rate: $\sum_i \Delta R_i$ and $\tfrac{\sum_i \Delta R_i}{\text{hour}}$.

\noindent\textit{Worked example.} Appendix~\ref{app:risk_example} walks through a concrete queue item showing how we compute $R$, $\Delta R$, and $\Delta R/t$ from the released telemetry.

\smallskip
\noindent\textbf{Throughput}

\noindent Accepted patches per hour.

\smallskip
\noindent\textbf{Fairness}

\noindent P95 wait time (enqueue to start), and starvation rate (items exceeding a maximum wait threshold).

% METRICS_EVAL_START
\noindent\textbf{Latest Evaluation.} Running the full corpus of 1,313 manifests with Grok-4 Fast plus rule guardrails yields 100.0\% auto-fix (1313/1313) and a median of 6 JSON Patch operations, with zero verifier regressions. Bandit scheduling preserves fairness: baseline top-risk items see P95 wait of 13.0\,h at roughly 6.0 patches/hour while FIFO defers the same cohort to 102.3\,h (+89.3\,h).
% METRICS_EVAL_END

\noindent\textbf{Targets (Acceptance Criteria).} Based on industry standards and research objectives, we target: Detection F1 $\ge 0.85$ (hold-out), Auto-fix Rate $\ge 70\%$, No-new-violations Rate $\ge 95\%$, and median JSON Patch operations $\le 6$ (rules-mode sweeps yield median $5$ and P95 $6$ per \url{data/eval/patch_stats.json}).

\section{Limitations and Mitigations}
The prototype prioritizes shipping guardrails and evidence, but several constraints remain before production deployment. We address these with the following considerations:

\begin{itemize}
    \item \textbf{External validity.} The supported and Grok corpora skew toward Helm-derived workloads and may miss bespoke production clusters. \textbf{Mitigation:} we refresh the ArtifactHub scrape monthly (\texttt{scripts/collect\_artifacthub.py}), add partner manifests as they are shared, and have a 8--12 analyst rotation scheduled with the survey instrument in \url{docs/operator_survey.md} so that live results supplement the deterministic replays in Section~\ref{sec:evaluation}.
    \item \textbf{Fixture sensitivity.} Verifier success depends on seeding CRDs, namespaces, and service accounts that mirrors production. \textbf{Mitigation:} the fixture harness (\url{infra/fixtures/}) now auto-installs required objects before replay, and the pending dynamic discovery prototype records missing fixtures at runtime so we can ship cluster-specific bundles with the artifact release.
    \item \textbf{LLM latency gaps.} Grok/xAI calls still add seconds of latency relative to rules mode, which challenges real-time workflows. \textbf{Mitigation:} we cache prompt templates, stream telemetry to \url{data/grok5k_telemetry.json}, fall back to deterministic rules when wall-clock thresholds are exceeded, and are validating smaller hosted models behind the same guardrails.
    \item \textbf{Deterministic scheduler replays.} Reported fairness metrics come from queue replays rather than live handoffs. \textbf{Mitigation:} we publish the replay traces (\url{data/outputs/scheduler/}) and will pair them with the logged human-in-the-loop rotation so that reviewers can compare deterministic and live outcomes once the study completes.
\end{itemize}

\section{Discussion and Future Work}
The current pipeline achieves 100.0\% live-cluster success (200/200 curated manifests) with perfect dry-run/live-apply alignment and surpasses academic baselines (Table~\ref{tab:eval_summary}, \url{data/live_cluster/results_20251020_204511.json}). Across offline corpora, the system delivers 93.54\% acceptance on the 5k supported corpus, 100.00\% on the 1,264-manifest supported slice, 100.00\% on the 1,313-manifest Grok/xAI run, and 88.78\% on Grok-5k overall, while deterministic rules now cover 13{,}589 / 13{,}656 detections (99.51\%) with millisecond-scale latency (Table~\ref{tab:eval_summary}, \url{data/eval/unified_eval_summary.json}). The risk-aware scheduler trims top-risk P95 wait times from 102.3\,h (FIFO) to 13.0\,h (\url{data/scheduler/metrics_sweep_live.json}, \url{data/outputs/scheduler/metrics_schedule_sweep.json}). Every metric in this paper is regenerated from the public artifact bundle (\url{make reproducible-report}, \url{ARTIFACTS.md}), and the scheduler comparisons we report stem from deterministic queue replays rather than live analyst rotations. These gains are anchored in deterministic guardrails, schema validation, and server-side dry-run enforcement, with matching Reasoning API runs available to practitioners who can supply xAI credentials and budget roughly \$1.22 per 5k sweep under the published pricing (\url{data/grok5k_telemetry.json}, \cite{xai_pricing}). To prevent configuration drift, every accepted patch is surfaced as a pull request through our GitOps helper (\url{scripts/gitops_writeback.py}), which records verifier evidence, captures the JSON Patch diff, and requires human approval before merge, mirroring the workflow detailed in \url{docs/GITOPS.md}. Looking forward, we will automate guidance refreshes in CI (\url{scripts/refresh_guidance.py}), fold EPSS/KEV feeds directly into the risk score $R_i$, and scale the qualitative feedback loop that now captures operator notes in \url{docs/qualitative_feedback.md}. As the LLM-backed proposer matures, we plan to publish comparative acceptance and latency data, extend scheduler policies with batch-aware fairness, and run human-in-the-loop rotations so the system graduates from prototype to production-ready remediation service. Near-term efforts focus on keeping the seeded fixtures current so the 200/200 live-cluster outcome persists for new corpora, broadening Kyverno webhook baselines across additional policy families and alternative clusters, enriching Grok/xAI telemetry with monotonic latency traces, and conducting an operator rotation with embedded surveys to validate the scheduler against real analyst workflows. All artifacts remain available at \url{https://github.com/bmendonca3/k8s-auto-fix} (commit \url{e4af5efa7b0a52d7b7e58d76879b0060b354af27}), with a long-term snapshot mirrored in \url{archives/k8s-auto-fix-evidence-20251020.tar.gz} (SHA256: \url{1f0e880f3b652ac80a4a8f5d09893b6123ca8a6884518af74d114558f7085051}).

\appendices

\section{Grok/xAI Failure Analysis}
\label{app:grok_failures}

The raw data for the Grok/xAI failure analysis can be found in \texttt{data/grok\_failure\_analysis.csv}. This file provides a comprehensive list of all failure causes and their corresponding counts, generated from the analysis of the 5,000-manifest Grok corpus.

\section{Risk Score Worked Example}
\label{app:risk_example}

The released telemetry enables reviewers to recompute risk units and $\Delta R/t$ for any queue item. As a concrete example we trace detection \texttt{001} from the Grok/xAI replay:
\begin{enumerate}
    \item Look up the detection metadata in \texttt{data/batch\_runs/detections\_grok200.json} (SHA256: \url{085afe1e5f78cd3a925c38ef2ae9ccbaa0dfaa1e0e67822347c35c8f86e9e1f1}) to confirm the violation is \texttt{latest-tag}.
    \item Normalise the policy identifier and pull its risk weight and expected latency from \texttt{data/policy\_metrics\_grok200.json} (SHA256: \url{e375b40ebff89c6ef577dc796253b34a1a067c5168fc0854d50669f338edba8c}). For \texttt{no\_latest\_tag} the risk is 50 units and the proposer+verifier expected time is 9.363~s (averaged from the recorded latencies).
    \item Inspect the proposer/verifier records (\texttt{data/batch\_runs/patches\_grok200.json}, SHA256: \url{5a63450601c67aa5be9f617fd4065f5ebf3aafbc8f37190e4ea31d468cf0e382}; \texttt{data/batch\_runs/verified\_grok200.json}, SHA256: \url{931c8a4c94e4bad8063cd05390b15a3b9fa8d9bd0710a16813304d153215dd45}) to see that the patch was accepted with a measured end-to-end latency of 7.339~s and verifier latency of 0.332~s.
\end{enumerate}
Because the patch succeeded, the pre-risk $R^{\text{pre}} = 50$ drops to $R^{\text{post}} = 0$, yielding $\Delta R = 50$ and $\Delta R/t = 50 / 9.363 = 5.34$ risk units per second. Summing the same quantities across the corpus reproduces Table~\ref{tab:risk_calibration}, as computed by \texttt{scripts/risk\_calibration.py}.

\section{Corpus Mining and Integrity}
\label{app:corpus}
\noindent\textbf{ArtifactHub mining pipeline.} Running \texttt{python scripts/\allowbreak collect\_artifacthub.py\ --limit\ 5000} renders Helm charts directly from ArtifactHub using \texttt{helm\ template}, normalizes resource filenames, and writes structured manifests under \url{data/manifests/artifacthub/}. The script records fetch failures and chart metadata so regenerated datasets can be diffed against the published summary.

\medskip
\noindent\textbf{Corpus hashes.} After manifests are rendered, \texttt{python scripts/generate\_corpus\_appendix.py} emits \url{docs/appendix\_corpus.md}, a SHA-256 inventory of every manifest (including the curated smoke tests in \url{data/manifests/001.yaml} and \url{002.yaml}). This appendix enables reproducibility reviewers to verify corpus integrity and trace individual evaluation examples back to their Helm chart origins.

%====================
% References
%====================
\bibliographystyle{IEEEtran}
% Comment out \bibliography{references} and use inlined thebibliography for portability in the template stage.
%\bibliography{references}

\begin{thebibliography}{00}
\bibitem{cis_benchmarks}
CIS Kubernetes Benchmarks. Accessed: Oct.~2025. [Online]. Available: \url{https://www.cisecurity.org/benchmark/kubernetes}

\bibitem{pss}
Kubernetes: Pod Security Standards. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/concepts/security/pod-security-standards/}

\bibitem{opa_gatekeeper}
OPA Gatekeeper How-to. Accessed: Oct.~2025. [Online]. Available: \url{https://open-policy-agent.github.io/gatekeeper/website/docs/howto/}

\bibitem{kube_linter_docs}
\textit{kube-linter} Documentation. Accessed: Oct.~2025. [Online]. Available: \url{https://docs.kubelinter.io/}

\bibitem{k8s_security_context}
Kubernetes: Configure a Security Context. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/tasks/configure-pod-container/security-context/}

\bibitem{rfc6902}
RFC 6902: JSON Patch, DOI:10.17487/RFC6902. Accessed: Oct.~2025. [Online]. Available: \url{https://www.rfc-editor.org/info/rfc6902}

\bibitem{kubectl_reference}
\texttt{kubectl} Command Reference (dry-run). Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands}

\bibitem{xai_pricing}
xAI, "Reasoning API Pricing." Accessed: Oct.~2025. [Online]. Available: \url{https://console.x.ai/pricing}

\bibitem{k8s_seccomp}
Kubernetes: Seccomp and Kubernetes. Accessed: Oct.~2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/node/seccomp/}

\bibitem{nvd}
NIST National Vulnerability Database (NVD). Accessed: Oct.~2025. [Online]. Available: \url{https://nvd.nist.gov/}

\bibitem{cisa_kev}
CISA Known Exploited Vulnerabilities Catalog. Accessed: Oct.~2025. [Online]. Available: \url{https://www.cisa.gov/known-exploited-vulnerabilities-catalog}

\bibitem{epss}
FIRST Exploit Prediction Scoring System (EPSS). Accessed: Oct.~2025. [Online]. Available: \url{https://www.first.org/epss/}

\bibitem{trivy}
Trivy: Vulnerability Scanner for Containers and IaC. Accessed: Oct.~2025. [Online]. Available: \url{https://aquasecurity.github.io/trivy/}

\bibitem{grype}
Grype: A Vulnerability Scanner for Container Images and Filesystems. Accessed: Oct.~2025. [Online]. Available: \url{https://github.com/anchore/grype}

\bibitem{swe_bench_verified}
SWE-bench Verified (background on closed-loop code repair evaluation). Accessed: Oct.~2025. [Online]. Available: \url{https://openai.com/index/introducing-swe-bench-verified/}

\bibitem{llmsecconfig}
Z. Ye, T. H. M. Le, and M. A. Babar, "LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations," in \emph{2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)}, 2025.

\bibitem{malul2024}
E. Malul, Y. Meidan, D. Mimran, Y. Elovici, and A. Shabtai, "GenKubeSec: LLM-Based Kubernetes Misconfiguration Detection, Localization, Reasoning, and Remediation," \emph{arXiv preprint arXiv:2405.19954}, 2024.

\bibitem{kubellm}
M. De Jesus, P. Sylvester, W. Clifford, A. Perez, and P. Lama, "LLM-Based Multi-Agent Framework For Troubleshooting Distributed Systems," in \emph{Proc. of the 2025 IEEE Cloud Summit}, 2025 (author's version).

\bibitem{kyverno_docs}
Kyverno Project, "Kyverno Documentation," Accessed: Oct.~2025. [Online]. Available: \url{https://kyverno.io/docs/}

\bibitem{borg}
A. Verma \emph{et al.}, "Large-scale Cluster Management at Google with Borg," in \emph{Proc. EuroSys}, 2015; supplemental SRE updates accessed Oct.~2025. [Online]. Available: \url{https://research.google/pubs/pub43438/}

\end{thebibliography}

%====================
% Author biographies
%====================
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{brian_mendonca_photo.png}}]{Brian Mendonca}
is an M.S.\ student at the Georgia Institute of Technology (2024--2026) focusing on secure DevOps, policy-driven remediation, and human-centered tooling for developer productivity. 

Prior to graduate study, he worked as an Aerospace Quality Engineer at BAE Systems (2024--2025) and at Tube Specialties Inc.\ (2025--present), where he led Lean/Six Sigma continuous improvement, nonconformance management, and 8D root-cause investigations supporting AS9100 compliance and on-time delivery. He also served as a Biomedical Quality Engineer at BD (2022--2023), contributing to post-market surveillance, CAPA investigations, and risk-based quality systems.

He received the B.E.\ in Mechanical Engineering (summa cum laude, GPA 3.99) from Arizona State University in 2021. His research interests include secure configuration management for cloud-native systems, program analysis for infrastructure-as-code, and data-informed quality engineering.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{vijay_madisetti_photo.png}}]{Vijay K. Madisetti}
is Professor of Cybersecurity and Privacy at the Georgia Institute of Technology. He earned his Ph.D.\ in Electrical Engineering and Computer Sciences from the University of California at Berkeley.

Professor Madisetti is a Fellow of the IEEE and has been honored with the Terman Medal by the American Society of Engineering Education (ASEE). He has authored several widely referenced textbooks on topics including cloud computing, data analytics, blockchain, and microservices, and has extensive experience in secure system architectures and privacy-preserving technologies.
\end{IEEEbiography}

\EOD

\end{document}
