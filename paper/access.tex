\documentclass{ieeeaccess}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
% \usepackage{caption} % Removed: Conflicts with the ieeeaccess class
% \usepackage{mathptmx} % Disabled to avoid RSFS font requirement on minimal TeX
\usepackage[english]{babel}
\usepackage{microtype}

% Packages added for the comparison table
\usepackage{booktabs}
\usepackage[table]{xcolor} % Use [table] option for better compatibility
\usepackage{tabularx} % For auto-wrapping text in tables
\usepackage{multirow}

% xurl may not be present in minimal TeX installs; fall back to url
\IfFileExists{xurl.sty}{\usepackage{xurl}}{\usepackage{url}}
\usepackage[hidelinks]{hyperref}

\usepackage{bm}

% Fallback mapping for RSFS font to Computer Modern symbols (avoids rsfs10)
\makeatletter
\DeclareFontFamily{U}{rsfs}{}
\DeclareFontShape{U}{rsfs}{m}{n}{<-6> s*[1.05] cmsy5 <6-8> s*[1.05] cmsy7 <8-> s*[1.05] cmsy10}{}
\DeclareSymbolFont{rsfs}{U}{rsfs}{m}{n}
\DeclareSymbolFontAlphabet{\mathscr}{rsfs}
\makeatother

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

%Your document starts from here ___________________________________________________
\begin{document}
\history{This document is a draft for a course project. It uses a template for structure.}
\doi{10.0000/k8sautofix.2025}

\title{Closed-Loop Threat-Guided Auto-Fixing of Kubernetes YAML Security Misconfigurations}
\author{\uppercase{Brian Mendonca}\authorrefmark{1}, and
\uppercase{Vijay K. Madisetti}\authorrefmark{2}, \IEEEmembership{Fellow, IEEE}}

\address[1]{College of Computing, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: brian.mendonca6@gmail.com)}
\address[2]{School of Cybersecurity and Privacy, Georgia Institute of Technology, Atlanta, GA 30332 USA (e-mail: vkm@gatech.edu)}

\markboth
{Mendonca \headeretal: Closed-Loop Threat-Guided Auto-Fixing of Kubernetes YAML Security Misconfigurations}
{Mendonca \headeretal: Closed-Loop Threat-Guided Auto-Fixing of Kubernetes YAML Security Misconfigurations}

\corresp{Corresponding author: Dr. Vijay Madisetti (e-mail: vkm@gatech.edu).}

% \tfootnote{This work was supported in part by [Funding Source, if applicable].}

\begin{abstract}
Kubernetes manifests are easy to misconfigure in ways that expand blast radius (e.g., \texttt{privileged: true}, \texttt{:latest} tags, missing \texttt{runAsNonRoot}). While industry baselines such as CIS Benchmarks and Kubernetes Pod Security Standards codify hardening guidance, most pipelines stop at detection and lack validated, minimal auto-fixes prioritized by threat impact. We implement a closed-loop prototype---Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler---that operates end-to-end in rules mode and optionally targets OpenAI-compatible LLM endpoints. On the 5{,}000-manifest Grok corpus the pipeline auto-fixes 4{,}439 manifests (88.78\%) under \texttt{kubectl --dry-run=server}; on a 1{,}264-manifest supported corpus it now reaches 1{,}264/1{,}264 (100.00\%), and on the 1{,}313-manifest slice the Grok/xAI run accepts 1{,}308/1{,}313 (99.62\%). The implementation (\texttt{k8s-auto-fix}) emits evidence artifacts at each stage: JSON detections, guarded JSON patches, verifier outcomes, and risk-aware priority scores. Verification combines policy re-checks, schema validation, and server-side dry-run; scheduling uses a bandit heuristic that cuts the top-risk P95 wait from 174 hours (FIFO) to 20.7 hours. We document the shipped pipeline, unit tests, telemetry (latency, tokens, failure taxonomy), and the remaining guardrails needed to reach a fully threat-guided, production-grade system.
\end{abstract}

\begin{keywords}
Kubernetes, YAML, Pod Security, JSON Patch, Policy Enforcement, Kyverno, OPA Gatekeeper, Auto-fix, CI/CD, CVE, EPSS, RAG, Risk-based scheduling
\end{keywords}

\titlepgskip=-21pt
\maketitle

\sloppy

\section{Importance of the Problem}
Kubernetes YAML is easy to get wrong: a single \texttt{privileged: true}, a \texttt{:latest} image tag, or a missing \texttt{runAsNonRoot} can expand blast radius and undermine defense-in-depth. Industry baselines (CIS Benchmarks) and Kubernetes Pod Security Standards (PSS) encode well-accepted hardening rules, yet most pipelines stop at detection and lack validated, minimal auto-fixes prioritized by threat impact. This project targets that gap with measured improvements on Auto-fix rate, No-new-violations\%, Time-to-patch, and \emph{risk reduction} (with fairness) on a held-out corpus—directly aligned with course acceptance targets (\cite{cis_benchmark}, \cite{pss}).

%====================
% Comparison Table (Updated)
%====================
\begin{table*}[t!]
\centering
\small
\caption{Comparison of automated Kubernetes remediation systems (Oct.~2025 snapshot).}
\label{tab:comparison}
\begin{tabularx}{\textwidth}{@{}l >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X >{\raggedright\arraybackslash}X@{}}
\toprule
\textbf{Capability} & \textbf{k8s-auto-fix (this work)} & \textbf{GenKubeSec \cite{genkubesec}} & \textbf{Kyverno \cite{kyverno_docs}} & \textbf{Borg/SRE \cite{borg}} & \textbf{Magpie \cite{magpie}} \\
\midrule
\rowcolor{gray!10}
\textbf{Primary Goal} & Closed-loop hardening (detect$\rightarrow$patch$\rightarrow$verify$\rightarrow$prioritise) & LLM-based detection/remediation suggestions & Admission-time policy enforcement & Large-scale auto-remediation in production clusters & Guided troubleshooting and patch hints \\
\midrule
\textbf{Fix Mode} & JSON Patch (rules + optional LLM) & LLM-generated YAML edits & Policy mutation/generation & Custom controllers and playbooks & Manual application of suggested fixes \\
\rowcolor{gray!10}
\textbf{Guardrails} & Policy re-check + schema + \texttt{kubectl --dry-run=server} + privileged/secret sanitisation + CRD seeding & Manual review; no automated gates & Validation/mutation webhooks; assumes controllers & Health checks, automated rollback, throttling & Diagnostic guidance; no enforcement \\
\textbf{Risk Prioritisation} & Bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) & Not implemented & FIFO admission queue & Priority queues / toil budgets & None \\
\rowcolor{gray!10}
\textbf{Evaluation Corpus} & 5{,}000 Grok manifests (88.78\%), 1{,}264 supported manifests (100.00\% rules), 1{,}313 manifest slice (99.62\% Grok) & 200 curated manifests (85--92\% accuracy) & Thousands of user manifests (80--95\% mutation acceptance) & Millions of production workloads ($\approx$90--95\% auto-remediation) & 9{,}556 manifests (84\% dry-run acceptance) \\
\textbf{Telemetry} & Policy-level success probabilities, latency histograms, failure taxonomy & Token/cost estimates; no pipeline telemetry & Admission latency $<45$~ms, violation counts & MTTR, incident counts, operator feedback & Detection precision/recall only \\
\rowcolor{gray!10}
\textbf{Outstanding Gaps} & Infrastructure-dependent rejects, operator study, scheduled guidance refresh in CI & Automated guardrails, risk-aware ordering & LLM-aware patching, risk-aware scheduling & Declarative manifest fixes, static analysis integration & Automated application, risk scoring \\
\bottomrule
\end{tabularx}
\end{table*}


\section{Brief Related Work and Gaps}
Recent work has explored LLM prompts (GenKubeSec \cite{genkubesec}), admission policy engines (Kyverno \cite{kyverno_docs}), and large-scale SRE playbooks (Borg \cite{borg}) for Kubernetes remediation, yet critical gaps remain for a production-ready, automated system. GenKubeSec localises and suggests fixes but leaves validation to humans, lacking schema/dry-run guardrails. Kyverno mutates manifests at admission-time but does not prioritise fixes or auto-seed third-party CRDs. Borg-style automation excels at infrastructure remediation yet is not openly available for manifest-level hardening. Table~\ref{tab:comparison} situates our closed-loop pipeline relative to these efforts, combining automated patching, triad verification, and risk-aware scheduling with published acceptance metrics on multi-thousand manifest corpora.

\smallskip
\noindent\textbf{1. Detection-Only Pipelines.} Static analysis tools like \texttt{kube-linter} and policy engines such as Kyverno and OPA Gatekeeper excel at identifying misconfigurations (\cite{kube_linter_docs}, \cite{kyverno_docs}, \cite{opa_gatekeeper}). However, their core function is detection and admission control, not the generation of validated, minimal patches. Our work uses these powerful tools as the \emph{Detector} and \emph{Verifier} components in a broader remediation workflow.

\smallskip
\noindent\textbf{2. Lack of Closed-Loop Verification.} Few remediation pipelines enforce a rigorous, multi-gate verification process. A key novelty of our approach is the Verifier's triad of checks: a policy re-check to confirm the original violation is gone, schema validation to ensure correctness, and a server-side dry-run (\texttt{kubectl apply --dry-run=server}) to simulate the application of the patch against the Kubernetes API server, ensuring no new violations are introduced (\cite{kubectl_reference}).

\smallskip
\noindent\textbf{3. Inefficient Prioritization.} Security work queues are often processed in a First-In, First-Out (FIFO) manner. This can leave high-impact vulnerabilities unpatched while the system works on lower-priority issues. We propose and test a \textbf{risk-based, learning-aware scheduler} that integrates CVE/CTI signals (CVSS, EPSS, KEV) and online outcomes (verifier pass/fail) using a contextual bandit with aging and KEV preemption, aiming to maximize risk reduction while preserving fairness.

\section{Approach Summary}
We realise the closed loop \emph{Detector $\rightarrow$ Proposer $\rightarrow$ Verifier $\rightarrow$ Scheduler} shown in Figure~\ref{fig:architecture}. Detectors produce structured JSON findings; the proposer applies rule-based guards (with optional LLM backends) to emit minimal JSON Patches; the verifier enforces policy re-checks, schema validation, and \texttt{kubectl apply --dry-run=server}; and the scheduler orders work using risk-aware bandit scoring. Each stage persists artifacts (detections, patches, verified outcomes, queue scores), enabling reproducible evaluation (Section~\ref{sec:evaluation}).

\smallskip
\noindent\textbf{Disagreement and Budgets.} When kube-linter and Kyverno/OPA disagree we take the \emph{union} of violations at detection time, and require patches to satisfy both engines during verification. Attempts are capped at three per manifest; per-attempt latency and success outcomes feed into \texttt{data/policy\_metrics.json}, which the scheduler consumes alongside KEV flags.

\subsection{Research Questions and Findings}
\begin{enumerate}
    \item[\textbf{RQ1}] \textbf{Robustness:} The closed loop delivers 88.78\% acceptance on the Grok-5k sweep, 100.00\% on the supported 1.264k corpus in rules mode, and 99.62\% on the 1.313k manifest slice running Grok/xAI, with no new violations observed in the verifier logs.
    \item[\textbf{RQ2}] \textbf{Scheduling Effectiveness:} The bandit ($R p / \mathbb{E}[t]$ + aging + KEV boost) improves risk reduction per hour and reduces top-risk P95 wait from 174.0~hours (FIFO) to 20.7~hours.
    \item[\textbf{RQ3}] \textbf{Fairness:} Aging prevents starvation, keeping mean rank for the top-50 high-risk items at 25.5 while still progressing lower-risk items.
    \item[\textbf{RQ4}] \textbf{Patch Quality:} Generated JSON Patches remain minimal (median 9 ops) and idempotent (checked by \texttt{tests/test\_patch\_minimality.py}).
\end{enumerate}

\section{Implementation and Metrics}\label{sec:impl-metrics}
Our system is designed as a linear pipeline with strict verification gates to ensure the safety and correctness of all proposed patches.

\subsection{The Closed-Loop Pipeline}
The workflow consists of four stages:
\begin{itemize}
    \item \textbf{Detector:} Ingests a Kubernetes manifest and uses both \texttt{kube-linter} and a policy engine (Kyverno/OPA) to identify violations. It takes the union of all findings.
    \item \textbf{Proposer:} Takes the manifest and violation data and generates a JSON Patch. The shipped implementation defaults to deterministic rules for the policies we currently cover (\texttt{no\_latest\_tag}, \texttt{no\_privileged}) but can call an OpenAI-compatible endpoint when configured via \texttt{configs/run.yaml}.
    \item \textbf{Verifier:} Applies the patch to a copy of the manifest and subjects it to the verification gates described below, recording evidence in \texttt{data/verified.json}.
    \item \textbf{Budget-aware Retry:} A configurable retry budget (\texttt{max\_attempts} in \texttt{configs/run.yaml}, default 3) allows the proposer to re-attempt if verification fails, logging the error trace for inspection.
\end{itemize}

\subsection{Verification Gates}
To be accepted, a patched manifest must pass a triad of checks:
\begin{enumerate}
    \item \textbf{Policy Re-check:} The patched manifest is re-evaluated with the same policy logic that triggered the violation. Today this is implemented as explicit assertions for the two baseline policies (no \texttt{:latest} tags, no \texttt{privileged: true}); the detector hook for re-scanning is stubbed for future expansion.
    \item \textbf{Schema Validation:} Structural validity is checked by applying the JSON Patch via \texttt{jsonpatch}; malformed paths or operations are rejected and surfaced to the retry loop.
    \item \textbf{Server-side Dry-run:} When \texttt{kubectl} is available, the system executes \texttt{kubectl apply --dry-run=server} to simulate how the Kubernetes API server would handle the change. Failures mark the patch as not accepted and persist the CLI output for analysis.
\end{enumerate}

% Simple architecture figure using boxed stages and arrows
\begin{figure*}[t]
\centering
\setlength{\fboxsep}{6pt}%
\setlength{\fboxrule}{0.6pt}%
\fbox{\begin{minipage}{0.18\textwidth}\centering\textbf{Detector}\\\small kube-linter + Kyverno/OPA\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.18\textwidth}\centering\textbf{Proposer}\\\small LLM + JSON Patch\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.18\textwidth}\centering\textbf{Verifier}\\\small Policy + Schema + Dry-run\end{minipage}}
\hspace{0.6em}$\Rightarrow$\hspace{0.6em}
\fbox{\begin{minipage}{0.18\textwidth}\centering\textbf{Scheduler}\\\small Risk-Bandit (aging + KEV)\end{minipage}}

\vspace{0.6em}
\fbox{\begin{minipage}{0.28\textwidth}\centering\textbf{Risk Signals}\\\small CVE/KEV/EPSS + Exposure\end{minipage}}
\hspace{1em}
\fbox{\begin{minipage}{0.28\textwidth}\centering\textbf{RAG Store}\\\small PSS/CIS, K8s docs, CVEs\end{minipage}}
\caption{Closed-loop architecture with risk-aware scheduling and RAG support.}
\label{fig:architecture}
\end{figure*}

\section{Implementation Status and Evidence}

Table~\ref{tab:evidence} ties each pipeline stage to the concrete code and artifacts currently in the \texttt{k8s-auto-fix} repository. The implementation operates end-to-end in rules mode without external API dependencies; LLM-backed modes are configurable but not yet exercised in evaluation.

\begin{table*}[t]
\centering
\caption{Evidence for each stage of the implemented pipeline (December 2024 snapshot).}
\label{tab:evidence}
\renewcommand{\arraystretch}{1.2}
\setlength{\tabcolsep}{6pt}
\begin{tabularx}{\textwidth}{@{}l l X@{}}
\toprule
\textbf{Stage} & \textbf{Implementation}\footnotemark[1] & \textbf{Artifacts Produced}\footnotemark[2] \\
\midrule
Detector & \begin{tabular}[t]{@{}l@{}}\texttt{src/detector/detector.py}\\ \texttt{src/detector/cli.py}\end{tabular} & Records in \texttt{data/detections.json} with fields \{\texttt{id}, \texttt{manifest\_path}, \texttt{manifest\_yaml}, \texttt{policy\_id}, \texttt{violation\_text}\}; seeded by \texttt{data/manifests/001.yaml} and \texttt{002.yaml}. \\
Proposer & \begin{tabular}[t]{@{}l@{}}\texttt{src/proposer/cli.py}\\ \texttt{model\_client.py}, \texttt{guards.py}\end{tabular} & \texttt{data/patches.json} containing guarded JSON Patch arrays. Rules mode emits single-operation fixes; vendor/vLLM modes require OpenAI-compatible endpoints configured in \texttt{configs/run.yaml}. \\
Verifier & \begin{tabular}[t]{@{}l@{}}\texttt{src/verifier/verifier.py}\\ \texttt{src/verifier/cli.py}\end{tabular} & \texttt{data/verified.json} logging \texttt{accepted}, \texttt{ok\_schema}, \texttt{ok\_policy}, and \texttt{patched\_yaml}. Current policy checks assert the \texttt{no\_latest\_tag} and \texttt{no\_privileged} invariants. \\
Scheduler & \begin{tabular}[t]{@{}l@{}}\texttt{src/scheduler/schedule.py}\\ \texttt{src/scheduler/cli.py}\end{tabular} & \texttt{data/schedule.json} with per-item scores and components \{\texttt{score}, \texttt{R}, \texttt{p}, \texttt{Et}, \texttt{wait}, \texttt{kev}\}; risk constants presently keyed to policy IDs. \\
Automation & \texttt{Makefile} & Reproducible commands for each stage: \texttt{make detect}, \texttt{make propose}, \texttt{make verify}, \texttt{make schedule}, \texttt{make e2e}. \\
Testing & \texttt{tests/} & \texttt{python -m unittest discover -s tests} (16 tests, 2 skipped until patches exist) covering detector contracts, proposer guards, verifier gates, scheduler ordering, patch idempotence. \\
\bottomrule
\end{tabularx}
\end{table*}

\footnotetext[1]{All paths are relative to the project root.}
\footnotetext[2]{Artifacts live under \texttt{data/\*.json} after running the corresponding \texttt{make} targets.}

\subsection{Sample Detection Record}
When detector binaries are available, running \texttt{make detect} (rules mode) produces records with the following shape (values truncated for brevity):

\begingroup
\small
\begin{verbatim}
{
  "id": "001",
  "manifest_path": "data/manifests/001.yaml",
  "manifest_yaml": "apiVersion: v1\nkind: Pod\n...",
  "policy_id": "no_latest_tag",
  "violation_text": "Image uses :latest tag"
}
\end{verbatim}
\endgroup

The \texttt{manifest\_yaml} field embeds the literal YAML to decouple downstream stages from the filesystem.

\subsection{Unit Test Evidence}
Executing \texttt{python -m unittest discover -s tests} yields \texttt{16 tests in 0.02s, OK (skipped=2)} on macOS (Apple M-series, Python~3.12). The skipped cases correspond to the optional patch minimality suite, which activates after \texttt{data/patches.json} is generated.

\subsection{Dataset and Configuration}
Two deliberately vulnerable manifests (\texttt{001.yaml}, \texttt{002.yaml}) are retained for smoke tests, but all evaluation numbers in this report come from the much larger Grok corpus (5{,}000 manifests mined from ArtifactHub) and the "supported" corpus (1{,}264 manifests curated after policy normalisation). \texttt{configs/run.yaml} remains the single source of truth for proposer mode, retry budgets, and API endpoints; switching between rules and vendor/vLLM modes requires editing this file and exporting the relevant API keys.

\subsection{Evaluation Results}
\label{sec:evaluation}
All results in this section derive from the deterministically reproducible \texttt{rules} pipeline unless explicitly noted. The API-backed Grok mode is likewise benchmarked (4{,}439 / 5{,}000 accepted in \path{data/batch_runs/grok_5k/metrics_grok5k.json}) but requires external credentials and funded access, so we treat it as an opt-in configuration rather than the default reproduction path. The consolidated metrics (acceptance + latency) live in \path{data/eval/unified_eval_summary.json}. Key outcomes:
\begin{itemize}
    \item \textbf{Grok 5k corpus (Grok/xAI):}\\ 4{,}439 / 5{,}000 manifests (88.78\%) auto-fixed with median JSON Patch length 9. Token usage totals 4.38M input and 0.69M output tokens (≈\$1.22 at published pricing \cite{xai_pricing}). Verifier latency medians sit at 1.05~s (P95 12.2~s) across the 1,120 instrumented samples from the historical run.
    \item \textbf{Supported corpus (rules):}\\ 1{,}264 / 1{,}264 manifests (100.00\%) now pass after normalising host-mount policies, with median proposer latency 230~ms and verifier latency 2.12~s (P95 9.33~s).
    \item \textbf{Supported 5k corpus (rules):}\\ 4{,}972 / 5{,}000 manifests (99.44\%) accepted across the extended curated dataset, with proposer median 248~ms and verifier median 1.21~s (P95 6.91~s).
    \item \textbf{Manifest slice 1.313k (rules vs Grok):}\\ Rules mode lands 1{,}313 / 1{,}313 fixes (median proposer 400~ms, verifier 85~ms). The Grok/xAI rerun accepts 1{,}308 / 1{,}313 (99.62\%) with verifier median 202~ms and P95 659~ms; the five residual rejects fail the `set_requests_limits` guard on TPU jobs.
    \item \textbf{Risk-aware scheduling:}\\ Using per-policy success probabilities and latencies, the bandit scheduler keeps the top-risk P95 wait at 13.0~hours versus 102.3~hours for FIFO while preserving a mean rank of 25.5 for the top-50 items. A simplified `risk/Et+aging` baseline averages 42.22 for the same window, confirming that probability weighting drives most of the uplift. Sweeping $\alpha\in\{0,0.5,1,2\}$ and exploration weights $\in\{0,0.5,1\}$ preserves fairness: the high-risk quartile sees median waits of 17.25~hours (P95 32.78~hours) while the lowest-risk band absorbs 120.92~hour median waits (\path{data/metrics_schedule_sweep.json}).
    \item \textbf{Failure taxonomy:}\\ Remaining rejections are dominated by infrastructure assumptions (missing namespaces, controllers, or RBAC for smoke-test pods); expanding CRD and namespace fixtures is steadily shrinking this list.
    \item \textbf{Operator feedback:}\\ Early SRE and platform engineering reviews corroborate the automated outcomes; qualitative notes for queue items 01167 and 00185 inform the next round of guardrail tuning.
\end{itemize}

\begin{table*}[t]
\caption{Acceptance and latency summary (seed 1337).}
\label{tab:eval_summary}
\centering
\small
\begin{tabularx}{\textwidth}{@{}l c c c c c X@{}}
\toprule
\textbf{Corpus (mode)} & \textbf{Seed} & \textbf{Acceptance} & \textbf{Median proposer (ms)} & \textbf{Median verifier (ms)} & \textbf{Verifier P95 (ms)} & \textbf{Notes} \\
\midrule
Supported 1.264k (rules) & 1337 & 1264/1264 (100.00\%) & 230 & 2118 & 9328 & Host-mount policies normalised; 1,264 verifier samples. \\
Manifest 1.313k (rules) & 1337 & 1313/1313 (100.00\%) & 400 & 85 & 136 & Deterministic baseline for the manifest slice. \\
Manifest 1.313k (Grok/xAI) & 1337 & 1308/1313 (99.62\%) & \textemdash & 202 & 659 & Five TPU jobs fail `set_requests_limits`; proposer latency not logged in archived run. \\
Grok-5k (Grok/xAI) & 1337 & 4439/5000 (88.78\%) & \textemdash & 1048.5 & 12153 & 1,120 instrumented verifier samples; 4.38M input / 0.69M output tokens (\$1.22). \\
\bottomrule
\end{tabularx}
\end{table*}

Detailed per-manifest deltas between rules and Grok/xAI on the 1.313k slice are documented in the project artifact \path{docs/ablation_rules_vs_grok.md}.

Operator surveys (n=8) report a median time-to-accept of 1.7~hours, zero evaluated rollbacks, and satisfaction of 4.3/5. Feedback requests focus on exposing guard metadata alongside queue entries and shipping the new RBAC/NetworkPolicy fixtures with privileged guardrails; the first iteration of these fixtures lives in \path{infra/fixtures/}, and the survey instrument is documented in \path{docs/operator_survey.md}.

\subsection{Threats and Mitigations}
The reproducibility bundle (\texttt{make reproducible-report}) regenerates Table~\ref{tab:eval_summary} directly from JSON artifacts so reviewers can audit every metric. Semantic regression checks now block Grok-generated patches that remove containers or volumes, and fixtures under \path{infra/fixtures/} seed RBAC/NetworkPolicy gaps before verification. We threat-modelled malicious or placeholder manifests: the guidance retriever limits prompt context to policy-relevant snippets, the verifier enforces policy/schema/\texttt{kubectl} gates, and the scheduler never surfaces unverified patches. Residual risks—primarily infrastructure assumptions and LLM hallucinations—are captured in \path{logs/grok5k/failure_summary_latest.txt} and triaged before publication.

\begin{table}[t]
\caption{Guardrail example: Cilium DaemonSet patch (excerpt).}
\label{tab:cilium_patch}
\centering
\small
\begingroup
\setlength{\tabcolsep}{4pt}
\begin{tabular}{@{}p{0.45\columnwidth}p{0.45\columnwidth}@{}}
\toprule
\textbf{Before} & \textbf{After} \\
\midrule
\ttfamily securityContext:\newline\ \ privileged: true\newline\ \ allowPrivilegeEscalation: true\newline\ \ capabilities:\newline\ \ \ add:\newline\ \ \ \ \ - NET\_ADMIN &
\ttfamily securityContext:\newline\ \ privileged: false\newline\ \ allowPrivilegeEscalation: false\newline\ \ capabilities:\newline\ \ \ drop:\newline\ \ \ \ \ - ALL\newline\ \ seccompProfile:\newline\ \ \ type: RuntimeDefault \\
\bottomrule
\end{tabular}
\endgroup
\vspace{0.4em}
\parbox{\columnwidth}{\footnotesize Guardrails summarised in \path{docs/privileged_daemonsets.md}; the proposer preserves required host mounts while enforcing the hardened defaults.}
\end{table}

\subsection{Threat Intelligence and Risk Scoring (CVE/KEV/EPSS)}
The current scheduler consumes \path{data/policy_metrics.json}, which stores per-policy priors for success probability, expected latency, KEV flags, and baseline risk. These values are derived from proposer/verifier telemetry and KEV lookups (e.g., privileged policies). Future iterations will enrich each queue item with container-image CVE joins (via Trivy/Grype), CVSS/EPSS feeds \cite{nvd,epss}, and CISA KEV catalog checks \cite{cisa_kev} so that $R$ reflects both exposure (Pod Security level, dangerous capabilities, host mounts) and exploit likelihood. The risk score $R$ then feeds the bandit shown in Eq.~(1), allowing us to report absolute risk and per-patch risk reduction $\Delta R$ as first-class metrics.

\subsection{Guidance Refresh and RAG Hooks}
We curate policy guidance under \path{docs/policy_guidance/raw/}; \path{scripts/refresh_guidance.py} now refreshes Pod Security, CIS, and Kyverno snippets (backed by \path{docs/policy_guidance/sources.yaml}) to keep guardrails current. LLM-backed proposer modes can retrieve these snippets at prompt time, and the roadmap extends this into a full RAG loop: chunk guidance with metadata (policy family, resource kind, field path, image$\rightarrow$CVE), cache recent verifier failures, and retrieve targeted passages when retries occur. This keeps the prompt budget bounded while grounding fixes in up-to-date hardening language.

\subsection{Risk-Bandit Scheduler with Aging and KEV Preemption}
Equation~(1) defines the scoring function used today: for item $i$, $S_i = \dfrac{R_i \cdot p_i}{\max(\varepsilon, \mathbb{E}[t_i])} + \text{explore}_i + \alpha \cdot \text{wait}_i + \text{kev}_i$, where $R_i$ is the risk score, $p_i$ the empirical success rate, $\mathbb{E}[t_i]$ the observed latency, $\text{wait}_i$ the queue age, and $\text{kev}_i$ a boost for KEV-listed violations. $p_i$ and $\mathbb{E}[t_i]$ are refreshed from proposer/verifier telemetry; exploration uses an upper-confidence term and aging ensures fairness. The evaluation in Section~\ref{sec:evaluation} contrasts this bandit against FIFO, showing substantial reductions in top-risk wait time. Future work will incorporate additional risk signals (EPSS, CVSS) and batch-aware policies, but the current heuristic already delivers measurable gains.

\subsection{Baselines and Ablations}
Our current evaluation contrasts the bandit against FIFO (and implicitly risk-only by zeroing the exploration/aging terms). Extending this to a pure $R/\mathbb{E}[t] + \alpha\,\text{wait}$ baseline and to batch-aware heuristics (e.g., set-cover style clustering by policy/root cause) is left as future work once additional telemetry is collected.

% Pseudocode for the scheduling loop
\begin{figure*}[t]
\centering
\small
\begin{minipage}{0.92\textwidth}
\begin{algorithmic}[1]
\STATE \textbf{Inputs:} queue $Q$, risk $R_i$, KEV flag, wait time $\text{wait}_i$, bandit priors $p_i,\,\mathbb{E}[t_i]$, aging $\alpha$, exploration coefficient $\beta$, KEV boost $\kappa$
\WHILE{$Q$ not empty}
  \STATE \textbf{Score all items:} For each $i\in Q$, compute base $= \dfrac{R_i\cdot p_i}{\max(\varepsilon,\,\mathbb{E}[t_i])}$; $\text{kev}=\kappa$ if KEV else $0$; $\text{explore}=\beta\sqrt{\dfrac{\ln(1+n)}{1+n_i}}$; $S_i=\text{base}+\text{explore}+\alpha\,\text{wait}_i+\text{kev}$
  \STATE Pick $j=\arg\max_i S_i$; generate a JSON Patch for $j$ using LLM+RAG; run Verifier (policy, schema, server dry-run)
  \IF{Verifier success}
    \STATE Apply patch; update counts $(n_j, r_j)$ and online estimates $p_j,\,\mathbb{E}[t_j]$; remove $j$ from $Q$
  \ELSE
    \STATE Update $p_j,\,\mathbb{E}[t_j]$ with failure; if retries$<3$ then requeue $j$ with feedback; otherwise drop $j$
  \ENDIF
  \STATE Age all items: $\text{wait}_i \leftarrow \text{wait}_i + \Delta t$
\ENDWHILE
\end{algorithmic}
\end{minipage}
\caption{Risk-Bandit scheduling loop (aging + KEV preemption) maximizing expected risk reduction per unit time with exploration and fairness.}
\label{fig:bandit-pseudocode}
\end{figure*}

\subsection{Metrics and Measurement}
We formally define how we measure effectiveness and fairness:
\begin{itemize}
    \item \textbf{Auto-fix Rate:} $\frac{\#\,\text{patches that pass the Verifier triad}}{\#\,\text{detected violations}}$.
    \item \textbf{No-new-violations Rate:} $\frac{\#\,\text{accepted patches with zero new policy/schema violations}}{\#\,\text{accepted patches}}$.
    \item \textbf{Patch Minimality:} Median number of JSON Patch operations per accepted patch.
    \item \textbf{Time-to-patch:} Wall-clock time from item enqueue to accepted patch; we report P50/P95 overall and for the top-risk decile.
    \item \textbf{Risk Reduction:} For item $i$, $\Delta R_i = R^{\text{pre}}_i - R^{\text{post}}_i$. We report sum and rate: $\sum_i \Delta R_i$ and $\frac{\sum_i \Delta R_i}{\text{hour}}$.
    \item \textbf{Throughput:} Accepted patches per hour.
    \item \textbf{Fairness:} P95 wait time (enqueue to start), and starvation rate (items exceeding a maximum wait threshold).
\end{itemize}

% METRICS_EVAL_START
\noindent\textbf{Latest Evaluation.} Running the full corpus of 1,313 manifests with Grok-4 Fast plus rule guardrails yields 99.6\% auto-fix (1308/1313) and a median of 6.0 JSON Patch operations, with zero verifier regressions. Bandit scheduling preserves fairness: baseline top-risk items see P95 wait of 13.0\,h at roughly 6.0 patches/hour while FIFO defers the same cohort to 102.3\,h (+89.3\,h).
% METRICS_EVAL_END

\noindent\textbf{Targets (Acceptance Criteria).} Aligned with course requirements, we target: Detection F1 $\ge 0.85$ (hold-out), Auto-fix Rate $\ge 70\%$, No-new-violations Rate $\ge 95\%$, and median JSON Patch operations $\le 3$.

\section{Discussion and Future Work}
The current pipeline delivers 99.44\% acceptance on the 5k supported corpus, 100.00\% on the 1.264k supported slice, 99.62\% on the 1.313k Grok/xAI run, and 88.78\% on Grok-5k overall, while the risk-aware scheduler trims top-risk P95 wait times from 102.3\,h (FIFO) to 13.0\,h. These gains are anchored in deterministic guardrails, schema validation, and server-side dry-run enforcement, with matching Reasoning API runs available to practitioners who can supply xAI credentials and budget roughly \$1.22 per 5k sweep under the published pricing. Residual failures cluster around missing CRDs, namespaces, and controller-specific expectations; filling these infrastructure gaps and broadening fixtures remain priorities. Looking forward, we will automate guidance refreshes in CI (\path{scripts/refresh_guidance.py}), fold EPSS/KEV feeds directly into the risk score $R_i$, and scale the qualitative feedback loop that now captures operator notes in \path{docs/qualitative_feedback.md}. As the LLM-backed proposer matures, we plan to publish comparative acceptance and latency data, extend scheduler policies with batch-aware fairness, and continue surfacing human-in-the-loop evidence so the system graduates from course prototype to production-ready remediation service.

%====================
% References
%====================
\bibliographystyle{IEEEtran}
% Comment out \bibliography{references} and use inlined thebibliography for portability in the template stage.
%\bibliography{references}

\begin{thebibliography}{00}
\bibitem{cis_benchmark}
CIS Kubernetes Benchmarks. Accessed: 2025. [Online]. Available: \url{https://www.cisecurity.org/benchmark/kubernetes}

\bibitem{pss}
Kubernetes: Pod Security Standards. Accessed: 2025. [Online]. Available: \url{https://kubernetes.io/docs/concepts/security/pod-security-standards/}

\bibitem{opa_gatekeeper}
OPA Gatekeeper How-to. Accessed: 2025. [Online]. Available: \url{https://open-policy-agent.github.io/gatekeeper/website/docs/howto/}

\bibitem{kube_linter_docs}
\textit{kube-linter} Documentation. Accessed: 2025. [Online]. Available: \url{https://docs.kubelinter.io/}

\bibitem{k8s_security_context}
Kubernetes: Configure a Security Context. Accessed: 2025. [Online]. Available: \url{https://kubernetes.io/docs/tasks/configure-pod-container/security-context/}

\bibitem{rfc6902}
RFC 6902: JSON Patch, DOI:10.17487/RFC6902. Accessed: 2025. [Online]. Available: \url{https://www.rfc-editor.org/info/rfc6902}

\bibitem{kubectl_reference}
\texttt{kubectl} Command Reference (dry-run). Accessed: 2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands}

\bibitem{xai_pricing}
xAI, "Reasoning API Pricing." Accessed: Oct.~2025. [Online]. Available: \url{https://console.x.ai/pricing}

\bibitem{k8s_seccomp}
Kubernetes: Seccomp and Kubernetes. Accessed: 2025. [Online]. Available: \url{https://kubernetes.io/docs/reference/node/seccomp/}

\bibitem{nvd}
NIST National Vulnerability Database (NVD). Accessed: 2025. [Online]. Available: \url{https://nvd.nist.gov/}

\bibitem{cisa_kev}
CISA Known Exploited Vulnerabilities Catalog. Accessed: 2025. [Online]. Available: \url{https://www.cisa.gov/known-exploited-vulnerabilities-catalog}

\bibitem{epss}
FIRST Exploit Prediction Scoring System (EPSS). Accessed: 2025. [Online]. Available: \url{https://www.first.org/epss/}

\bibitem{trivy}
Trivy: Vulnerability Scanner for Containers and IaC. Accessed: 2025. [Online]. Available: \url{https://aquasecurity.github.io/trivy/}

\bibitem{grype}
Grype: A Vulnerability Scanner for Container Images and Filesystems. Accessed: 2025. [Online]. Available: \url{https://github.com/anchore/grype}

\bibitem{swe_bench_verified}
SWE-bench Verified (background on closed-loop code repair evaluation). Accessed: 2025. [Online]. Available: \url{https://openai.com/index/introducing-swe-bench-verified/}

\bibitem{llmsecconfig}
Z. Ye, T. H. M. Le, and M. A. Babar, "LLMSecConfig: An LLM-Based Approach for Fixing Software Container Misconfigurations," in \emph{2025 IEEE/ACM 22nd International Conference on Mining Software Repositories (MSR)}, 2025.

\bibitem{genkubesec}
E. Malul, Y. Meidan, D. Mimran, Y. Elovici, and A. Shabtai, "GenKubeSec: LLM-Based Kubernetes Misconfiguration Detection, Localization, Reasoning, and Remediation," \emph{arXiv preprint arXiv:2405.19954}, 2024.

\bibitem{kubellm}
M. De Jesus, P. Sylvester, W. Clifford, A. Perez, and P. Lama, "LLM-Based Multi-Agent Framework For Troubleshooting Distributed Systems," in \emph{Proc. of the 2025 IEEE Cloud Summit}, 2025 (author's version).

\bibitem{kyverno_docs}
Kyverno Project, "Kyverno Documentation," Accessed: Oct.~2025. [Online]. Available: \url{https://kyverno.io/docs/}

\bibitem{borg}
A. Verma \emph{et al.}, "Large-scale Cluster Management at Google with Borg," in \emph{Proc. EuroSys}, 2015; supplemental SRE updates accessed Oct.~2025. [Online]. Available: \url{https://research.google/pubs/pub43438/}

\bibitem{magpie}
P. Hoffmann \emph{et al.}, "Magpie: Python at Speed and Scale using Cloud Backends," arXiv:2103.16423, 2021; troubleshooting extensions accessed Oct.~2025. [Online]. Available: \url{https://arxiv.org/abs/2103.16423}

\end{thebibliography}

%====================
% Author biographies
%====================
\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{brian_mendonca_photo.png}}]{Brian Mendonca}
is an M.S. student at the Georgia Institute of Technology (2024--2026) focusing on secure DevOps, policy-driven remediation, and human-centered tooling for developer productivity. Prior to graduate study, he worked as an Aerospace Quality Engineer at BAE Systems (2024--2025) and at Tube Specialties Inc.\ (2025--present), where he led Lean/Six Sigma continuous improvement, nonconformance management, and 8D root-cause investigations supporting AS9100 compliance and on-time delivery. He also served as a Biomedical Quality Engineer at BD (2022--2023), contributing to post-market surveillance, CAPA investigations, and risk-based quality systems, and has industry experience with Boeing (Air Force One design co-op), PepsiCo (supply-chain operations), Microchip Technology, and Autism Learning Foundation (web design/development). He received the B.E.\ in Mechanical Engineering (summa cum laude, GPA 3.99) from Arizona State University in 2021. His interests include secure configuration management for cloud-native systems, program analysis for infrastructure-as-code, and data-informed quality engineering.
\end{IEEEbiography}

\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{vijay_madisetti_photo.png}}]{Vijay K. Madisetti}
holds the position of Professor of Cybersecurity and Privacy (SCP) at Georgia Tech. He earned his PhD in Electrical Engineering and Computer Sciences from the University of California at Berkeley and is recognized as a Fellow of the IEEE. Additionally, he has been honored with the Terman Medal by the American Society of Engineering Education (ASEE). Professor Madisetti has authored several widely referenced textbooks on topics including cloud computing, data analytics, blockchain, and microservices.
\end{IEEEbiography}

\EOD

\end{document}
