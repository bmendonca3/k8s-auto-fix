[
  {
    "id": "001",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "002",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "003",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "004",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "005",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n  - name: nginx\n    image: nginx:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "006",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: true\n    command:\n    - sleep\n    - '3600'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ubuntu\" does not have a read-only root file system"
  },
  {
    "id": "007",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: true\n    command:\n    - sleep\n    - '3600'\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ubuntu\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "008",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: true\n    command:\n    - sleep\n    - '3600'\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ubuntu\" is privileged"
  },
  {
    "id": "009",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: true\n    command:\n    - sleep\n    - '3600'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ubuntu\" is not set to runAsNonRoot"
  },
  {
    "id": "010",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: true\n    command:\n    - sleep\n    - '3600'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ubuntu\" has cpu request 0"
  },
  {
    "id": "011",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n  - name: ubuntu\n    image: ubuntu:22.04\n    securityContext:\n      privileged: true\n    command:\n    - sleep\n    - '3600'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ubuntu\" has memory limit 0"
  },
  {
    "id": "012",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"web\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "013",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"web\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "014",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "015",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "016",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"web\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "017",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "018",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "019",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n  - name: web\n    image: nginx:latest\n    ports:\n    - containerPort: 8080\n      hostPort: 30080\n    securityContext:\n      capabilities:\n        add:\n        - NET_RAW\n        - SYS_ADMIN\n  volumes:\n  - name: host-data\n    hostPath:\n      path: /var/lib/data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "020",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/020_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "021",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/021_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "022",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/022_service_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  type: ClusterIP\n  selector:\n    tier: airflow\n    component: api-server\n    release: release-name\n  ports:\n  - name: api-server\n    port: 8080\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:api-server release:release-name tier:airflow])"
  },
  {
    "id": "023",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/023_service_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  type: ClusterIP\n  selector:\n    tier: airflow\n    component: redis\n    release: release-name\n  ports:\n  - name: redis-db\n    protocol: TCP\n    port: 6379\n    targetPort: 6379\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:redis release:release-name tier:airflow])"
  },
  {
    "id": "024",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/024_service_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9102'\nspec:\n  type: ClusterIP\n  selector:\n    tier: airflow\n    component: statsd\n    release: release-name\n  ports:\n  - name: statsd-ingest\n    protocol: UDP\n    port: 9125\n    targetPort: 9125\n  - name: statsd-scrape\n    protocol: TCP\n    port: 9102\n    targetPort: 9102\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:statsd release:release-name tier:airflow])"
  },
  {
    "id": "025",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/025_service_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  clusterIP: None\n  selector:\n    tier: airflow\n    component: triggerer\n    release: release-name\n  ports:\n  - name: triggerer-logs\n    protocol: TCP\n    port: 8794\n    targetPort: 8794\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:triggerer release:release-name tier:airflow])"
  },
  {
    "id": "026",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/026_service_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  clusterIP: None\n  selector:\n    tier: airflow\n    component: worker\n    release: release-name\n  ports:\n  - name: worker-logs\n    protocol: TCP\n    port: 8793\n    targetPort: 8793\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[component:worker release:release-name tier:airflow])"
  },
  {
    "id": "027",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api-server\" does not have a read-only root file system"
  },
  {
    "id": "028",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "029",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-api-server\" not found"
  },
  {
    "id": "030",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api-server\" has cpu request 0"
  },
  {
    "id": "031",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "032",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api-server\" has memory limit 0"
  },
  {
    "id": "033",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "034",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dag-processor\" does not have a read-only root file system"
  },
  {
    "id": "035",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dag-processor-log-groomer\" does not have a read-only root file system"
  },
  {
    "id": "036",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "037",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-dag-processor\" not found"
  },
  {
    "id": "038",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dag-processor\" has cpu request 0"
  },
  {
    "id": "039",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dag-processor-log-groomer\" has cpu request 0"
  },
  {
    "id": "040",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "041",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dag-processor\" has memory limit 0"
  },
  {
    "id": "042",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dag-processor-log-groomer\" has memory limit 0"
  },
  {
    "id": "043",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "044",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scheduler\" does not have a read-only root file system"
  },
  {
    "id": "045",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scheduler-log-groomer\" does not have a read-only root file system"
  },
  {
    "id": "046",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "047",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-scheduler\" not found"
  },
  {
    "id": "048",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scheduler\" has cpu request 0"
  },
  {
    "id": "049",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scheduler-log-groomer\" has cpu request 0"
  },
  {
    "id": "050",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "051",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scheduler\" has memory limit 0"
  },
  {
    "id": "052",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scheduler-log-groomer\" has memory limit 0"
  },
  {
    "id": "053",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "054",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/030_deployment_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statsd\" does not have a read-only root file system"
  },
  {
    "id": "055",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/030_deployment_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-statsd\" not found"
  },
  {
    "id": "056",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/030_deployment_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statsd\" has cpu request 0"
  },
  {
    "id": "057",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/030_deployment_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statsd\" has memory limit 0"
  },
  {
    "id": "058",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/031_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 16.1.0\n        helm.sh/chart: postgresql-13.2.24\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:16.1.0-debian-11-r15\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n          runAsUser: 1001\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits: {}\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgresql\" does not have a read-only root file system"
  },
  {
    "id": "059",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/031_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 16.1.0\n        helm.sh/chart: postgresql-13.2.24\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:16.1.0-debian-11-r15\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n          runAsUser: 1001\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits: {}\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgresql\" has memory limit 0"
  },
  {
    "id": "060",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "061",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-redis\" not found"
  },
  {
    "id": "062",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "063",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "064",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "065",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"triggerer\" does not have a read-only root file system"
  },
  {
    "id": "066",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"triggerer-log-groomer\" does not have a read-only root file system"
  },
  {
    "id": "067",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "068",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-triggerer\" not found"
  },
  {
    "id": "069",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"triggerer\" has cpu request 0"
  },
  {
    "id": "070",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"triggerer-log-groomer\" has cpu request 0"
  },
  {
    "id": "071",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "072",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"triggerer\" has memory limit 0"
  },
  {
    "id": "073",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"triggerer-log-groomer\" has memory limit 0"
  },
  {
    "id": "074",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "075",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "076",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"worker\" does not have a read-only root file system"
  },
  {
    "id": "077",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"worker-log-groomer\" does not have a read-only root file system"
  },
  {
    "id": "078",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-worker\" not found"
  },
  {
    "id": "079",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "080",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"worker\" has cpu request 0"
  },
  {
    "id": "081",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"worker-log-groomer\" has cpu request 0"
  },
  {
    "id": "082",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "083",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"worker\" has memory limit 0"
  },
  {
    "id": "084",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"worker-log-groomer\" has memory limit 0"
  },
  {
    "id": "085",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/038_job_release-name-create-user.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"create-user\" does not have a read-only root file system"
  },
  {
    "id": "086",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/038_job_release-name-create-user.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-create-user-job\" not found"
  },
  {
    "id": "087",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/038_job_release-name-create-user.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"create-user\" has cpu request 0"
  },
  {
    "id": "088",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/038_job_release-name-create-user.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"create-user\" has memory limit 0"
  },
  {
    "id": "089",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/039_job_release-name-run-airflow-migrations.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"run-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "090",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/039_job_release-name-run-airflow-migrations.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-airflow-migrate-database-job\" not found"
  },
  {
    "id": "091",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/039_job_release-name-run-airflow-migrations.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"run-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "092",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/039_job_release-name-run-airflow-migrations.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"run-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "093",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/038_service_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  type: ClusterIP\n  ports:\n  - name: http-webhook\n    port: 7000\n    targetPort: webhook\n  selector:\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-applicationset-controller])"
  },
  {
    "id": "094",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/039_service_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\n  name: release-name-argocd-repo-server\n  namespace: default\nspec:\n  ports:\n  - name: tcp-repo-server\n    protocol: TCP\n    port: 8081\n    targetPort: repo-server\n  selector:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-repo-server])"
  },
  {
    "id": "095",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/040_service_release-name-argocd-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8080\n  - name: https\n    protocol: TCP\n    port: 443\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-server])"
  },
  {
    "id": "096",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/041_service_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ports:\n  - name: http\n    protocol: TCP\n    port: 5556\n    targetPort: http\n  - name: grpc\n    protocol: TCP\n    port: 5557\n    targetPort: grpc\n  selector:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-dex-server])"
  },
  {
    "id": "097",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/042_service_release-name-argocd-redis.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: redis\n  selector:\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-redis])"
  },
  {
    "id": "098",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argocd-applicationset-controller\" not found"
  },
  {
    "id": "099",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"applicationset-controller\" has cpu request 0"
  },
  {
    "id": "100",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"applicationset-controller\" has memory limit 0"
  },
  {
    "id": "101",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/044_deployment_release-name-argocd-notifications-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-notifications-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argocd-notifications-controller\" not found"
  },
  {
    "id": "102",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/044_deployment_release-name-argocd-notifications-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-notifications-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"notifications-controller\" has cpu request 0"
  },
  {
    "id": "103",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/044_deployment_release-name-argocd-notifications-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-notifications-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"notifications-controller\" has memory limit 0"
  },
  {
    "id": "104",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-argocd-repo-server\" not found"
  },
  {
    "id": "105",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"copyutil\" has cpu request 0"
  },
  {
    "id": "106",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"repo-server\" has cpu request 0"
  },
  {
    "id": "107",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"copyutil\" has memory limit 0"
  },
  {
    "id": "108",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"repo-server\" has memory limit 0"
  },
  {
    "id": "109",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/046_deployment_release-name-argocd-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-server\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argocd-server\" not found"
  },
  {
    "id": "110",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/046_deployment_release-name-argocd-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-server\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "111",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/046_deployment_release-name-argocd-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-server\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "112",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argocd-dex-server\" not found"
  },
  {
    "id": "113",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"copyutil\" has cpu request 0"
  },
  {
    "id": "114",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dex-server\" has cpu request 0"
  },
  {
    "id": "115",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"copyutil\" has memory limit 0"
  },
  {
    "id": "116",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dex-server\" has memory limit 0"
  },
  {
    "id": "117",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/048_deployment_release-name-argocd-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-redis\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: redis\n        image: ecr-public.aws.com/docker/library/redis:7.2.8-alpine\n        imagePullPolicy: IfNotPresent\n        args:\n        - --save\n        - ''\n        - --appendonly\n        - 'no'\n        - --requirepass $(REDIS_PASSWORD)\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n        ports:\n        - name: redis\n          containerPort: 6379\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /health\n          name: health\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: health\n        configMap:\n          name: release-name-argocd-redis-health-configmap\n          defaultMode: 493\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "118",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/048_deployment_release-name-argocd-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-redis\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: redis\n        image: ecr-public.aws.com/docker/library/redis:7.2.8-alpine\n        imagePullPolicy: IfNotPresent\n        args:\n        - --save\n        - ''\n        - --appendonly\n        - 'no'\n        - --requirepass $(REDIS_PASSWORD)\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n        ports:\n        - name: redis\n          containerPort: 6379\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /health\n          name: health\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: health\n        configMap:\n          name: release-name-argocd-redis-health-configmap\n          defaultMode: 493\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "119",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/049_statefulset_release-name-argocd-application-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-application-controller\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argocd-application-controller\" not found"
  },
  {
    "id": "120",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/049_statefulset_release-name-argocd-application-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-application-controller\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"application-controller\" has cpu request 0"
  },
  {
    "id": "121",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/049_statefulset_release-name-argocd-application-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-application-controller\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"application-controller\" has memory limit 0"
  },
  {
    "id": "122",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/053_job_release-name-argocd-redis-secret-init.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-argocd-redis-secret-init\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-argocd-redis-secret-init\" not found"
  },
  {
    "id": "123",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/053_job_release-name-argocd-redis-secret-init.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-argocd-redis-secret-init\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"secret-init\" has cpu request 0"
  },
  {
    "id": "124",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/053_job_release-name-argocd-redis-secret-init.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-argocd-redis-secret-init\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"secret-init\" has memory limit 0"
  },
  {
    "id": "125",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/025_service_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  ports:\n  - port: 2746\n    targetPort: 2746\n  selector:\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argo-workflows-server])"
  },
  {
    "id": "126",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/026_deployment_release-name-argo-workflows-workflow-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: release-name-argo-workflows-workflow-controller\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources: {}\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-argo-workflows-workflow-controller\" not found"
  },
  {
    "id": "127",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/026_deployment_release-name-argo-workflows-workflow-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: release-name-argo-workflows-workflow-controller\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources: {}\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"controller\" has cpu request 0"
  },
  {
    "id": "128",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/026_deployment_release-name-argo-workflows-workflow-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: release-name-argo-workflows-workflow-controller\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources: {}\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "129",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/027_deployment_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"argo-server\" does not have a read-only root file system"
  },
  {
    "id": "130",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/027_deployment_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-argo-workflows-server\" not found"
  },
  {
    "id": "131",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/027_deployment_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"argo-server\" has cpu request 0"
  },
  {
    "id": "132",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/027_deployment_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"argo-server\" has memory limit 0"
  },
  {
    "id": "133",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/009_service_release-name-postgresql-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-headless\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app: postgresql\n    release: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:postgresql release:release-name])"
  },
  {
    "id": "134",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/010_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app: postgresql\n    release: release-name\n    role: master\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:postgresql release:release-name role:master])"
  },
  {
    "id": "135",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/011_service_hub.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/component: hub\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:hub app.kubernetes.io/instance:release-name app.kubernetes.io/name:artifact-hub])"
  },
  {
    "id": "136",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/012_service_trivy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8081\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/component: trivy\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:trivy app.kubernetes.io/instance:release-name app.kubernetes.io/name:artifact-hub])"
  },
  {
    "id": "137",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "138",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-migrator-run\" does not have a read-only root file system"
  },
  {
    "id": "139",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "140",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hub\" does not have a read-only root file system"
  },
  {
    "id": "141",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hub\" not found"
  },
  {
    "id": "142",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-migrator-run\" is not set to runAsNonRoot"
  },
  {
    "id": "143",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "144",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hub\" is not set to runAsNonRoot"
  },
  {
    "id": "145",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-migrator-run\" has cpu request 0"
  },
  {
    "id": "146",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "147",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hub\" has cpu request 0"
  },
  {
    "id": "148",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-migrator-run\" has memory limit 0"
  },
  {
    "id": "149",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "150",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hub\" has memory limit 0"
  },
  {
    "id": "151",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/014_deployment_trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"trivy\" does not have a read-only root file system"
  },
  {
    "id": "152",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/014_deployment_trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"trivy\" is not set to runAsNonRoot"
  },
  {
    "id": "153",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/014_deployment_trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"trivy\" has cpu request 0"
  },
  {
    "id": "154",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/014_deployment_trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"trivy\" has memory limit 0"
  },
  {
    "id": "155",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"release-name-postgresql\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "156",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-chmod-data\" does not have a read-only root file system"
  },
  {
    "id": "157",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-postgresql\" does not have a read-only root file system"
  },
  {
    "id": "158",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-chmod-data\" is not set to runAsNonRoot"
  },
  {
    "id": "159",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-chmod-data\" has memory limit 0"
  },
  {
    "id": "160",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-postgresql\" has memory limit 0"
  },
  {
    "id": "161",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "162",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "163",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "164",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"db-migrator\" does not have a read-only root file system"
  },
  {
    "id": "165",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "166",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"db-migrator\" is not set to runAsNonRoot"
  },
  {
    "id": "167",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "168",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"db-migrator\" has cpu request 0"
  },
  {
    "id": "169",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "170",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"db-migrator\" has memory limit 0"
  },
  {
    "id": "171",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "172",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "173",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scanner\" does not have a read-only root file system"
  },
  {
    "id": "174",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "175",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"scanner\" is not set to runAsNonRoot"
  },
  {
    "id": "176",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "177",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scanner\" has cpu request 0"
  },
  {
    "id": "178",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "179",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scanner\" has memory limit 0"
  },
  {
    "id": "180",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "181",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "182",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tracker\" does not have a read-only root file system"
  },
  {
    "id": "183",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "184",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tracker\" is not set to runAsNonRoot"
  },
  {
    "id": "185",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "186",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tracker\" has cpu request 0"
  },
  {
    "id": "187",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "188",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tracker\" has memory limit 0"
  },
  {
    "id": "189",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/008_service_release-name-sealed-secrets.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 8080\n    targetPort: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/name: sealed-secrets\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:sealed-secrets])"
  },
  {
    "id": "190",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/009_service_release-name-sealed-secrets-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-sealed-secrets-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n    app.kubernetes.io/component: metrics\nspec:\n  type: ClusterIP\n  ports:\n  - name: metrics\n    port: 8081\n    targetPort: metrics\n    nodePort: null\n  selector:\n    app.kubernetes.io/name: sealed-secrets\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:sealed-secrets])"
  },
  {
    "id": "191",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/010_deployment_release-name-sealed-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: release-name-sealed-secrets\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits: {}\n          requests: {}\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-sealed-secrets\" not found"
  },
  {
    "id": "192",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/010_deployment_release-name-sealed-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: release-name-sealed-secrets\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits: {}\n          requests: {}\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"controller\" has cpu request 0"
  },
  {
    "id": "193",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/010_deployment_release-name-sealed-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: release-name-sealed-secrets\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits: {}\n          requests: {}\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "194",
    "manifest_path": "data/manifests/artifacthub/bitnami/external-dns/002_poddisruptionbudget_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: external-dns\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "195",
    "manifest_path": "data/manifests/artifacthub/bitnami/external-dns/006_service_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: http\n    port: 7979\n    protocol: TCP\n    targetPort: http\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: external-dns\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:external-dns])"
  },
  {
    "id": "196",
    "manifest_path": "data/manifests/artifacthub/bitnami/external-dns/007_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  revisionHistoryLimit: 10\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: external-dns\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/version: 0.18.0\n        helm.sh/chart: external-dns-9.0.3\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: external-dns\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      containers:\n      - name: external-dns\n        image: docker.io/bitnami/external-dns:0.18.0-debian-12-r4\n        imagePullPolicy: IfNotPresent\n        args:\n        - --metrics-address=:7979\n        - --log-level=info\n        - --log-format=text\n        - --policy=upsert-only\n        - --provider=aws\n        - --registry=txt\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --aws-api-retries=3\n        - --aws-zone-type=\n        - --aws-batch-change-size=1000\n        env:\n        - name: AWS_DEFAULT_REGION\n          value: us-east-1\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 7979\n        livenessProbe:\n          tcpSocket:\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 2\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-external-dns\" not found"
  },
  {
    "id": "197",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/002_poddisruptionbudget_release-name-kafka-broker.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-kafka-broker\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: broker\n    app.kubernetes.io/part-of: kafka\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: broker\n      app.kubernetes.io/part-of: kafka\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "198",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/003_poddisruptionbudget_release-name-kafka-controller.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-kafka-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: controller-eligible\n      app.kubernetes.io/part-of: kafka\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "199",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/009_service_release-name-kafka-controller-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kafka-controller-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-interbroker\n    port: 9094\n    protocol: TCP\n    targetPort: interbroker\n  - name: tcp-client\n    port: 9092\n    protocol: TCP\n    targetPort: client\n  - name: tcp-controller\n    protocol: TCP\n    port: 9093\n    targetPort: controller\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller-eligible app.kubernetes.io/instance:release-name app.kubernetes.io/name:kafka app.kubernetes.io/part-of:kafka])"
  },
  {
    "id": "200",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/010_service_release-name-kafka.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kafka\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: kafka\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-client\n    port: 9092\n    protocol: TCP\n    targetPort: client\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/part-of: kafka\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kafka app.kubernetes.io/part-of:kafka])"
  },
  {
    "id": "201",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/011_statefulset_release-name-kafka-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-kafka-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  podManagementPolicy: Parallel\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: controller-eligible\n      app.kubernetes.io/part-of: kafka\n  serviceName: release-name-kafka-controller-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: kafka\n        app.kubernetes.io/version: 4.0.0\n        helm.sh/chart: kafka-32.4.3\n        app.kubernetes.io/component: controller-eligible\n        app.kubernetes.io/part-of: kafka\n      annotations:\n        checksum/configuration: d96601aff02b0e88a9bc5c8593a6d0446462e05650b1eb84af185e551160d1c8\n        checksum/secret: dba2cdb043e84e63d768aad3292379237050a24915b1b3e70c1f2408e8cd76e8\n    spec:\n      automountServiceAccountToken: false\n      hostNetwork: false\n      hostIPC: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: kafka\n                  app.kubernetes.io/component: controller-eligible\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        seccompProfile:\n          type: RuntimeDefault\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-kafka\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-config\n        image: docker.io/bitnami/kafka:4.0.0-debian-12-r10\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add: []\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \". /opt/bitnami/scripts/libkafka.sh\\nconfigure_kafka_sasl() {\\n    # Replace\\\n          \\ placeholders with passwords\\n    replace_in_file \\\"$KAFKA_CONF_FILE\\\"\\\n          \\ \\\"interbroker-password-placeholder\\\" \\\"$KAFKA_INTER_BROKER_PASSWORD\\\"\\n\\\n          \\    replace_in_file \\\"$KAFKA_CONF_FILE\\\" \\\"controller-password-placeholder\\\"\\\n          \\ \\\"$KAFKA_CONTROLLER_PASSWORD\\\"\\n    read -r -a passwords <<< \\\"$(tr ',;'\\\n          \\ ' ' <<<\\\"${KAFKA_CLIENT_PASSWORDS:-}\\\")\\\"\\n    for ((i = 0; i < ${#passwords[@]};\\\n          \\ i++)); do\\n        replace_in_file \\\"$KAFKA_CONF_FILE\\\" \\\"password-placeholder-${i}\\\\\\\n          \\\"\\\" \\\"${passwords[i]}\\\\\\\"\\\"\\n    done\\n}\\n\\ncp /configmaps/server.properties\\\n          \\ $KAFKA_CONF_FILE\\n\\n# Get pod ID and role, last and second last fields\\\n          \\ in the pod name respectively\\nPOD_ID=\\\"${MY_POD_NAME##*-}\\\"\\nPOD_ROLE=\\\"\\\n          ${MY_POD_NAME%-*}\\\"; POD_ROLE=\\\"${POD_ROLE##*-}\\\"\\n\\n# Configure node.id\\n\\\n          ID=$((POD_ID + KAFKA_MIN_ID))\\n[[ -f \\\"/bitnami/kafka/data/meta.properties\\\"\\\n          \\ ]] && ID=\\\"$(grep \\\"node.id\\\" /bitnami/kafka/data/meta.properties | awk\\\n          \\ -F '=' '{print $2}')\\\"\\nkafka_server_conf_set \\\"node.id\\\" \\\"$ID\\\"\\n# Configure\\\n          \\ initial controllers\\nif [[ \\\"controller\\\" =~ \\\"$POD_ROLE\\\" ]]; then\\n\\\n          \\    INITIAL_CONTROLLERS=()\\n    for ((i = 0; i < 3; i++)); do\\n       \\\n          \\ var=\\\"KAFKA_CONTROLLER_${i}_DIR_ID\\\"; DIR_ID=\\\"${!var}\\\"\\n        [[ $i\\\n          \\ -eq $POD_ID ]] && [[ -f \\\"/bitnami/kafka/data/meta.properties\\\" ]] &&\\\n          \\ DIR_ID=\\\"$(grep \\\"directory.id\\\" /bitnami/kafka/data/meta.properties |\\\n          \\ awk -F '=' '{print $2}')\\\"\\n        INITIAL_CONTROLLERS+=(\\\"${i}@${KAFKA_FULLNAME}-${POD_ROLE}-${i}.${KAFKA_CONTROLLER_SVC_NAME}.${MY_POD_NAMESPACE}.svc.${CLUSTER_DOMAIN}:${KAFKA_CONTROLLER_PORT}:${DIR_ID}\\\"\\\n          )\\n    done\\n    echo \\\"${INITIAL_CONTROLLERS[*]}\\\" | awk -v OFS=',' '{$1=$1}1'\\\n          \\ > /shared/initial-controllers.txt\\nfi\\nreplace_in_file \\\"$KAFKA_CONF_FILE\\\"\\\n          \\ \\\"advertised-address-placeholder\\\" \\\"${MY_POD_NAME}.${KAFKA_FULLNAME}-${POD_ROLE}-headless.${MY_POD_NAMESPACE}.svc.${CLUSTER_DOMAIN}\\\"\\\n          \\nsasl_env_vars=(\\n  KAFKA_CLIENT_PASSWORDS\\n  KAFKA_INTER_BROKER_PASSWORD\\n\\\n          \\  KAFKA_INTER_BROKER_CLIENT_SECRET\\n  KAFKA_CONTROLLER_PASSWORD\\n  KAFKA_CONTROLLER_CLIENT_SECRET\\n\\\n          )\\nfor env_var in \\\"${sasl_env_vars[@]}\\\"; do\\n    file_env_var=\\\"${env_var}_FILE\\\"\\\n          \\n    if [[ -n \\\"${!file_env_var:-}\\\" ]]; then\\n        if [[ -r \\\"${!file_env_var:-}\\\"\\\n          \\ ]]; then\\n            export \\\"${env_var}=$(< \\\"${!file_env_var}\\\")\\\"\\n\\\n          \\            unset \\\"${file_env_var}\\\"\\n        else\\n            warn \\\"\\\n          Skipping export of '${env_var}'. '${!file_env_var:-}' is not readable.\\\"\\\n          \\n        fi\\n    fi\\ndone\\nconfigure_kafka_sasl\\nif [[ -f /secret-config/server-secret.properties\\\n          \\ ]]; then\\n    cat /secret-config/server-secret.properties >> $KAFKA_CONF_FILE\\n\\\n          fi\\n\"\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_FULLNAME\n          value: release-name-kafka\n        - name: CLUSTER_DOMAIN\n          value: cluster.local\n        - name: KAFKA_VOLUME_DIR\n          value: /bitnami/kafka\n        - name: KAFKA_CONF_FILE\n          value: /config/server.properties\n        - name: KAFKA_MIN_ID\n          value: '0'\n        - name: KAFKA_CONTROLLER_SVC_NAME\n          value: release-name-kafka-controller-headless\n        - name: KAFKA_CONTROLLER_PORT\n          value: '9093'\n        - name: KAFKA_CONTROLLER_0_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-0-id\n        - name: KAFKA_CONTROLLER_1_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-1-id\n        - name: KAFKA_CONTROLLER_2_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-2-id\n        - name: KAFKA_CLIENT_USERS\n          value: user1\n        - name: KAFKA_CLIENT_PASSWORDS_FILE\n          value: /opt/bitnami/kafka/config/secrets/client-passwords\n        - name: KAFKA_INTER_BROKER_USER\n          value: inter_broker_user\n        - name: KAFKA_INTER_BROKER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/inter-broker-password\n        - name: KAFKA_CONTROLLER_USER\n          value: controller_user\n        - name: KAFKA_CONTROLLER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/controller-password\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/kafka\n        - name: kafka-config\n          mountPath: /config\n        - name: kafka-configmaps\n          mountPath: /configmaps\n        - name: kafka-secret-config\n          mountPath: /secret-config\n        - name: tmp\n          mountPath: /tmp\n        - name: init-shared\n          mountPath: /shared\n        - name: kafka-sasl\n          mountPath: /opt/bitnami/kafka/config/secrets\n          readOnly: true\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:4.0.0-debian-12-r10\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n        env:\n        - name: KAFKA_HEAP_OPTS\n          value: -XX:InitialRAMPercentage=75 -XX:MaxRAMPercentage=75\n        - name: KAFKA_CFG_PROCESS_ROLES\n          value: controller,broker\n        - name: KAFKA_INITIAL_CONTROLLERS_FILE\n          value: /shared/initial-controllers.txt\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: KAFKA_KRAFT_CLUSTER_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: cluster-id\n        - name: KAFKA_KRAFT_BOOTSTRAP_SCRAM_USERS\n          value: 'true'\n        - name: KAFKA_CLIENT_USERS\n          value: user1\n        - name: KAFKA_CLIENT_PASSWORDS_FILE\n          value: /opt/bitnami/kafka/config/secrets/client-passwords\n        - name: KAFKA_INTER_BROKER_USER\n          value: inter_broker_user\n        - name: KAFKA_INTER_BROKER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/inter-broker-password\n        - name: KAFKA_CONTROLLER_USER\n          value: controller_user\n        - name: KAFKA_CONTROLLER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/controller-password\n        ports:\n        - name: controller\n          containerPort: 9093\n        - name: client\n          containerPort: 9092\n        - name: interbroker\n          containerPort: 9094\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - pgrep\n            - -f\n            - kafka\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: controller\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/kafka\n        - name: logs\n          mountPath: /opt/bitnami/kafka/logs\n        - name: kafka-config\n          mountPath: /opt/bitnami/kafka/config/server.properties\n          subPath: server.properties\n        - name: tmp\n          mountPath: /tmp\n        - name: init-shared\n          mountPath: /shared\n        - name: kafka-sasl\n          mountPath: /opt/bitnami/kafka/config/secrets\n          readOnly: true\n      volumes:\n      - name: kafka-configmaps\n        configMap:\n          name: release-name-kafka-controller-configuration\n      - name: kafka-secret-config\n        emptyDir: {}\n      - name: kafka-config\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: init-shared\n        emptyDir: {}\n      - name: kafka-sasl\n        projected:\n          sources:\n          - secret:\n              name: release-name-kafka-user-passwords\n      - name: logs\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kafka\" not found"
  },
  {
    "id": "202",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/003_poddisruptionbudget_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "203",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/004_poddisruptionbudget_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: keycloak\n      app.kubernetes.io/component: keycloak\n      app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "204",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/010_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "205",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/011_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "206",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/012_service_release-name-keycloak-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-keycloak-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: http\n  publishNotReadyAddresses: true\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:keycloak app.kubernetes.io/instance:release-name app.kubernetes.io/name:keycloak app.kubernetes.io/part-of:keycloak])"
  },
  {
    "id": "207",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/013_service_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:keycloak app.kubernetes.io/instance:release-name app.kubernetes.io/name:keycloak app.kubernetes.io/part-of:keycloak])"
  },
  {
    "id": "208",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/014_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 17.6.0\n        helm.sh/chart: postgresql-16.7.26\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:17.6.0-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_USER\n          value: bn_keycloak\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/password\n        - name: POSTGRES_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRES_DATABASE\n          value: bitnami_keycloak\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"bn_keycloak\" -d \"dbname=bitnami_keycloak\" -h 127.0.0.1\n              -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"bn_keycloak\" -d \"dbname=bitnami_keycloak\" -h 127.0.0.1\n              -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-postgresql\" not found"
  },
  {
    "id": "209",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/015_statefulset_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  podManagementPolicy: Parallel\n  serviceName: release-name-keycloak-headless\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: keycloak\n      app.kubernetes.io/component: keycloak\n      app.kubernetes.io/part-of: keycloak\n  template:\n    metadata:\n      annotations:\n        checksum/configmap-env-vars: 32b97b2f95a4b4c37d1e7ba71916ca4c1f73d024fd8a3e077f2c86fba821b469\n        checksum/secrets: 04a6ccee11f98c05bad75a87ca3f46ce99e265b4b5d35d83a1a541641c64d855\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: keycloak\n        app.kubernetes.io/version: 26.3.3\n        helm.sh/chart: keycloak-25.2.0\n        app.kubernetes.io/component: keycloak\n        app.kubernetes.io/part-of: keycloak\n    spec:\n      serviceAccountName: release-name-keycloak\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: keycloak\n                  app.kubernetes.io/component: keycloak\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-write-dirs\n        image: docker.io/bitnami/keycloak:26.3.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - '. /opt/bitnami/scripts/liblog.sh\n\n\n          info \"Copying writable dirs to empty dir\"\n\n          # In order to not break the application functionality we need to make some\n\n          # directories writable, so we need to copy it to an empty dir volume\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/lib/quarkus /emptydir/app-quarkus-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/data /emptydir/app-data-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/providers /emptydir/app-providers-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/themes /emptydir/app-themes-dir\n\n          info \"Copy operation completed\"\n\n          '\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: keycloak\n        image: docker.io/bitnami/keycloak:26.3.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        envFrom:\n        - configMapRef:\n            name: release-name-keycloak-env-vars\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        - name: discovery\n          containerPort: 7800\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 1\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /realms/master\n            port: http\n            scheme: HTTP\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /bitnami/keycloak\n          subPath: app-volume-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/lib/quarkus\n          subPath: app-quarkus-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/data\n          subPath: app-data-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/providers\n          subPath: app-providers-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/themes\n          subPath: app-themes-dir\n        - name: keycloak-secrets\n          mountPath: /opt/bitnami/keycloak/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: keycloak-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-keycloak\n          - secret:\n              name: release-name-postgresql\n              items:\n              - key: password\n                path: db-password\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-keycloak\" not found"
  },
  {
    "id": "210",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/002_poddisruptionbudget_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "211",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/006_service_release-name-mariadb-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\nspec:\n  type: ClusterIP\n  publishNotReadyAddresses: true\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/part-of: mariadb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb app.kubernetes.io/part-of:mariadb])"
  },
  {
    "id": "212",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/007_service_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb])"
  },
  {
    "id": "213",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "214",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "215",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mariadb\" not found"
  },
  {
    "id": "216",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/003_poddisruptionbudget_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: console\n      app.kubernetes.io/part-of: minio\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "217",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/004_poddisruptionbudget_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: minio\n      app.kubernetes.io/part-of: minio\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "218",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/008_service_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 9090\n    targetPort: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:console app.kubernetes.io/instance:release-name app.kubernetes.io/name:minio app.kubernetes.io/part-of:minio])"
  },
  {
    "id": "219",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/009_service_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  type: ClusterIP\n  ports:\n  - name: tcp-api\n    port: 9000\n    targetPort: api\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:minio app.kubernetes.io/instance:release-name app.kubernetes.io/name:minio app.kubernetes.io/part-of:minio])"
  },
  {
    "id": "220",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/010_deployment_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: minio\n      app.kubernetes.io/part-of: minio\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: minio\n        app.kubernetes.io/version: 2025.7.23\n        helm.sh/chart: minio-17.0.21\n        app.kubernetes.io/component: minio\n        app.kubernetes.io/part-of: minio\n      annotations:\n        checksum/credentials-secret: d7f9b363039fe1a5911ad90a3de83dbcfe0eba441592f6815c91be1861016459\n    spec:\n      serviceAccountName: release-name-minio\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: minio\n                  app.kubernetes.io/component: minio\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: OnRootMismatch\n        supplementalGroups: []\n        sysctls: []\n      initContainers: null\n      containers:\n      - name: minio\n        image: docker.io/bitnami/minio:2025.7.23-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MINIO_DISTRIBUTED_MODE_ENABLED\n          value: 'no'\n        - name: MINIO_SCHEME\n          value: http\n        - name: MINIO_FORCE_NEW_KEYS\n          value: 'no'\n        - name: MINIO_ROOT_USER_FILE\n          value: /opt/bitnami/minio/secrets/root-user\n        - name: MINIO_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/minio/secrets/root-password\n        - name: MINIO_SKIP_CLIENT\n          value: 'yes'\n        - name: MINIO_API_PORT_NUMBER\n          value: '9000'\n        - name: MINIO_BROWSER\n          value: 'off'\n        - name: MINIO_PROMETHEUS_AUTH_TYPE\n          value: public\n        - name: MINIO_DATA_DIR\n          value: /bitnami/minio/data\n        ports:\n        - name: api\n          containerPort: 9000\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: api\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          tcpSocket:\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/minio/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /.mc\n          subPath: app-mc-dir\n        - name: minio-credentials\n          mountPath: /opt/bitnami/minio/secrets/\n        - name: data\n          mountPath: /bitnami/minio/data\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: minio-credentials\n        secret:\n          secretName: release-name-minio\n      - name: data\n        persistentVolumeClaim:\n          claimName: release-name-minio\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-minio\" not found"
  },
  {
    "id": "221",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/011_deployment_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: console\n      app.kubernetes.io/part-of: minio\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: minio\n        app.kubernetes.io/version: 2025.7.23\n        helm.sh/chart: minio-17.0.21\n        app.kubernetes.io/component: console\n        app.kubernetes.io/part-of: minio\n    spec:\n      serviceAccountName: release-name-minio\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: minio\n                  app.kubernetes.io/component: console\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: console\n        image: docker.io/bitnami/minio-object-browser:2.0.2-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - server\n        - --host\n        - 0.0.0.0\n        - --port\n        - '9090'\n        env:\n        - name: CONSOLE_MINIO_SERVER\n          value: http://release-name-minio:9000\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        ports:\n        - name: http\n          containerPort: 9090\n        livenessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          httpGet:\n            path: /minio\n            port: http\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /.console\n          subPath: app-console-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-minio\" not found"
  },
  {
    "id": "222",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/002_poddisruptionbudget_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "223",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/007_service_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  publishNotReadyAddresses: false\n  ports:\n  - name: mongodb\n    port: 27017\n    targetPort: mongodb\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/component: mongodb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:mongodb app.kubernetes.io/instance:release-name app.kubernetes.io/name:mongodb])"
  },
  {
    "id": "224",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"log-dir\" is using an invalid container image, \"registry-1.docker.io/bitnami/mongodb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "225",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongodb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mongodb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "226",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mongodb\" not found"
  },
  {
    "id": "227",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/002_poddisruptionbudget_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mysql\n      app.kubernetes.io/part-of: mysql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "228",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/006_service_release-name-mysql-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mysql-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: mysql\n    port: 3306\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mysql])"
  },
  {
    "id": "229",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/007_service_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mysql app.kubernetes.io/part-of:mysql])"
  },
  {
    "id": "230",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/008_statefulset_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  podManagementPolicy: ''\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mysql\n      app.kubernetes.io/part-of: mysql\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mysql-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 0aa4c7bb029f4871ca0cdece35adf5a5caaf6f2e016ef25c8cae18901c047e48\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mysql\n        app.kubernetes.io/version: 9.4.0\n        helm.sh/chart: mysql-14.0.3\n        app.kubernetes.io/part-of: mysql\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mysql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mysql\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: docker.io/bitnami/mysql:9.4.0-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mysql/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mysql/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mysql\n        image: docker.io/bitnami/mysql:9.4.0-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MYSQL_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mysql/secrets/mysql-root-password\n        - name: MYSQL_ENABLE_SSL\n          value: 'no'\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_DATABASE\n          value: my_database\n        envFrom: null\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin ping -uroot -p\\\"${password_aux}\\\" | grep \\\"mysqld is\\\n              \\ alive\\\"\\n\"\n        startupProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin ping -uroot -p\\\"${password_aux}\\\" | grep \\\"mysqld is\\\n              \\ alive\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mysql\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/logs\n          subPath: app-logs-dir\n        - name: config\n          mountPath: /opt/bitnami/mysql/conf/my.cnf\n          subPath: my.cnf\n        - name: mysql-credentials\n          mountPath: /opt/bitnami/mysql/secrets/\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-mysql\n      - name: mysql-credentials\n        secret:\n          secretName: release-name-mysql\n          items:\n          - key: mysql-root-password\n            path: mysql-root-password\n          - key: mysql-password\n            path: mysql-password\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mysql\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mysql\" not found"
  },
  {
    "id": "231",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/002_poddisruptionbudget_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "232",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/005_service_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\n  annotations: null\nspec:\n  type: LoadBalancer\n  sessionAffinity: None\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n  - name: https\n    port: 443\n    targetPort: https\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: nginx\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:nginx])"
  },
  {
    "id": "233",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"registry-1.docker.io/bitnami/nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "234",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "235",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-nginx\" not found"
  },
  {
    "id": "236",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/002_poddisruptionbudget_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "237",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/005_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "238",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/006_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "239",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgresql\" is using an invalid container image, \"registry-1.docker.io/bitnami/postgresql:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "240",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-postgresql\" not found"
  },
  {
    "id": "241",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/002_poddisruptionbudget_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: rabbitmq\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "242",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/008_service_release-name-rabbitmq-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-rabbitmq-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  clusterIP: None\n  ports:\n  - name: epmd\n    port: 4369\n    targetPort: epmd\n  - name: amqp\n    port: 5672\n    targetPort: amqp\n  - name: dist\n    port: 25672\n    targetPort: dist\n  - name: http-stats\n    port: 15672\n    targetPort: stats\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: rabbitmq\n  publishNotReadyAddresses: true\n  trafficDistribution: PreferClose\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:rabbitmq])"
  },
  {
    "id": "243",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/009_service_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: amqp\n    port: 5672\n    targetPort: amqp\n    nodePort: null\n  - name: epmd\n    port: 4369\n    targetPort: epmd\n    nodePort: null\n  - name: dist\n    port: 25672\n    targetPort: dist\n    nodePort: null\n  - name: http-stats\n    port: 15672\n    targetPort: stats\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: rabbitmq\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:rabbitmq])"
  },
  {
    "id": "244",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/010_statefulset_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  serviceName: release-name-rabbitmq-headless\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: rabbitmq\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: rabbitmq\n        app.kubernetes.io/version: 4.1.3\n        helm.sh/chart: rabbitmq-16.0.14\n      annotations:\n        checksum/config: 81f24711d28981f706e1ae5b2e3ae075d7014b462120496ce6f50d5053194f5e\n        checksum/secret: 0db9412a28617166460cac5333a5cf8ebbc34cfe87087e0f09b593395c5fa678\n    spec:\n      serviceAccountName: release-name-rabbitmq\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: rabbitmq\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: true\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      terminationGracePeriodSeconds: 120\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-plugins-dir\n        image: docker.io/bitnami/rabbitmq:4.1.3-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - '#!/bin/bash\n\n\n          . /opt/bitnami/scripts/liblog.sh\n\n\n          info \"Copying plugins dir to empty dir\"\n\n          # In order to not break the possibility of installing custom plugins, we\n          need\n\n          # to make the plugins directory writable, so we need to copy it to an empty\n          dir volume\n\n          cp -r --preserve=mode /opt/bitnami/rabbitmq/plugins/ /emptydir/app-plugins-dir\n\n          '\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: rabbitmq\n        image: docker.io/bitnami/rabbitmq:4.1.3-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/bash\n              - -ec\n              - \"if [[ -f /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh ]]; then\\n\\\n                \\    /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh -t \\\"120\\\" -d \\\"\\\n                false\\\"\\nelse\\n    rabbitmqctl stop_app\\nfi\\n\"\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RABBITMQ_FORCE_BOOT\n          value: 'no'\n        - name: RABBITMQ_NODE_NAME\n          value: rabbit@$(MY_POD_NAME).release-name-rabbitmq-headless.$(MY_POD_NAMESPACE).svc.cluster.local\n        - name: RABBITMQ_UPDATE_PASSWORD\n          value: 'no'\n        - name: RABBITMQ_MNESIA_DIR\n          value: /opt/bitnami/rabbitmq/.rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)\n        - name: RABBITMQ_LDAP_ENABLE\n          value: 'no'\n        - name: RABBITMQ_LOGS\n          value: '-'\n        - name: RABBITMQ_ULIMIT_NOFILES\n          value: '65535'\n        - name: RABBITMQ_USE_LONGNAME\n          value: 'true'\n        - name: RABBITMQ_ERL_COOKIE_FILE\n          value: /opt/bitnami/rabbitmq/secrets/rabbitmq-erlang-cookie\n        - name: RABBITMQ_LOAD_DEFINITIONS\n          value: 'no'\n        - name: RABBITMQ_DEFINITIONS_FILE\n          value: /app/load_definition.json\n        - name: RABBITMQ_SECURE_PASSWORD\n          value: 'yes'\n        - name: RABBITMQ_USERNAME\n          value: user\n        - name: RABBITMQ_PASSWORD_FILE\n          value: /opt/bitnami/rabbitmq/secrets/rabbitmq-password\n        - name: RABBITMQ_PLUGINS\n          value: rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap\n        envFrom: null\n        ports:\n        - name: amqp\n          containerPort: 5672\n        - name: dist\n          containerPort: 25672\n        - name: stats\n          containerPort: 15672\n        - name: epmd\n          containerPort: 4369\n        - name: metrics\n          containerPort: 9419\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 20\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - curl -f --user user:$(< $RABBITMQ_PASSWORD_FILE) 127.0.0.1:15672/api/health/checks/virtual-hosts\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 20\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - curl -f --user user:$(< $RABBITMQ_PASSWORD_FILE) 127.0.0.1:15672/api/health/checks/local-alarms\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: configuration\n          mountPath: /bitnami/rabbitmq/conf\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/etc/rabbitmq\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/var/lib/rabbitmq\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/.rabbitmq/\n          subPath: app-erlang-cookie\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/var/log/rabbitmq\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/plugins\n          subPath: app-plugins-dir\n        - name: data\n          mountPath: /opt/bitnami/rabbitmq/.rabbitmq/mnesia\n        - name: rabbitmq-secrets\n          mountPath: /opt/bitnami/rabbitmq/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: configuration\n        projected:\n          sources:\n          - secret:\n              name: release-name-rabbitmq-config\n      - name: rabbitmq-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-rabbitmq\n          - secret:\n              name: release-name-rabbitmq\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: rabbitmq\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-rabbitmq\" not found"
  },
  {
    "id": "245",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/002_poddisruptionbudget_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "246",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/003_poddisruptionbudget_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "247",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/010_service_release-name-redis-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "248",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/011_service_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  type: ClusterIP\n  internalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/component: master\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:master app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "249",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/012_service_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  type: ClusterIP\n  internalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/component: replica\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:replica app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "250",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"registry-1.docker.io/bitnami/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "251",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-redis-master\" not found"
  },
  {
    "id": "252",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"registry-1.docker.io/bitnami/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "253",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-redis-replica\" not found"
  },
  {
    "id": "254",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/003_poddisruptionbudget_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query-frontend\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "255",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/004_poddisruptionbudget_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "256",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/007_service_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9090\n    targetPort: http\n    protocol: TCP\n    name: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query-frontend\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query-frontend app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "257",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/008_service_release-name-thanos-query-grpc.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query-grpc\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  type: ClusterIP\n  ports:\n  - port: 10901\n    targetPort: grpc\n    protocol: TCP\n    name: grpc\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "258",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/009_service_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9090\n    targetPort: http\n    protocol: TCP\n    name: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "259",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/010_deployment_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: thanos\n        app.kubernetes.io/version: 0.39.2\n        helm.sh/chart: thanos-17.3.1\n        app.kubernetes.io/component: query-frontend\n    spec:\n      serviceAccountName: release-name-thanos-query-frontend\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: thanos\n                  app.kubernetes.io/component: query-frontend\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: query-frontend\n        image: docker.io/bitnami/thanos:0.39.2-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - query-frontend\n        - --log.level=info\n        - --log.format=logfmt\n        - --http-address=0.0.0.0:9090\n        - --query-frontend.downstream-url=http://release-name-thanos-query:9090\n        ports:\n        - name: http\n          containerPort: 9090\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/healthy\n            port: http\n            scheme: HTTP\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/ready\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts: null\n      volumes: null\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-thanos-query-frontend\" not found"
  },
  {
    "id": "260",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/011_deployment_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: thanos\n        app.kubernetes.io/version: 0.39.2\n        helm.sh/chart: thanos-17.3.1\n        app.kubernetes.io/component: query\n    spec:\n      serviceAccountName: release-name-thanos-query\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: thanos\n                  app.kubernetes.io/component: query\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: query\n        image: docker.io/bitnami/thanos:0.39.2-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - query\n        - --log.level=info\n        - --log.format=logfmt\n        - --grpc-address=0.0.0.0:10901\n        - --http-address=0.0.0.0:10902\n        - --query.replica-label=replica\n        - --alert.query-url=http://release-name-thanos-query.default.svc.cluster.local:9090\n        ports:\n        - name: http\n          containerPort: 10902\n          protocol: TCP\n        - name: grpc\n          containerPort: 10901\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/healthy\n            port: http\n            scheme: HTTP\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/ready\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts: null\n      volumes: null\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-thanos-query\" not found"
  },
  {
    "id": "261",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/003_poddisruptionbudget_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "262",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/004_poddisruptionbudget_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "263",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/011_service_release-name-mariadb-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\nspec:\n  type: ClusterIP\n  publishNotReadyAddresses: true\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/part-of: mariadb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb app.kubernetes.io/part-of:mariadb])"
  },
  {
    "id": "264",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/012_service_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb])"
  },
  {
    "id": "265",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/013_service_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  type: LoadBalancer\n  externalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: wordpress\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:wordpress])"
  },
  {
    "id": "266",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"prepare-base-dir\" is using an invalid container image, \"registry-1.docker.io/bitnami/wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "267",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wordpress\" is using an invalid container image, \"registry-1.docker.io/bitnami/wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "268",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-wordpress\" not found"
  },
  {
    "id": "269",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "270",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "271",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mariadb\" not found"
  },
  {
    "id": "272",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/035_service_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - protocol: TCP\n    port: 9402\n    name: http-metrics\n  selector:\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cainjector app.kubernetes.io/instance:release-name app.kubernetes.io/name:cainjector])"
  },
  {
    "id": "273",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/036_service_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - protocol: TCP\n    port: 9402\n    name: tcp-prometheus-servicemonitor\n    targetPort: http-metrics\n  selector:\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:cert-manager])"
  },
  {
    "id": "274",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/037_service_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  - name: metrics\n    port: 9402\n    protocol: TCP\n    targetPort: http-metrics\n  selector:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:webhook app.kubernetes.io/instance:release-name app.kubernetes.io/name:webhook])"
  },
  {
    "id": "275",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-cert-manager-cainjector\" not found"
  },
  {
    "id": "276",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-manager-cainjector\" has cpu request 0"
  },
  {
    "id": "277",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-manager-cainjector\" has memory limit 0"
  },
  {
    "id": "278",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-cert-manager\" not found"
  },
  {
    "id": "279",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-manager-controller\" has cpu request 0"
  },
  {
    "id": "280",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-manager-controller\" has memory limit 0"
  },
  {
    "id": "281",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-cert-manager-webhook\" not found"
  },
  {
    "id": "282",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-manager-webhook\" has cpu request 0"
  },
  {
    "id": "283",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-manager-webhook\" has memory limit 0"
  },
  {
    "id": "284",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "285",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-cert-manager-startupapicheck\" not found"
  },
  {
    "id": "286",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-manager-startupapicheck\" has cpu request 0"
  },
  {
    "id": "287",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-manager-startupapicheck\" has memory limit 0"
  },
  {
    "id": "288",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/019_service_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9964'\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/name: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    io.cilium/app: proxy\nspec:\n  clusterIP: None\n  type: ClusterIP\n  selector:\n    k8s-app: cilium-envoy\n  ports:\n  - name: envoy-metrics\n    port: 9964\n    protocol: TCP\n    targetPort: envoy-metrics\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:cilium-envoy])"
  },
  {
    "id": "289",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/020_service_hubble-peer.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hubble-peer\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: hubble-peer\nspec:\n  selector:\n    k8s-app: cilium\n  ports:\n  - name: peer-service\n    port: 443\n    protocol: TCP\n    targetPort: 4244\n  internalTrafficPolicy: Local\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[k8s-app:cilium])"
  },
  {
    "id": "290",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cilium-agent\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "291",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "292",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-agent\" does not expose port 9879 for the HTTPGet"
  },
  {
    "id": "293",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"apply-sysctl-overwrites\" does not have a read-only root file system"
  },
  {
    "id": "294",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "295",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "296",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"config\" does not have a read-only root file system"
  },
  {
    "id": "297",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"install-cni-binaries\" does not have a read-only root file system"
  },
  {
    "id": "298",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mount-bpf-fs\" does not have a read-only root file system"
  },
  {
    "id": "299",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mount-cgroup\" does not have a read-only root file system"
  },
  {
    "id": "300",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium\" not found"
  },
  {
    "id": "301",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"apply-sysctl-overwrites\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "302",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "303",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "304",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"mount-bpf-fs\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "305",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"mount-cgroup\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "306",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"mount-bpf-fs\" is privileged"
  },
  {
    "id": "307",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cilium-agent\" does not expose port 9879 for the HTTPGet"
  },
  {
    "id": "308",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"apply-sysctl-overwrites\" is not set to runAsNonRoot"
  },
  {
    "id": "309",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "310",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "311",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"config\" is not set to runAsNonRoot"
  },
  {
    "id": "312",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"install-cni-binaries\" is not set to runAsNonRoot"
  },
  {
    "id": "313",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mount-bpf-fs\" is not set to runAsNonRoot"
  },
  {
    "id": "314",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mount-cgroup\" is not set to runAsNonRoot"
  },
  {
    "id": "315",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"apply-sysctl-overwrites\""
  },
  {
    "id": "316",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"mount-cgroup\""
  },
  {
    "id": "317",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "startup-port",
    "violation_text": "container \"cilium-agent\" does not expose port 9879 for the HTTPGet"
  },
  {
    "id": "318",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"apply-sysctl-overwrites\" has cpu request 0"
  },
  {
    "id": "319",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "320",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "321",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config\" has cpu request 0"
  },
  {
    "id": "322",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mount-bpf-fs\" has cpu request 0"
  },
  {
    "id": "323",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mount-cgroup\" has cpu request 0"
  },
  {
    "id": "324",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"apply-sysctl-overwrites\" has memory limit 0"
  },
  {
    "id": "325",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "326",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "327",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config\" has memory limit 0"
  },
  {
    "id": "328",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"install-cni-binaries\" has memory limit 0"
  },
  {
    "id": "329",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mount-bpf-fs\" has memory limit 0"
  },
  {
    "id": "330",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mount-cgroup\" has memory limit 0"
  },
  {
    "id": "331",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "332",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "333",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-envoy\" does not have a read-only root file system"
  },
  {
    "id": "334",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium-envoy\" not found"
  },
  {
    "id": "335",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-envoy\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "336",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "337",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-envoy\" is not set to runAsNonRoot"
  },
  {
    "id": "338",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "startup-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "339",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-envoy\" has cpu request 0"
  },
  {
    "id": "340",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-envoy\" has memory limit 0"
  },
  {
    "id": "341",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "342",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-operator\" does not expose port 9234 for the HTTPGet"
  },
  {
    "id": "343",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-operator\" does not have a read-only root file system"
  },
  {
    "id": "344",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium-operator\" not found"
  },
  {
    "id": "345",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cilium-operator\" does not expose port 9234 for the HTTPGet"
  },
  {
    "id": "346",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "347",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-operator\" has cpu request 0"
  },
  {
    "id": "348",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-operator\" has memory limit 0"
  },
  {
    "id": "349",
    "manifest_path": "data/manifests/artifacthub/cluster-autoscaler/cluster-autoscaler/001_poddisruptionbudget_release-name-aws-cluster-autoscaler.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cluster-autoscaler-9.50.1\n  name: release-name-aws-cluster-autoscaler\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: aws-cluster-autoscaler\n  maxUnavailable: 1\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "350",
    "manifest_path": "data/manifests/artifacthub/cluster-autoscaler/cluster-autoscaler/007_service_release-name-aws-cluster-autoscaler.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cluster-autoscaler-9.50.1\n  name: release-name-aws-cluster-autoscaler\n  namespace: default\nspec:\n  ports:\n  - port: 8085\n    protocol: TCP\n    targetPort: 8085\n    name: http\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n  type: ClusterIP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:aws-cluster-autoscaler])"
  },
  {
    "id": "351",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/018_service_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  type: ClusterIP\n  selector:\n    app: release-name-datadog-cluster-agent\n  ports:\n  - port: 5005\n    name: agentport\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog-cluster-agent])"
  },
  {
    "id": "352",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/019_service_release-name-datadog-cluster-agent-admission-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog-cluster-agent-admission-controller\n  namespace: default\n  labels:\n    app: release-name-datadog\n    chart: datadog-3.136.1\n    release: release-name\n    heritage: Helm\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  selector:\n    app: release-name-datadog-cluster-agent\n  ports:\n  - port: 443\n    targetPort: 8000\n    name: datadog-webhook\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog-cluster-agent])"
  },
  {
    "id": "353",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/020_service_release-name-datadog.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog\n  namespace: default\n  labels:\n    app: release-name-datadog\n    chart: datadog-3.136.1\n    release: release-name\n    heritage: Helm\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  selector:\n    app: release-name-datadog\n  ports:\n  - protocol: UDP\n    port: 8125\n    targetPort: 8125\n    name: dogstatsdport\n  - protocol: TCP\n    port: 8126\n    targetPort: 8126\n    name: traceport\n  internalTrafficPolicy: Local\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog])"
  },
  {
    "id": "354",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cluster-agent\" does not expose port 5556 for the HTTPGet"
  },
  {
    "id": "355",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-volume\" does not have a read-only root file system"
  },
  {
    "id": "356",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-datadog-cluster-agent\" not found"
  },
  {
    "id": "357",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cluster-agent\" does not expose port 5556 for the HTTPGet"
  },
  {
    "id": "358",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cluster-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "359",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-volume\" is not set to runAsNonRoot"
  },
  {
    "id": "360",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "startup-port",
    "violation_text": "container \"cluster-agent\" does not expose port 5556 for the HTTPGet"
  },
  {
    "id": "361",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cluster-agent\" has cpu request 0"
  },
  {
    "id": "362",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-volume\" has cpu request 0"
  },
  {
    "id": "363",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cluster-agent\" has memory limit 0"
  },
  {
    "id": "364",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-volume\" has memory limit 0"
  },
  {
    "id": "365",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/001_poddisruptionbudget_elasticsearch-master-pdb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: elasticsearch-master-pdb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n",
    "policy_id": "pdb-unhealthy-pod-eviction-policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "366",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/004_service_elasticsearch-master.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations: {}\nspec:\n  type: ClusterIP\n  selector:\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  publishNotReadyAddresses: false\n  ports:\n  - name: http\n    protocol: TCP\n    port: 9200\n  - name: transport\n    protocol: TCP\n    port: 9300\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:elasticsearch-master chart:elasticsearch release:release-name])"
  },
  {
    "id": "367",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/005_service_elasticsearch-master-headless.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch-master-headless\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\nspec:\n  clusterIP: None\n  publishNotReadyAddresses: true\n  selector:\n    app: elasticsearch-master\n  ports:\n  - name: http\n    port: 9200\n  - name: transport\n    port: 9300\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:elasticsearch-master])"
  },
  {
    "id": "368",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"configure-sysctl\" does not have a read-only root file system"
  },
  {
    "id": "369",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"elasticsearch\" does not have a read-only root file system"
  },
  {
    "id": "370",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"configure-sysctl\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "371",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"configure-sysctl\" is privileged"
  },
  {
    "id": "372",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"configure-sysctl\" is not set to runAsNonRoot"
  },
  {
    "id": "373",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"configure-sysctl\" has cpu request 0"
  },
  {
    "id": "374",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"configure-sysctl\" has memory limit 0"
  },
  {
    "id": "375",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-mvgmy-test\" does not have a read-only root file system"
  },
  {
    "id": "376",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-mvgmy-test\" has cpu request 0"
  },
  {
    "id": "377",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-mvgmy-test\" has memory limit 0"
  },
  {
    "id": "378",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/004_service_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  selector:\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n  ports:\n  - name: http\n    port: 7979\n    targetPort: http\n    protocol: TCP\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:external-dns])"
  },
  {
    "id": "379",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-external-dns\" not found"
  },
  {
    "id": "380",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"external-dns\" has cpu request 0"
  },
  {
    "id": "381",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"external-dns\" has memory limit 0"
  },
  {
    "id": "382",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/037_service_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\n    external-secrets.io/component: webhook\nspec:\n  type: ClusterIP\n  ports:\n  - port: 443\n    targetPort: webhook\n    protocol: TCP\n    name: webhook\n  selector:\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:external-secrets-webhook])"
  },
  {
    "id": "383",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"external-secrets-cert-controller\" not found"
  },
  {
    "id": "384",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cert-controller\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "385",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cert-controller\" has cpu request 0"
  },
  {
    "id": "386",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cert-controller\" has memory limit 0"
  },
  {
    "id": "387",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-external-secrets\" not found"
  },
  {
    "id": "388",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"external-secrets\" has cpu request 0"
  },
  {
    "id": "389",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"external-secrets\" has memory limit 0"
  },
  {
    "id": "390",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"external-secrets-webhook\" not found"
  },
  {
    "id": "391",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"webhook\" does not expose port 8081 for the HTTPGet"
  },
  {
    "id": "392",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"webhook\" has cpu request 0"
  },
  {
    "id": "393",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"webhook\" has memory limit 0"
  },
  {
    "id": "394",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/005_service_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - port: 2020\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:fluent-bit])"
  },
  {
    "id": "395",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluent-bit\" does not have a read-only root file system"
  },
  {
    "id": "396",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-fluent-bit\" not found"
  },
  {
    "id": "397",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluent-bit\" is not set to runAsNonRoot"
  },
  {
    "id": "398",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fluent-bit\" has cpu request 0"
  },
  {
    "id": "399",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fluent-bit\" has memory limit 0"
  },
  {
    "id": "400",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  }
]