[
  {
    "id": "001",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "002",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "003",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "004",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "005",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-prometheus-node-exporter\" not found"
  },
  {
    "id": "006",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"node-exporter\""
  },
  {
    "id": "007",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"node-exporter\""
  },
  {
    "id": "008",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"node-exporter\""
  },
  {
    "id": "009",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "010",
    "manifest_path": "data/eval/artifacthub_sample/003_061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "011",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "012",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "013",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "014",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "host-pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "015",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-prometheus-node-exporter\" not found"
  },
  {
    "id": "016",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/\" is mounted on container \"node-exporter\""
  },
  {
    "id": "017",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"node-exporter\""
  },
  {
    "id": "018",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/sys\" is mounted on container \"node-exporter\""
  },
  {
    "id": "019",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "020",
    "manifest_path": "data/eval/artifacthub_sample/007_019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "021",
    "manifest_path": "data/eval/artifacthub_sample/010_006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n  restartPolicy: Never\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "022",
    "manifest_path": "data/eval/artifacthub_sample/010_006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"docker.io/busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "023",
    "manifest_path": "data/eval/artifacthub_sample/010_006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "024",
    "manifest_path": "data/eval/artifacthub_sample/010_006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "025",
    "manifest_path": "data/eval/artifacthub_sample/011_006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "026",
    "manifest_path": "data/eval/artifacthub_sample/011_006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"configure-sysctl\" does not have a read-only root file system"
  },
  {
    "id": "027",
    "manifest_path": "data/eval/artifacthub_sample/011_006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"elasticsearch\" does not have a read-only root file system"
  },
  {
    "id": "028",
    "manifest_path": "data/eval/artifacthub_sample/011_006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"configure-sysctl\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "029",
    "manifest_path": "data/eval/artifacthub_sample/011_006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"configure-sysctl\" is privileged"
  },
  {
    "id": "030",
    "manifest_path": "data/eval/artifacthub_sample/011_006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"configure-sysctl\" is not set to runAsNonRoot"
  },
  {
    "id": "031",
    "manifest_path": "data/eval/artifacthub_sample/011_006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"configure-sysctl\" has cpu request 0"
  },
  {
    "id": "032",
    "manifest_path": "data/eval/artifacthub_sample/011_006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"configure-sysctl\" has memory limit 0"
  },
  {
    "id": "033",
    "manifest_path": "data/eval/artifacthub_sample/013_020_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "034",
    "manifest_path": "data/eval/artifacthub_sample/018_007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "035",
    "manifest_path": "data/eval/artifacthub_sample/018_007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "036",
    "manifest_path": "data/eval/artifacthub_sample/018_007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wget\" does not have a read-only root file system"
  },
  {
    "id": "037",
    "manifest_path": "data/eval/artifacthub_sample/018_007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wget\" is not set to runAsNonRoot"
  },
  {
    "id": "038",
    "manifest_path": "data/eval/artifacthub_sample/018_007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "039",
    "manifest_path": "data/eval/artifacthub_sample/018_007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "040",
    "manifest_path": "data/eval/artifacthub_sample/019_006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "041",
    "manifest_path": "data/eval/artifacthub_sample/019_006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "042",
    "manifest_path": "data/eval/artifacthub_sample/019_006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluent-bit\" does not have a read-only root file system"
  },
  {
    "id": "043",
    "manifest_path": "data/eval/artifacthub_sample/019_006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-fluent-bit\" not found"
  },
  {
    "id": "044",
    "manifest_path": "data/eval/artifacthub_sample/019_006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluent-bit\" is not set to runAsNonRoot"
  },
  {
    "id": "045",
    "manifest_path": "data/eval/artifacthub_sample/019_006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fluent-bit\" has cpu request 0"
  },
  {
    "id": "046",
    "manifest_path": "data/eval/artifacthub_sample/019_006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fluent-bit\" has memory limit 0"
  },
  {
    "id": "047",
    "manifest_path": "data/eval/artifacthub_sample/020_007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "048",
    "manifest_path": "data/eval/artifacthub_sample/020_007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgresql\" is using an invalid container image, \"registry-1.docker.io/bitnami/postgresql:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "049",
    "manifest_path": "data/eval/artifacthub_sample/020_007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-postgresql\" not found"
  },
  {
    "id": "050",
    "manifest_path": "data/eval/artifacthub_sample/021_008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "051",
    "manifest_path": "data/eval/artifacthub_sample/021_008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "052",
    "manifest_path": "data/eval/artifacthub_sample/021_008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "053",
    "manifest_path": "data/eval/artifacthub_sample/021_008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mariadb\" not found"
  },
  {
    "id": "054",
    "manifest_path": "data/eval/artifacthub_sample/022_015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "055",
    "manifest_path": "data/eval/artifacthub_sample/022_015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "056",
    "manifest_path": "data/eval/artifacthub_sample/022_015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "057",
    "manifest_path": "data/eval/artifacthub_sample/022_015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mariadb\" not found"
  },
  {
    "id": "058",
    "manifest_path": "data/eval/artifacthub_sample/023_014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "059",
    "manifest_path": "data/eval/artifacthub_sample/023_014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"prepare-base-dir\" is using an invalid container image, \"registry-1.docker.io/bitnami/wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "060",
    "manifest_path": "data/eval/artifacthub_sample/023_014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wordpress\" is using an invalid container image, \"registry-1.docker.io/bitnami/wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "061",
    "manifest_path": "data/eval/artifacthub_sample/023_014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-wordpress\" not found"
  },
  {
    "id": "062",
    "manifest_path": "data/eval/artifacthub_sample/024_014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "063",
    "manifest_path": "data/eval/artifacthub_sample/024_014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"registry-1.docker.io/bitnami/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "064",
    "manifest_path": "data/eval/artifacthub_sample/024_014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-redis-replica\" not found"
  },
  {
    "id": "065",
    "manifest_path": "data/eval/artifacthub_sample/026_013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "066",
    "manifest_path": "data/eval/artifacthub_sample/026_013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"registry-1.docker.io/bitnami/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "067",
    "manifest_path": "data/eval/artifacthub_sample/026_013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-redis-master\" not found"
  },
  {
    "id": "068",
    "manifest_path": "data/eval/artifacthub_sample/028_007_service_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mongodb\n    port: 27017\n    targetPort: mongodb\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/component: mongodb\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:mongodb app.kubernetes.io/instance:release-name app.kubernetes.io/name:mongodb])"
  },
  {
    "id": "069",
    "manifest_path": "data/eval/artifacthub_sample/029_008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "070",
    "manifest_path": "data/eval/artifacthub_sample/029_008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"log-dir\" is using an invalid container image, \"registry-1.docker.io/bitnami/mongodb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "071",
    "manifest_path": "data/eval/artifacthub_sample/029_008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mongodb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mongodb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "072",
    "manifest_path": "data/eval/artifacthub_sample/029_008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-mongodb\" not found"
  },
  {
    "id": "073",
    "manifest_path": "data/eval/artifacthub_sample/030_006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "074",
    "manifest_path": "data/eval/artifacthub_sample/030_006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n    spec:\n      serviceAccountName: release-name-nginx\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"registry-1.docker.io/bitnami/nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "075",
    "manifest_path": "data/eval/artifacthub_sample/030_006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n    spec:\n      serviceAccountName: release-name-nginx\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "076",
    "manifest_path": "data/eval/artifacthub_sample/030_006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n    spec:\n      serviceAccountName: release-name-nginx\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-nginx\" not found"
  },
  {
    "id": "077",
    "manifest_path": "data/eval/artifacthub_sample/031_005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "078",
    "manifest_path": "data/eval/artifacthub_sample/031_005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "079",
    "manifest_path": "data/eval/artifacthub_sample/031_005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-promtail\" not found"
  },
  {
    "id": "080",
    "manifest_path": "data/eval/artifacthub_sample/031_005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"promtail\" is not set to runAsNonRoot"
  },
  {
    "id": "081",
    "manifest_path": "data/eval/artifacthub_sample/031_005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"promtail\" has cpu request 0"
  },
  {
    "id": "082",
    "manifest_path": "data/eval/artifacthub_sample/031_005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"promtail\" has memory limit 0"
  },
  {
    "id": "083",
    "manifest_path": "data/eval/artifacthub_sample/032_012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "084",
    "manifest_path": "data/eval/artifacthub_sample/032_012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-test\" does not have a read-only root file system"
  },
  {
    "id": "085",
    "manifest_path": "data/eval/artifacthub_sample/032_012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-grafana-test\" not found"
  },
  {
    "id": "086",
    "manifest_path": "data/eval/artifacthub_sample/032_012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"release-name-test\" is not set to runAsNonRoot"
  },
  {
    "id": "087",
    "manifest_path": "data/eval/artifacthub_sample/032_012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"release-name-test\" has cpu request 0"
  },
  {
    "id": "088",
    "manifest_path": "data/eval/artifacthub_sample/032_012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-test\" has memory limit 0"
  },
  {
    "id": "089",
    "manifest_path": "data/eval/artifacthub_sample/033_014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "090",
    "manifest_path": "data/eval/artifacthub_sample/033_014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "091",
    "manifest_path": "data/eval/artifacthub_sample/033_014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-promtail\" not found"
  },
  {
    "id": "092",
    "manifest_path": "data/eval/artifacthub_sample/033_014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"promtail\" is not set to runAsNonRoot"
  },
  {
    "id": "093",
    "manifest_path": "data/eval/artifacthub_sample/033_014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"promtail\" has cpu request 0"
  },
  {
    "id": "094",
    "manifest_path": "data/eval/artifacthub_sample/033_014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"promtail\" has memory limit 0"
  },
  {
    "id": "095",
    "manifest_path": "data/eval/artifacthub_sample/034_013_service_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  type: ClusterIP\n  ports:\n  - port: 3100\n    protocol: TCP\n    name: http-metrics\n    targetPort: http-metrics\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "096",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "097",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "098",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "099",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-pre-upgrade\" does not have a read-only root file system"
  },
  {
    "id": "100",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "101",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"longhorn-pre-upgrade\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "102",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"longhorn-pre-upgrade\" is privileged"
  },
  {
    "id": "103",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-pre-upgrade\" is not set to runAsNonRoot"
  },
  {
    "id": "104",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-pre-upgrade\" has cpu request 0"
  },
  {
    "id": "105",
    "manifest_path": "data/eval/artifacthub_sample/038_043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-pre-upgrade\" has memory limit 0"
  },
  {
    "id": "106",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "107",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "108",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "109",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pre-pull-share-manager-image\" does not have a read-only root file system"
  },
  {
    "id": "110",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "111",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"longhorn-manager\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "112",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"longhorn-manager\" is privileged"
  },
  {
    "id": "113",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"longhorn-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "114",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pre-pull-share-manager-image\" is not set to runAsNonRoot"
  },
  {
    "id": "115",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"longhorn-manager\" has cpu request 0"
  },
  {
    "id": "116",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pre-pull-share-manager-image\" has cpu request 0"
  },
  {
    "id": "117",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"longhorn-manager\" has memory limit 0"
  },
  {
    "id": "118",
    "manifest_path": "data/eval/artifacthub_sample/039_039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pre-pull-share-manager-image\" has memory limit 0"
  },
  {
    "id": "119",
    "manifest_path": "data/eval/artifacthub_sample/041_022_service_release-name-valkey-cluster-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-valkey-cluster-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: tcp-redis\n  - name: tcp-redis-bus\n    port: 16379\n    targetPort: tcp-redis-bus\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: valkey-cluster\n",
    "policy_id": "dangling-service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:valkey-cluster])"
  },
  {
    "id": "120",
    "manifest_path": "data/eval/artifacthub_sample/042_030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "121",
    "manifest_path": "data/eval/artifacthub_sample/042_030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "122",
    "manifest_path": "data/eval/artifacthub_sample/042_030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"wget\" does not have a read-only root file system"
  },
  {
    "id": "123",
    "manifest_path": "data/eval/artifacthub_sample/042_030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"wget\" is not set to runAsNonRoot"
  },
  {
    "id": "124",
    "manifest_path": "data/eval/artifacthub_sample/042_030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "125",
    "manifest_path": "data/eval/artifacthub_sample/042_030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "126",
    "manifest_path": "data/eval/artifacthub_sample/051_043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "127",
    "manifest_path": "data/eval/artifacthub_sample/051_043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      serviceAccountName: argocd-applicationset-controller\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"argocd-applicationset-controller\" not found"
  },
  {
    "id": "128",
    "manifest_path": "data/eval/artifacthub_sample/051_043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      serviceAccountName: argocd-applicationset-controller\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"applicationset-controller\" has cpu request 0"
  },
  {
    "id": "129",
    "manifest_path": "data/eval/artifacthub_sample/051_043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      serviceAccountName: argocd-applicationset-controller\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"applicationset-controller\" has memory limit 0"
  },
  {
    "id": "130",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "131",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "host-ports",
    "violation_text": "container exposes hostPort"
  },
  {
    "id": "132",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "133",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-operator\" does not expose port 9234 for the HTTPGet"
  },
  {
    "id": "134",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-operator\" does not have a read-only root file system"
  },
  {
    "id": "135",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium-operator\" not found"
  },
  {
    "id": "136",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cilium-operator\" does not expose port 9234 for the HTTPGet"
  },
  {
    "id": "137",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "138",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-operator\" has cpu request 0"
  },
  {
    "id": "139",
    "manifest_path": "data/eval/artifacthub_sample/055_023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n      serviceAccountName: cilium-operator\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-operator\" has memory limit 0"
  },
  {
    "id": "140",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "141",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "142",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop-net-raw-capability",
    "violation_text": "container \"cilium-agent\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "143",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "144",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-agent\" does not expose port 9879 for the HTTPGet"
  },
  {
    "id": "145",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"apply-sysctl-overwrites\" does not have a read-only root file system"
  },
  {
    "id": "146",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "147",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "148",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"config\" does not have a read-only root file system"
  },
  {
    "id": "149",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"install-cni-binaries\" does not have a read-only root file system"
  },
  {
    "id": "150",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mount-bpf-fs\" does not have a read-only root file system"
  },
  {
    "id": "151",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mount-cgroup\" does not have a read-only root file system"
  },
  {
    "id": "152",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium\" not found"
  },
  {
    "id": "153",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"apply-sysctl-overwrites\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "154",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "155",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "156",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"mount-bpf-fs\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "157",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"mount-cgroup\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "158",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"mount-bpf-fs\" is privileged"
  },
  {
    "id": "159",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cilium-agent\" does not expose port 9879 for the HTTPGet"
  },
  {
    "id": "160",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"apply-sysctl-overwrites\" is not set to runAsNonRoot"
  },
  {
    "id": "161",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "162",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "163",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"config\" is not set to runAsNonRoot"
  },
  {
    "id": "164",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"install-cni-binaries\" is not set to runAsNonRoot"
  },
  {
    "id": "165",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mount-bpf-fs\" is not set to runAsNonRoot"
  },
  {
    "id": "166",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mount-cgroup\" is not set to runAsNonRoot"
  },
  {
    "id": "167",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"apply-sysctl-overwrites\""
  },
  {
    "id": "168",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "sensitive-host-mounts",
    "violation_text": "host system directory \"/proc\" is mounted on container \"mount-cgroup\""
  },
  {
    "id": "169",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "startup-port",
    "violation_text": "container \"cilium-agent\" does not expose port 9879 for the HTTPGet"
  },
  {
    "id": "170",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"apply-sysctl-overwrites\" has cpu request 0"
  },
  {
    "id": "171",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "172",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "173",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"config\" has cpu request 0"
  },
  {
    "id": "174",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mount-bpf-fs\" has cpu request 0"
  },
  {
    "id": "175",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mount-cgroup\" has cpu request 0"
  },
  {
    "id": "176",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"apply-sysctl-overwrites\" has memory limit 0"
  },
  {
    "id": "177",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "178",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "179",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"config\" has memory limit 0"
  },
  {
    "id": "180",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"install-cni-binaries\" has memory limit 0"
  },
  {
    "id": "181",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mount-bpf-fs\" has memory limit 0"
  },
  {
    "id": "182",
    "manifest_path": "data/eval/artifacthub_sample/056_021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      serviceAccountName: cilium\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mount-cgroup\" has memory limit 0"
  },
  {
    "id": "183",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "184",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "host-ports",
    "violation_text": "container exposes hostPort"
  },
  {
    "id": "185",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "hostpath-volume",
    "violation_text": "volume uses hostPath"
  },
  {
    "id": "186",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "host-network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "187",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "liveness-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "188",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-envoy\" does not have a read-only root file system"
  },
  {
    "id": "189",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"cilium-envoy\" not found"
  },
  {
    "id": "190",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-envoy\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "191",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "readiness-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "192",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-envoy\" is not set to runAsNonRoot"
  },
  {
    "id": "193",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "startup-port",
    "violation_text": "container \"cilium-envoy\" does not expose port 9878 for the HTTPGet"
  },
  {
    "id": "194",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-envoy\" has cpu request 0"
  },
  {
    "id": "195",
    "manifest_path": "data/eval/artifacthub_sample/057_022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  template:\n    metadata:\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      serviceAccountName: cilium-envoy\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-envoy\" has memory limit 0"
  },
  {
    "id": "196",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "197",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "198",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "199",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tracker\" does not have a read-only root file system"
  },
  {
    "id": "200",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "201",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tracker\" is not set to runAsNonRoot"
  },
  {
    "id": "202",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "203",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tracker\" has cpu request 0"
  },
  {
    "id": "204",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "205",
    "manifest_path": "data/eval/artifacthub_sample/061_018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: tracker\n          image: artifacthub/tracker:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: tracker-config\n            mountPath: /home/tracker/.cfg\n            readOnly: true\n        volumes:\n        - name: tracker-config\n          secret:\n            secretName: tracker-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tracker\" has memory limit 0"
  },
  {
    "id": "206",
    "manifest_path": "data/eval/artifacthub_sample/062_015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "207",
    "manifest_path": "data/eval/artifacthub_sample/062_015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"release-name-postgresql\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "208",
    "manifest_path": "data/eval/artifacthub_sample/062_015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-chmod-data\" does not have a read-only root file system"
  },
  {
    "id": "209",
    "manifest_path": "data/eval/artifacthub_sample/062_015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"release-name-postgresql\" does not have a read-only root file system"
  },
  {
    "id": "210",
    "manifest_path": "data/eval/artifacthub_sample/062_015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-chmod-data\" is not set to runAsNonRoot"
  },
  {
    "id": "211",
    "manifest_path": "data/eval/artifacthub_sample/062_015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-chmod-data\" has memory limit 0"
  },
  {
    "id": "212",
    "manifest_path": "data/eval/artifacthub_sample/062_015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"release-name-postgresql\" has memory limit 0"
  },
  {
    "id": "213",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "214",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "215",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-migrator-run\" does not have a read-only root file system"
  },
  {
    "id": "216",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "217",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hub\" does not have a read-only root file system"
  },
  {
    "id": "218",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"hub\" not found"
  },
  {
    "id": "219",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-migrator-run\" is not set to runAsNonRoot"
  },
  {
    "id": "220",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "221",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hub\" is not set to runAsNonRoot"
  },
  {
    "id": "222",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-migrator-run\" has cpu request 0"
  },
  {
    "id": "223",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "224",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hub\" has cpu request 0"
  },
  {
    "id": "225",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-migrator-run\" has memory limit 0"
  },
  {
    "id": "226",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "227",
    "manifest_path": "data/eval/artifacthub_sample/064_013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hub\" has memory limit 0"
  },
  {
    "id": "228",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "229",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "job-ttl-seconds-after-finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "230",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "231",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "232",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"db-migrator\" does not have a read-only root file system"
  },
  {
    "id": "233",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "234",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"db-migrator\" is not set to runAsNonRoot"
  },
  {
    "id": "235",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "236",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"db-migrator\" has cpu request 0"
  },
  {
    "id": "237",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "238",
    "manifest_path": "data/eval/artifacthub_sample/065_016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"db-migrator\" has memory limit 0"
  },
  {
    "id": "239",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "240",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "241",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "242",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"scanner\" does not have a read-only root file system"
  },
  {
    "id": "243",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "244",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"scanner\" is not set to runAsNonRoot"
  },
  {
    "id": "245",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "246",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"scanner\" has cpu request 0"
  },
  {
    "id": "247",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "248",
    "manifest_path": "data/eval/artifacthub_sample/066_017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          helm.sh/chart: artifact-hub-1.21.0\n          app.kubernetes.io/name: artifact-hub\n          app.kubernetes.io/instance: release-name\n          app.kubernetes.io/version: 1.21.0\n          app.kubernetes.io/managed-by: Helm\n      spec:\n        serviceAccountName: default\n        initContainers:\n        - name: check-db-ready\n          image: docker.io/artifacthub/postgres:latest\n          imagePullPolicy: IfNotPresent\n          env:\n          - name: PGHOST\n            value: release-name-postgresql.default\n          - name: PGPORT\n            value: '5432'\n          - name: PGUSER\n            value: postgres\n          command:\n          - sh\n          - -c\n          - until pg_isready; do echo waiting for database; sleep 2; done;\n        containers:\n        - name: scanner\n          image: artifacthub/scanner:v1.21.0\n          imagePullPolicy: IfNotPresent\n          volumeMounts:\n          - name: scanner-config\n            mountPath: /home/scanner/.cfg\n            readOnly: true\n        volumes:\n        - name: scanner-config\n          secret:\n            secretName: scanner-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"scanner\" has memory limit 0"
  },
  {
    "id": "249",
    "manifest_path": "data/eval/artifacthub_sample/069_020_deployment_release-name-kubernetes-dashboard-api.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  annotations: null\n  name: release-name-kubernetes-dashboard-api\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-api\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-api\n        app.kubernetes.io/version: 1.13.0\n        app.kubernetes.io/component: api\n      annotations:\n        checksum/config: 204c1765e58ad5edfbba4d9e2caec1985db487314063c390ec4bb1957d34bc3d\n    spec:\n      containers:\n      - name: kubernetes-dashboard-api\n        image: docker.io/kubernetesui/dashboard-api:1.13.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --metrics-scraper-service-name=release-name-kubernetes-dashboard-metrics-scraper\n        env:\n        - name: CSRF_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kubernetes-dashboard-csrf\n              key: private.key\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: api\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-api\n",
    "policy_id": "cap-sys-admin",
    "violation_text": "container capabilities must drop SYS_ADMIN"
  },
  {
    "id": "250",
    "manifest_path": "data/eval/artifacthub_sample/069_020_deployment_release-name-kubernetes-dashboard-api.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  name: release-name-kubernetes-dashboard-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-api\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-api\n        app.kubernetes.io/version: 1.13.0\n        app.kubernetes.io/component: api\n      annotations:\n        checksum/config: 204c1765e58ad5edfbba4d9e2caec1985db487314063c390ec4bb1957d34bc3d\n    spec:\n      containers:\n      - name: kubernetes-dashboard-api\n        image: docker.io/kubernetesui/dashboard-api:1.13.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --metrics-scraper-service-name=release-name-kubernetes-dashboard-metrics-scraper\n        env:\n        - name: CSRF_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kubernetes-dashboard-csrf\n              key: private.key\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: api\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-api\n",
    "policy_id": "non-existent-service-account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-api\" not found"
  }
]