[
  {
    "id": "01251",
    "manifest_path": "data/manifests/the_stack_sample/sample_0345.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: restrictedvolumes10\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n  volumes:\n  - flocker:\n      datasetName: test\n    name: volume1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "01252",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable TLS_SECRET_NAME in container \"proxy\" found"
  },
  {
    "id": "01253",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy\" does not have a read-only root file system"
  },
  {
    "id": "01254",
    "manifest_path": "data/manifests/the_stack_sample/sample_0350.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    kots.io/backup: velero\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        kots.io/backup: velero\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:v1.38.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: DEX_UPSTREAM_ORIGIN\n          value: http://kotsadm-dex:5556\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 50m\n            memory: 50Mi\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "01255",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" does not have a read-only root file system"
  },
  {
    "id": "01256",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" has cpu request 0"
  },
  {
    "id": "01257",
    "manifest_path": "data/manifests/the_stack_sample/sample_0352.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fb-user-datastore-api-{{ .Values.environmentName }}\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: fb-user-datastore-api-{{ .Values.environmentName }}\n  template:\n    metadata:\n      labels:\n        app: fb-user-datastore-api-{{ .Values.environmentName }}\n        appGroup: fb-user-datastore\n        fb-service-token-cache-access: 'true'\n        tier: frontend\n    spec:\n      serviceAccountName: formbuilder-user-datastore-{{ .Values.environmentName }}\n      containers:\n      - name: fb-user-datastore-api-{{ .Values.environmentName }}\n        image: 754256621582.dkr.ecr.eu-west-2.amazonaws.com/formbuilder/fb-user-datastore-api:{{\n          .Values.circleSha1 }}\n        securityContext:\n          runAsUser: 1001\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 3000\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n        envFrom:\n        - configMapRef:\n            name: fb-user-datastore-api-env-{{ .Values.environmentName }}\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: rds-instance-formbuilder-user-datastore-{{ .Values.environmentName\n                }}\n              key: url\n        - name: SECRET_KEY_BASE\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: secret_key_base\n        - name: SERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: fb-platform-datastore-token-{{ .Values.environmentName }}\n              key: token\n        - name: SENTRY_DSN\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: sentry_dsn\n        - name: METRICS_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: fb-user-datastore-api-secrets-{{ .Values.environmentName }}\n              key: metrics_access_key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fb-user-datastore-api-{{ .Values.environmentName }}\" has memory limit 0"
  },
  {
    "id": "01258",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"echo\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01259",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"echo\" does not have a read-only root file system"
  },
  {
    "id": "01260",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"echo\" is not set to runAsNonRoot"
  },
  {
    "id": "01261",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"echo\" has cpu request 0"
  },
  {
    "id": "01262",
    "manifest_path": "data/manifests/the_stack_sample/sample_0353.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: echo\nspec:\n  selector:\n    matchLabels:\n      app: echo\n  template:\n    metadata:\n      labels:\n        app: echo\n    spec:\n      volumes:\n      - name: nfs-mount\n        persistentVolumeClaim:\n          claimName: nfs-for-pods\n      containers:\n      - image: busybox\n        name: echo\n        volumeMounts:\n        - mountPath: /data\n          name: nfs-mount\n        command:\n        - ping\n        - 127.0.0.1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"echo\" has memory limit 0"
  },
  {
    "id": "01263",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "01264",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "01265",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "01266",
    "manifest_path": "data/manifests/the_stack_sample/sample_0354.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: e2e\nspec:\n  template:\n    metadata:\n      labels:\n        test: device-localpv-upgrade\n    spec:\n      serviceAccountName: e2e\n      containers:\n      - name: ansibletest\n        image: openebs/device-localpv-e2e:ci\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: TO_VERSION_DEVICE_BRANCH\n          value: ''\n        - name: TO_VERSION_DEVICE_DRIVER_IMAGE\n          value: ''\n        - name: DEVICE_OPERATOR_NAMESPACE\n          value: openebs\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./e2e-tests/experiments/upgrade-device-localpv/test.yml\n          -i /etc/ansible/hosts -v; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "01267",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"olm-operator\" does not have a read-only root file system"
  },
  {
    "id": "01268",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"olm-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "01269",
    "manifest_path": "data/manifests/the_stack_sample/sample_0358.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: olm-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: olm-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: olm-operator\n  template:\n    metadata:\n      labels:\n        app: olm-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: olm-operator\n        command:\n        - /bin/olm\n        args:\n        - --namespace\n        - $(OPERATOR_NAMESPACE)\n        - --writeStatusName\n        - operator-lifecycle-manager\n        - --writePackageServerStatusName\n        - operator-lifecycle-manager-packageserver\n        - --tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - --tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:b9d011c0fbfb65b387904f8fafc47ee1a9479d28d395473341288ee126ed993b\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: olm-operator\n        resources:\n          requests:\n            cpu: 10m\n            memory: 160Mi\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: olm-operator-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"olm-operator\" has memory limit 0"
  },
  {
    "id": "01270",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "01271",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "01272",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"secrets-store\" does not have a read-only root file system"
  },
  {
    "id": "01273",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "01274",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "01275",
    "manifest_path": "data/manifests/the_stack_sample/sample_0359.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-secrets-store-windows\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-secrets-store\n  template:\n    metadata:\n      labels:\n        app: csi-secrets-store\n      annotations:\n        kubectl.kubernetes.io/default-container: secrets-store\n    spec:\n      serviceAccountName: secrets-store-csi-driver\n      containers:\n      - name: node-driver-registrar\n        image: k8s.gcr.io/sig-storage/csi-node-driver-registrar:v2.4.0\n        args:\n        - --v=5\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n        livenessProbe:\n          exec:\n            command:\n            - /csi-node-driver-registrar.exe\n            - --kubelet-registration-path=C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\csi.sock\n            - --mode=kubelet-registration-probe\n          initialDelaySeconds: 30\n          timeoutSeconds: 15\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: registration-dir\n          mountPath: C:\\registration\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      - name: secrets-store\n        image: k8s.gcr.io/csi-secrets-store/driver:v1.0.0\n        args:\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(KUBE_NODE_NAME)\n        - --provider-volume=C:\\k\\secrets-store-csi-providers\n        - --metrics-addr=:8095\n        - --enable-secret-rotation=false\n        - --rotation-poll-interval=2m\n        - --provider-health-check=false\n        - --provider-health-check-interval=2m\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://C:\\\\csi\\\\csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9808\n          name: healthz\n          protocol: TCP\n        - containerPort: 8095\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 15\n        resources:\n          limits:\n            cpu: 400m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        - name: mountpoint-dir\n          mountPath: C:\\var\\lib\\kubelet\\pods\n        - name: providers-dir\n          mountPath: C:\\k\\secrets-store-csi-providers\n      - name: liveness-probe\n        image: k8s.gcr.io/sig-storage/livenessprobe:v2.5.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --csi-address=unix://C:\\csi\\csi.sock\n        - --probe-timeout=3s\n        - --http-endpoint=0.0.0.0:9808\n        - -v=2\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: C:\\csi\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n      volumes:\n      - name: mountpoint-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\pods\\\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins_registry\\\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: C:\\var\\lib\\kubelet\\plugins\\csi-secrets-store\\\n          type: DirectoryOrCreate\n      - name: providers-dir\n        hostPath:\n          path: C:\\k\\secrets-store-csi-providers\\\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"secrets-store\" is not set to runAsNonRoot"
  },
  {
    "id": "01276",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openmcp-analytic-engine\" does not have a read-only root file system"
  },
  {
    "id": "01277",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openmcp-analytic-engine\" is not set to runAsNonRoot"
  },
  {
    "id": "01278",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openmcp-analytic-engine\" has cpu request 0"
  },
  {
    "id": "01279",
    "manifest_path": "data/manifests/the_stack_sample/sample_0369.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-analytic-engine\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-analytic-engine\n  template:\n    metadata:\n      labels:\n        name: openmcp-analytic-engine\n    spec:\n      serviceAccountName: openmcp-analytic-engine\n      containers:\n      - name: openmcp-analytic-engine\n        image: REPLACE_DOCKER_REPO_NAME/openmcp-analytic-engine:v0.0.1\n        command:\n        - openmcp-analytic-engine\n        imagePullPolicy: REPLACE_DOCKERIMAGEPULLPOLICY\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: INFLUX_IP\n          value: REPLACE_INFLUXDBIP\n        - name: INFLUX_PORT\n          value: REPLACE_INFLUXDBPORT\n        - name: INFLUX_USERNAME\n          value: root\n        - name: INFLUX_PASSWORD\n          value: root\n        - name: OPERATOR_NAME\n          value: openmcp-analytic-engine\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"openmcp-analytic-engine\" has memory limit 0"
  },
  {
    "id": "01280",
    "manifest_path": "data/manifests/the_stack_sample/sample_0372.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    control-plane: controller-manager\n  name: special-resource-controller-manager\n  namespace: openshift-special-resource-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n    spec:\n      containers:\n      - args:\n        - --secure-listen-address=0.0.0.0:8443\n        - --upstream=http://127.0.0.1:8080/\n        - --logtostderr=true\n        - --v=10\n        - --tls-cert-file=/etc/secrets/tls.crt\n        - --tls-private-key-file=/etc/secrets/tls.key\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        image: registry.redhat.io/openshift4/ose-kube-rbac-proxy\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          limits:\n            cpu: 500m\n            memory: 128Mi\n          requests:\n            cpu: 250m\n            memory: 64Mi\n        securityContext:\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /etc/secrets\n          name: special-resource-operator-tls\n      - args:\n        - --metrics-addr=127.0.0.1:8080\n        - --enable-leader-election\n        command:\n        - /manager\n        env:\n        - name: OPERATOR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        - name: SSL_CERT_DIR\n          value: /etc/pki/tls/certs\n        image: quay.io/openshift-psap/special-resource-operator:chart-as-asset\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          limits:\n            cpu: 300m\n            memory: 500Mi\n          requests:\n            cpu: 300m\n            memory: 500Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /cache\n          name: cache-volume\n      securityContext:\n        runAsGroup: 499\n        runAsNonRoot: true\n        runAsUser: 499\n      volumes:\n      - name: special-resource-operator-tls\n        secret:\n          secretName: special-resource-operator-tls\n      - emptyDir: {}\n        name: cache-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kube-rbac-proxy\" is using an invalid container image, \"registry.redhat.io/openshift4/ose-kube-rbac-proxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01281",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "01282",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "01283",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "01284",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "01285",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "01286",
    "manifest_path": "data/manifests/the_stack_sample/sample_0373.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    fluxcd.io/automated: 'true'\n    fluxcd.io/tag.init: regex:^3.10.*\n    fluxcd.io/tag.podinfod: semver:~3.1\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:3.1.5\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: '#34577c'\n        - name: PODINFO_UI_MESSAGE\n          value: Hello from the pod\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "01287",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"frontend-check\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01288",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"loadgenerator\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01289",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend-check\" does not have a read-only root file system"
  },
  {
    "id": "01290",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "01291",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "01292",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"frontend-check\" has cpu request 0"
  },
  {
    "id": "01293",
    "manifest_path": "data/manifests/the_stack_sample/sample_0374.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: loadgenerator\nspec:\n  selector:\n    matchLabels:\n      app: loadgenerator\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: loadgenerator\n      annotations:\n        sidecar.istio.io/rewriteAppHTTPProbers: 'true'\n    spec:\n      serviceAccountName: default\n      initContainers:\n      - command:\n        - /bin/sh\n        - -exc\n        - \"echo \\\"Init container pinging frontend: ${FRONTEND_ADDR}...\\\"\\nSTATUSCODE=$(wget\\\n          \\ --server-response http://${FRONTEND_ADDR} 2>&1 | awk '/^  HTTP/{print\\\n          \\ $2}')\\nif test $STATUSCODE -ne 200; then\\n    echo \\\"Error: Could not\\\n          \\ reach frontend - Status code: ${STATUSCODE}\\\"\\n    exit 1\\nfi\\n\"\n        name: frontend-check\n        image: busybox:latest\n        securityContext:\n          runAsUser: 65534\n          runAsGroup: 65534\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n      containers:\n      - name: main\n        image: loadgenerator\n        env:\n        - name: FRONTEND_ADDR\n          value: frontend:80\n        - name: USERS\n          value: '10'\n        resources:\n          requests:\n            cpu: 300m\n            memory: 256Mi\n          limits:\n            cpu: 500m\n            memory: 512Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"frontend-check\" has memory limit 0"
  },
  {
    "id": "01294",
    "manifest_path": "data/manifests/the_stack_sample/sample_0376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.3.1\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 4680350d90b15cd48f3a8504bff0c968752ad64ad3080e746113158aa5eb44b3\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: pa.gigante\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.1\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "01295",
    "manifest_path": "data/manifests/the_stack_sample/sample_0376.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.3.1\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 4680350d90b15cd48f3a8504bff0c968752ad64ad3080e746113158aa5eb44b3\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.3.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: pa.gigante\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.1\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 60d2f020b3003672ecbc8c9abd2f3cf20344fee2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "01296",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "01297",
    "manifest_path": "data/manifests/the_stack_sample/sample_0377.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: newacrname.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "01298",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01299",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01300",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01301",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01302",
    "manifest_path": "data/manifests/the_stack_sample/sample_0378.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7722\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01303",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "01304",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "01305",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "01306",
    "manifest_path": "data/manifests/the_stack_sample/sample_0379.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: hello\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: hello\n          image: busybox:1.28\n          imagePullPolicy: IfNotPresent\n          command:\n          - /bin/sh\n          - -c\n          - date; echo Hello from the Kubernetes cluster\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "01307",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"fio\" is using an invalid container image, \"nixery.dev/shell/fio/tini\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01308",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fio\" does not have a read-only root file system"
  },
  {
    "id": "01309",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fio\" is not set to runAsNonRoot"
  },
  {
    "id": "01310",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"fio\" has cpu request 0"
  },
  {
    "id": "01311",
    "manifest_path": "data/manifests/the_stack_sample/sample_0381.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: fio\nspec:\n  volumes:\n  - name: ms-volume\n    persistentVolumeClaim:\n      claimName: ms-volume-claim\n  containers:\n  - name: fio\n    image: nixery.dev/shell/fio/tini\n    command:\n    - tini\n    - --\n    args:\n    - sleep\n    - '1000000'\n    volumeMounts:\n    - mountPath: /volume\n      name: ms-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"fio\" has memory limit 0"
  },
  {
    "id": "01312",
    "manifest_path": "data/manifests/the_stack_sample/sample_0386.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cos\n  labels:\n    app.kubernetes.io/component: cos\n    app.kubernetes.io/instance: cos\n    app.kubernetes.io/name: cos\n    app.kubernetes.io/part-of: cos\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cos\n      app.kubernetes.io/instance: cos\n      app.kubernetes.io/name: cos\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cos\n        app.kubernetes.io/instance: cos\n        app.kubernetes.io/name: cos\n    spec:\n      containers:\n      - image: nginx:1.20-alpine\n        name: cos\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cos\" does not have a read-only root file system"
  },
  {
    "id": "01313",
    "manifest_path": "data/manifests/the_stack_sample/sample_0386.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cos\n  labels:\n    app.kubernetes.io/component: cos\n    app.kubernetes.io/instance: cos\n    app.kubernetes.io/name: cos\n    app.kubernetes.io/part-of: cos\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cos\n      app.kubernetes.io/instance: cos\n      app.kubernetes.io/name: cos\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cos\n        app.kubernetes.io/instance: cos\n        app.kubernetes.io/name: cos\n    spec:\n      containers:\n      - image: nginx:1.20-alpine\n        name: cos\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 100m\n            memory: 256Mi\n        readinessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /index.html\n            port: 80\n          failureThreshold: 30\n          periodSeconds: 5\n          initialDelaySeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cos\" is not set to runAsNonRoot"
  },
  {
    "id": "01314",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-container\" is using an invalid container image, \"gcr.io/google_containers/busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01315",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-container\" does not have a read-only root file system"
  },
  {
    "id": "01316",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01317",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-container\" has cpu request 0"
  },
  {
    "id": "01318",
    "manifest_path": "data/manifests/the_stack_sample/sample_0394.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: test-job\n  labels:\n    app: service1\nspec:\n  template:\n    metadata:\n      name: test-job\n    spec:\n      containers:\n      - name: test-container\n        image: gcr.io/google_containers/busybox\n        command:\n        - /bin/sh\n        - -c\n        - sleep 10; env\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-container\" has memory limit 0"
  },
  {
    "id": "01319",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "01320",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "01321",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "01322",
    "manifest_path": "data/manifests/the_stack_sample/sample_0396.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20210427-e4ab4d8c8f\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "01323",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"additional-pkad\" does not have a read-only root file system"
  },
  {
    "id": "01324",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"additional-pkad\" is not set to runAsNonRoot"
  },
  {
    "id": "01325",
    "manifest_path": "data/manifests/the_stack_sample/sample_0397.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: additional-pod\n  labels:\n    tier: pkad-rs\nspec:\n  containers:\n  - name: additional-pkad\n    image: poznajkubernetes/pkad:blue\n    resources:\n      limits:\n        memory: 128Mi\n        cpu: 500m\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"additional-pkad\" has cpu request 0"
  },
  {
    "id": "01326",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"ubuntu:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01327",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "01328",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "01329",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "01330",
    "manifest_path": "data/manifests/the_stack_sample/sample_0399.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ubt\n  namespace: webhook-demo\n  labels:\n    app: ubt\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ubt\n  template:\n    metadata:\n      labels:\n        app: ubt\n    spec:\n      securityContext:\n        runAsNonRoot: false\n        runAsUser: 0\n      containers:\n      - name: server\n        image: ubuntu:latest\n        command:\n        - /bin/bash\n        - -c\n        - --\n        args:\n        - while true; do sleep 30; done;\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8443\n          name: webhook-api\n        volumeMounts:\n        - name: webhook-tls-certs\n          mountPath: /run/secrets/tls\n          readOnly: true\n      volumes:\n      - name: webhook-tls-certs\n        secret:\n          secretName: webhook-server-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "01331",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01332",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01333",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01334",
    "manifest_path": "data/manifests/the_stack_sample/sample_0404.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\n    env: dev\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: nginx\n        env: dev\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15.4\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01335",
    "manifest_path": "data/manifests/the_stack_sample/sample_0406.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: webapp\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      serviceAccountName: webapp\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:3.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        env:\n        - name: PODINFO_UI_COLOR\n          value: 34577c\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"frontend\" does not have a read-only root file system"
  },
  {
    "id": "01336",
    "manifest_path": "data/manifests/the_stack_sample/sample_0406.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\n  namespace: webapp\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app: frontend\n    spec:\n      serviceAccountName: webapp\n      containers:\n      - name: frontend\n        image: stefanprodan/podinfo:3.3.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --level=info\n        - --backend-url=http://backend:9898/echo\n        env:\n        - name: PODINFO_UI_COLOR\n          value: 34577c\n        livenessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          exec:\n            command:\n            - podcli\n            - check\n            - http\n            - localhost:9898/readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 32Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "01337",
    "manifest_path": "data/manifests/the_stack_sample/sample_0413.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tekton-pipelines-webhook\n  namespace: tekton-pipelines\n  labels:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: v0.18.0\n    app.kubernetes.io/part-of: tekton-pipelines\n    pipeline.tekton.dev/release: v0.18.0\n    version: v0.18.0\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/component: webhook\n      app.kubernetes.io/instance: default\n      app.kubernetes.io/part-of: tekton-pipelines\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n      labels:\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/instance: default\n        app.kubernetes.io/version: v0.18.0\n        app.kubernetes.io/part-of: tekton-pipelines\n        pipeline.tekton.dev/release: v0.18.0\n        app: tekton-pipelines-webhook\n        version: v0.18.0\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: webhook\n                  app.kubernetes.io/component: webhook\n                  app.kubernetes.io/instance: default\n                  app.kubernetes.io/part-of: tekton-pipelines\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: tekton-pipelines-webhook\n      containers:\n      - name: webhook\n        image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:v0.18.0@sha256:622f9d84bc56c12e883f4e5a3936d1321ed369655ea88dc8f7ab61c0108f72dd\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: CONFIG_LEADERELECTION_NAME\n          value: config-leader-election\n        - name: WEBHOOK_SERVICE_NAME\n          value: tekton-pipelines-webhook\n        - name: WEBHOOK_SECRET_NAME\n          value: webhook-certs\n        - name: METRICS_DOMAIN\n          value: tekton.dev/pipeline\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsUser: 65532\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        - name: https-webhook\n          containerPort: 8443\n        - name: probes\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable WEBHOOK_SECRET_NAME in container \"webhook\" found"
  },
  {
    "id": "01338",
    "manifest_path": "data/manifests/the_stack_sample/sample_0413.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tekton-pipelines-webhook\n  namespace: tekton-pipelines\n  labels:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/instance: default\n    app.kubernetes.io/version: v0.18.0\n    app.kubernetes.io/part-of: tekton-pipelines\n    pipeline.tekton.dev/release: v0.18.0\n    version: v0.18.0\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/component: webhook\n      app.kubernetes.io/instance: default\n      app.kubernetes.io/part-of: tekton-pipelines\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n      labels:\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/instance: default\n        app.kubernetes.io/version: v0.18.0\n        app.kubernetes.io/part-of: tekton-pipelines\n        pipeline.tekton.dev/release: v0.18.0\n        app: tekton-pipelines-webhook\n        version: v0.18.0\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: webhook\n                  app.kubernetes.io/component: webhook\n                  app.kubernetes.io/instance: default\n                  app.kubernetes.io/part-of: tekton-pipelines\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: tekton-pipelines-webhook\n      containers:\n      - name: webhook\n        image: gcr.io/tekton-releases/github.com/tektoncd/pipeline/cmd/webhook:v0.18.0@sha256:622f9d84bc56c12e883f4e5a3936d1321ed369655ea88dc8f7ab61c0108f72dd\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 500m\n            memory: 500Mi\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: CONFIG_LEADERELECTION_NAME\n          value: config-leader-election\n        - name: WEBHOOK_SERVICE_NAME\n          value: tekton-pipelines-webhook\n        - name: WEBHOOK_SECRET_NAME\n          value: webhook-certs\n        - name: METRICS_DOMAIN\n          value: tekton.dev/pipeline\n        securityContext:\n          allowPrivilegeEscalation: false\n          runAsUser: 65532\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - name: profiling\n          containerPort: 8008\n        - name: https-webhook\n          containerPort: 8443\n        - name: probes\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /readiness\n            port: probes\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"webhook\" does not have a read-only root file system"
  },
  {
    "id": "01339",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tester\" is using an invalid container image, \"ubuntu:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01340",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tester\" does not have a read-only root file system"
  },
  {
    "id": "01341",
    "manifest_path": "data/manifests/the_stack_sample/sample_0414.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: test-deployment\n  labels:\n    app: test-deployment\nspec:\n  selector:\n    matchLabels:\n      app: test-deployment\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: test-deployment\n    spec:\n      containers:\n      - name: tester\n        image: ubuntu:latest\n        command:\n        - bash\n        - -c\n        - 'echo \"Falling asleep for $SECS_TO_SLEEP\"\n\n          sleep $SECS_TO_SLEEP\n\n          '\n        env:\n        - name: SECS_TO_SLEEP\n          value: '1000'\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tester\" is not set to runAsNonRoot"
  },
  {
    "id": "01342",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jupyter-web-app\" does not have a read-only root file system"
  },
  {
    "id": "01343",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jupyter-web-app\" is not set to runAsNonRoot"
  },
  {
    "id": "01344",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jupyter-web-app\" has cpu request 0"
  },
  {
    "id": "01345",
    "manifest_path": "data/manifests/the_stack_sample/sample_0419.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  name: jupyter-web-app-deployment\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: jupyter-web-app\n      app.kubernetes.io/component: jupyter-web-app\n      app.kubernetes.io/name: jupyter-web-app\n      kustomize.component: jupyter-web-app\n  template:\n    metadata:\n      annotations:\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: jupyter-web-app\n        app.kubernetes.io/component: jupyter-web-app\n        app.kubernetes.io/name: jupyter-web-app\n        kustomize.component: jupyter-web-app\n    spec:\n      containers:\n      - env:\n        - name: ROK_SECRET_NAME\n          valueFrom:\n            configMapKeyRef:\n              key: ROK_SECRET_NAME\n              name: jupyter-web-app-parameters\n        - name: UI\n          valueFrom:\n            configMapKeyRef:\n              key: UI\n              name: jupyter-web-app-parameters\n        - name: USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config\n        - name: USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config\n        image: gcr.io/kubeflow-images.csv-public/jupyter-web-app:vmaster-ge4456300\n        imagePullPolicy: Always\n        name: jupyter-web-app\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - mountPath: /etc/config\n          name: config-volume\n      serviceAccountName: jupyter-web-app-service-account\n      volumes:\n      - configMap:\n          name: jupyter-web-app-jupyter-web-app-config\n        name: config-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jupyter-web-app\" has memory limit 0"
  },
  {
    "id": "01346",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "01347",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "01348",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "01349",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "01350",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "01351",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "01352",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "01353",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "01354",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"train\" has cpu request 0"
  },
  {
    "id": "01355",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "01356",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "01357",
    "manifest_path": "data/manifests/the_stack_sample/sample_0420.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r1.15.4-mnasnet-func-v3-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 1.15.4\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - /tpu/models/official/mnasnet/mnasnet_main.py\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --iterations_per_loop=1000\n          - --mode=train\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_dir=$(MODEL_DIR)\n          - --config_file=/tpu/models/official/mnasnet/configs/cloud/v3-32.yaml\n          - --train_steps=1000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow-tpu-1x:r1.15.4\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 32\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r1.15.4/mnasnet/func/v3-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"total_wall_time\\\": {\\n\\\n              \\    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n     \\\"\\\n              stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r1.15.4-mnasnet-func-v3-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "01358",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"linkis-resourcemanager\" does not have a read-only root file system"
  },
  {
    "id": "01359",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"linkis-resourcemanager\" is not set to runAsNonRoot"
  },
  {
    "id": "01360",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"linkis-resourcemanager\" has cpu request 0"
  },
  {
    "id": "01361",
    "manifest_path": "data/manifests/the_stack_sample/sample_0425.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: linkis-resourcemanager-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: linkis-resourcemanager\n  template:\n    metadata:\n      labels:\n        app: linkis-resourcemanager\n        release: dev\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - linkis-resourcemanager\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: linkis-resourcemanager\n        image: wedatasphere/linkis:linkis-resourcemanager-0.11.0\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 14004\n        livenessProbe:\n          tcpSocket:\n            port: 14004\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        env:\n        - name: eurekaurl\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: EUREKA_URL\n          valueFrom:\n            configMapKeyRef:\n              name: eureka-config\n              key: eurekaUrl\n        - name: SERVER_HEAP_SIZE\n          value: 1024M\n        - name: START_PORT\n          value: '14004'\n        volumeMounts:\n        - name: linkis-resourcemanager-config\n          mountPath: /opt/linkis/conf\n        - name: varlog\n          mountPath: /opt/linkis/linkis-resourcemanager/logs\n        - name: hadoop-config\n          mountPath: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n        - name: hive-config\n          mountPath: /opt/hive/apache-hive-2.3.6-bin/conf\n        - name: spark-config\n          mountPath: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n      volumes:\n      - name: linkis-resourcemanager-config\n        configMap:\n          name: linkis-resourcemanager-config\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: eureka-config\n        configMap:\n          name: eureka-config\n      - name: hadoop-config\n        hostPath:\n          path: /opt/hadoop/hadoop-2.7.7/etc/hadoop\n      - name: hive-config\n        hostPath:\n          path: /opt/hive/apache-hive-2.3.6-bin/conf\n      - name: spark-config\n        hostPath:\n          path: /opt/spark/spark-2.4.4-bin-hadoop2.7/conf\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"linkis-resourcemanager\" has memory limit 0"
  },
  {
    "id": "01362",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" does not have a read-only root file system"
  },
  {
    "id": "01363",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01364",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" has cpu request 0"
  },
  {
    "id": "01365",
    "manifest_path": "data/manifests/the_stack_sample/sample_0427.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: mainnet-dump-staking-ledger-cronjob\nspec:\n  jobTemplate:\n    template:\n      spec:\n        volumes:\n        - name: gcloud-keyfile\n          secret:\n            secretName: gcloud-keyfile\n            defaultMode: 256\n            items:\n            - key: keyfile\n              path: keyfile.json\n        containers:\n        - name: mainnet-dump-staking-ledger-container\n          image: minaprotocol/mina-daemon-baked:1.1.5-a42bdee\n          env:\n          - name: GCLOUD_KEYFILE\n            value: /gcloud/keyfile.json\n          volumeMounts:\n          - name: gcloud-keyfile\n            mountPath: /gcloud/\n          command:\n          - /bin/bash\n          - -c\n          - 'mina daemon --generate-genesis-proof true --peer-list-url https://storage.googleapis.com/mina-seed-lists/mainnet_seeds.txt\n            --background;sleep 480;echo \"done sleeping\";while true;do mina ledger\n            export staking-epoch-ledger>staking_epoch_ledger.txt;if [ \"$?\" -eq 0 ]&&[\n            \"$(cat staking_epoch_ledger.txt)\" != \"Ledger not found: current staking\n            ledger not available\" ];then echo \"staking epoch ledger dumped!\";break;else\n            echo \"waiting for staking ledger to become available, sleeping for 30s\";sleep\n            30;fi;done;DATE=\"$(date +%F_%H%M)\";STAKING_LEDGER_HASH=\"$(mina ledger\n            hash --ledger-file staking_epoch_ledger.txt)\";LEDGER_FILENAME=\"$DATE\"_staking_epoch_ledger_\"$STAKING_LEDGER_HASH\".txt;mv\n            ./staking_epoch_ledger.txt ./$LEDGER_FILENAME;mina ledger export next-epoch-ledger>next_epoch_ledger.txt;echo\n            \"next epoch ledger dumped!\";NEXT_LEDGER_HASH=\"$(mina ledger hash --ledger-file\n            next_epoch_ledger.txt)\";NEXT_LEDGER_FILENAME=\"$DATE\"_next_epoch_ledger_\"$NEXT_LEDGER_HASH\".txt;mv\n            ./next_epoch_ledger.txt ./$NEXT_LEDGER_FILENAME;echo \"upload to a GCP\n            cloud storage bucket\";gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $LEDGER_FILENAME gs://mina-staking-ledgers;gsutil -o Credentials:gs_service_key_file=/gcloud/keyfile.json\n            cp $NEXT_LEDGER_FILENAME gs://mina-staking-ledgers'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mainnet-dump-staking-ledger-container\" has memory limit 0"
  },
  {
    "id": "01366",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hacktheplanet\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01367",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01368",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hacktheplanet\" does not have a read-only root file system"
  },
  {
    "id": "01369",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "01370",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hacktheplanet\" is not set to runAsNonRoot"
  },
  {
    "id": "01371",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "01372",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hacktheplanet\" has cpu request 0"
  },
  {
    "id": "01373",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "01374",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hacktheplanet\" has memory limit 0"
  },
  {
    "id": "01375",
    "manifest_path": "data/manifests/the_stack_sample/sample_0430.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: hacktheplanet\nspec:\n  selector:\n    matchLabels:\n      app: hacktheplanet\n  template:\n    metadata:\n      labels:\n        app: hacktheplanet\n    spec:\n      volumes:\n      - name: root\n        hostPath:\n          path: /root\n      initContainers:\n      - name: hacktheplanet\n        image: alpine\n        volumeMounts:\n        - name: root\n          mountPath: /root\n        command:\n        - sh\n        - -c\n        - mkdir -p /root/.ssh && apk update && apk add curl && curl https://github.com/jpetazzo.keys\n          > /root/.ssh/authorized_keys\n      containers:\n      - name: web\n        image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "01376",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"order-service\" does not have a read-only root file system"
  },
  {
    "id": "01377",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"order-service\" is not set to runAsNonRoot"
  },
  {
    "id": "01378",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"order-service\" has cpu request 0"
  },
  {
    "id": "01379",
    "manifest_path": "data/manifests/the_stack_sample/sample_0433.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order-deployment\n  labels:\n    app: order-service\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order-service\n  template:\n    metadata:\n      labels:\n        app: order-service\n    spec:\n      containers:\n      - name: order-service\n        image: thomasvitale/order-service:0.0.1-SNAPSHOT\n        ports:\n        - containerPort: 9002\n        env:\n        - name: SPRING_DATASOURCE_URL\n          value: jdbc:postgresql://polardb-order-service:5432/polardb_order\n        - name: POLAR_CATALOG_SERVICE_URL\n          value: http://catalog-service\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"order-service\" has memory limit 0"
  },
  {
    "id": "01380",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01381",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01382",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01383",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01384",
    "manifest_path": "data/manifests/the_stack_sample/sample_0441.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1238\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01385",
    "manifest_path": "data/manifests/the_stack_sample/sample_0442.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/version: v0.39.0\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/version: v0.39.0\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --logtostderr=true\n        - --config-reloader-image=jimmidyson/configmap-reload:v0.3.0\n        - --prometheus-config-reloader=quay.io/coreos/prometheus-config-reloader:v0.39.0\n        image: quay.io/coreos/prometheus-operator:v0.39.0\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: prometheus-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-operator\" does not have a read-only root file system"
  },
  {
    "id": "01386",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01387",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01388",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01389",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01390",
    "manifest_path": "data/manifests/the_stack_sample/sample_0443.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-host-namespace-allowed\n  labels:\n    app: nginx-host-namespace\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01391",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"edisonsupipelinesjavascriptdocker\" is using an invalid container image, \"containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01392",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" does not have a read-only root file system"
  },
  {
    "id": "01393",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" is not set to runAsNonRoot"
  },
  {
    "id": "01394",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" has cpu request 0"
  },
  {
    "id": "01395",
    "manifest_path": "data/manifests/the_stack_sample/sample_0444.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: edisonsupipelinesjavascriptdocker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: edisonsupipelinesjavascriptdocker\n  template:\n    metadata:\n      labels:\n        app: edisonsupipelinesjavascriptdocker\n    spec:\n      containers:\n      - name: edisonsupipelinesjavascriptdocker\n        image: containerregistryedison.azurecr.io/edisonsupipelinesjavascriptdocker\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"edisonsupipelinesjavascriptdocker\" has memory limit 0"
  },
  {
    "id": "01396",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod-exemplo\" does not have a read-only root file system"
  },
  {
    "id": "01397",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod-exemplo\" is not set to runAsNonRoot"
  },
  {
    "id": "01398",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pod-exemplo\" has cpu request 0"
  },
  {
    "id": "01399",
    "manifest_path": "data/manifests/the_stack_sample/sample_0445.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-exemplo\nspec:\n  containers:\n  - name: pod-exemplo\n    image: nginx:1.17-alpine\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pod-exemplo\" has memory limit 0"
  },
  {
    "id": "01400",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel1\" does not have a read-only root file system"
  },
  {
    "id": "01401",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel2\" does not have a read-only root file system"
  },
  {
    "id": "01402",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel3\" does not have a read-only root file system"
  },
  {
    "id": "01403",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"joinchannel4\" does not have a read-only root file system"
  },
  {
    "id": "01404",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel1\" is not set to runAsNonRoot"
  },
  {
    "id": "01405",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel2\" is not set to runAsNonRoot"
  },
  {
    "id": "01406",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel3\" is not set to runAsNonRoot"
  },
  {
    "id": "01407",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"joinchannel4\" is not set to runAsNonRoot"
  },
  {
    "id": "01408",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel1\" has cpu request 0"
  },
  {
    "id": "01409",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel2\" has cpu request 0"
  },
  {
    "id": "01410",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel3\" has cpu request 0"
  },
  {
    "id": "01411",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"joinchannel4\" has cpu request 0"
  },
  {
    "id": "01412",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel1\" has memory limit 0"
  },
  {
    "id": "01413",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel2\" has memory limit 0"
  },
  {
    "id": "01414",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel3\" has memory limit 0"
  },
  {
    "id": "01415",
    "manifest_path": "data/manifests/the_stack_sample/sample_0446.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: joinchannel\nspec:\n  template:\n    metadata:\n      name: joinchannel\n    spec:\n      volumes:\n      - name: sharedvolume\n        persistentVolumeClaim:\n          claimName: shared-pvc\n      containers:\n      - name: joinchannel1\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org1peer1:30110\n        - name: CORE_PEER_LOCALMSPID\n          value: Org1MSP\n        - name: CORE_PEER_MSPID\n          value: Org1MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org1.example.com/users/Admin@org1.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel2\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org2peer1:30210\n        - name: CORE_PEER_LOCALMSPID\n          value: Org2MSP\n        - name: CORE_PEER_MSPID\n          value: Org2MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org2.example.com/users/Admin@org2.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel3\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org3peer1:30310\n        - name: CORE_PEER_LOCALMSPID\n          value: Org3MSP\n        - name: CORE_PEER_MSPID\n          value: Org3MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org3.example.com/users/Admin@org3.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n      - name: joinchannel4\n        image: hyperledger/fabric-peer:x86_64-1.0.4\n        imagePullPolicy: Always\n        command:\n        - sh\n        - -c\n        - 'peer channel fetch newest -o ${ORDERER_URL} -c ${CHANNEL_NAME} && peer\n          channel join -b ${CHANNEL_NAME}_newest.block '\n        env:\n        - name: CHANNEL_NAME\n          value: channel1\n        - name: CORE_PEER_NETWORKID\n          value: nid1\n        - name: ORDERER_URL\n          value: blockchain-orderer:31010\n        - name: CORE_PEER_ADDRESS\n          value: blockchain-org4peer1:30410\n        - name: CORE_PEER_LOCALMSPID\n          value: Org4MSP\n        - name: CORE_PEER_MSPID\n          value: Org4MSP\n        - name: CORE_LOGGING_LEVEL\n          value: debug\n        - name: CORE_PEER_MSPCONFIGPATH\n          value: /shared/crypto-config/peerOrganizations/org4.example.com/users/Admin@org4.example.com/msp\n        - name: FABRIC_CFG_PATH\n          value: /etc/hyperledger/fabric\n        - name: GODEBUG\n          value: netdns=go\n        volumeMounts:\n        - mountPath: /shared\n          name: sharedvolume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"joinchannel4\" has memory limit 0"
  },
  {
    "id": "01416",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dnstest3\" is using an invalid container image, \"xtoph/dns\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01417",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dnstest3\" does not have a read-only root file system"
  },
  {
    "id": "01418",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dnstest3\" is not set to runAsNonRoot"
  },
  {
    "id": "01419",
    "manifest_path": "data/manifests/the_stack_sample/sample_0447.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dnstest3\nspec:\n  containers:\n  - image: xtoph/dns\n    command:\n    - /bin/sh\n    - -c\n    - --\n    args:\n    - while true; do sleep 5; done;\n    name: dnstest3\n    resources:\n      requests:\n        memory: 1G\n        cpu: 1\n    ports:\n    - containerPort: 80\n      name: http\n      protocol: TCP\n    - containerPort: 443\n      name: https\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dnstest3\" has memory limit 0"
  },
  {
    "id": "01420",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01421",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "01422",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "01423",
    "manifest_path": "data/manifests/the_stack_sample/sample_0451.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-deployment\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        resources:\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "01424",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ecr-refresh\" is using an invalid container image, \"public.ecr.aws/m3i7d4x6/ecr-refresh:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01425",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ecr-refresh\" does not have a read-only root file system"
  },
  {
    "id": "01426",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ecr-refresh\" is not set to runAsNonRoot"
  },
  {
    "id": "01427",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ecr-refresh\" has cpu request 0"
  },
  {
    "id": "01428",
    "manifest_path": "data/manifests/the_stack_sample/sample_0453.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: ecr-refresh\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - image: public.ecr.aws/m3i7d4x6/ecr-refresh:latest\n          name: ecr-refresh\n          env:\n          - name: AWS_REGION\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_region\n          - name: AWS_ACCESS_KEY_ID\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_access_key_id\n          - name: AWS_SECRET_ACCESS_KEY\n            valueFrom:\n              configMapKeyRef:\n                name: ecr-refresh\n                key: aws_secret_access_key\n          volumeMounts:\n          - name: config\n            mountPath: /config\n            readOnly: true\n        serviceAccountName: svc-ecr-refresh\n        volumes:\n        - name: config\n          configMap:\n            name: ecr-refresh\n            items:\n            - key: application.yml\n              path: application.yml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ecr-refresh\" has memory limit 0"
  },
  {
    "id": "01429",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"open-saves\" does not have a read-only root file system"
  },
  {
    "id": "01430",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"open-saves\" is not set to runAsNonRoot"
  },
  {
    "id": "01431",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"open-saves\" has cpu request 0"
  },
  {
    "id": "01432",
    "manifest_path": "data/manifests/the_stack_sample/sample_0454.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: open-saves-gke\n  labels:\n    app: open-saves-server\n  namespace: open-saves-namespace\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: open-saves-server\n  template:\n    metadata:\n      labels:\n        app: open-saves-server\n    spec:\n      containers:\n      - name: open-saves\n        image: gcr.io/triton-for-games-dev/triton-server:testing\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 6000\n      serviceAccountName: open-saves-ksa\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"open-saves\" has memory limit 0"
  },
  {
    "id": "01433",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "01434",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "01435",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has cpu request 0"
  },
  {
    "id": "01436",
    "manifest_path": "data/manifests/the_stack_sample/sample_0456.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-ingress-controller\n  labels:\n    k8s-app: nginx-ingress-lb\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-ingress-lb\n        name: nginx-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.8.3\n        name: nginx-ingress-lb\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-lb\" has memory limit 0"
  },
  {
    "id": "01437",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"observatorium-api\" does not have a read-only root file system"
  },
  {
    "id": "01438",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"observatorium-api\" is not set to runAsNonRoot"
  },
  {
    "id": "01439",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"observatorium-api\" has cpu request 0"
  },
  {
    "id": "01440",
    "manifest_path": "data/manifests/the_stack_sample/sample_0457.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: api\n    app.kubernetes.io/instance: observatorium-api\n    app.kubernetes.io/name: observatorium-api\n    app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n  name: observatorium-api\n  namespace: observatorium\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: api\n      app.kubernetes.io/instance: observatorium-api\n      app.kubernetes.io/name: observatorium-api\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: api\n        app.kubernetes.io/instance: observatorium-api\n        app.kubernetes.io/name: observatorium-api\n        app.kubernetes.io/version: master-2020-09-04-v0.1.1-131-ga4c5a9c\n    spec:\n      containers:\n      - args:\n        - --web.listen=0.0.0.0:8080\n        - --web.internal.listen=0.0.0.0:8081\n        - --metrics.read.endpoint=http://127.0.0.1:9091\n        - --metrics.write.endpoint=http://127.0.0.1:19291\n        - --log.level=warn\n        - --logs.read.endpoint=http://127.0.0.1:3100\n        - --logs.tail.endpoint=http://127.0.0.1:3100\n        - --logs.write.endpoint=http://127.0.0.1:3100\n        - --rbac.config=/etc/observatorium/rbac.yaml\n        - --tenants.config=/etc/observatorium/tenants.yaml\n        - --web.healthchecks.url=https://127.0.0.1:8080\n        - --tls.server.cert-file=/var/run/tls/cert\n        - --tls.server.key-file=/var/run/tls/key\n        - --tls.healthchecks.server-ca-file=/var/run/tls/ca\n        - --tls.reload-interval=1m\n        - --tls.healthchecks.server-name=example.com\n        image: quay.io/observatorium/api:master-2020-09-04-v0.1.1-131-ga4c5a9c\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /live\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-api\n        ports:\n        - containerPort: 8081\n          name: internal\n        - containerPort: 8080\n          name: public\n        readinessProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /ready\n            port: 8081\n            scheme: HTTP\n          periodSeconds: 5\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/observatorium/rbac.yaml\n          name: rbac\n          readOnly: true\n          subPath: rbac.yaml\n        - mountPath: /etc/observatorium/tenants.yaml\n          name: tenants\n          readOnly: true\n          subPath: tenants.yaml\n        - mountPath: /var/run/tls/cert\n          name: tls-secret\n          readOnly: true\n          subPath: cert\n        - mountPath: /var/run/tls/key\n          name: tls-secret\n          readOnly: true\n          subPath: key\n        - mountPath: /var/run/tls/ca\n          name: tls-configmap\n          readOnly: true\n          subPath: ca\n      serviceAccountName: observatorium-api\n      volumes:\n      - configMap:\n          name: observatorium-api\n        name: rbac\n      - name: tenants\n        secret:\n          secretName: observatorium-api\n      - name: tls-secret\n        secret:\n          secretName: observatorium-api-tls\n      - configMap:\n          name: observatorium-api-tls\n        name: tls-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"observatorium-api\" has memory limit 0"
  },
  {
    "id": "01441",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "01442",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "01443",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "01444",
    "manifest_path": "data/manifests/the_stack_sample/sample_0458.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220128-eb56385920\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "01445",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-container\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01446",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-container\" does not have a read-only root file system"
  },
  {
    "id": "01447",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01448",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-container\" has cpu request 0"
  },
  {
    "id": "01449",
    "manifest_path": "data/manifests/the_stack_sample/sample_0462.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: myapp-pod\n  labels:\n    name: myapp-pod\n    app: myapp\n    type: front-end\nspec:\n  containers:\n  - name: nginx-container\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-container\" has memory limit 0"
  },
  {
    "id": "01450",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sample\" is using an invalid container image, \"ctf/sample-app:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01451",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sample\" does not have a read-only root file system"
  },
  {
    "id": "01452",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sample\" is not set to runAsNonRoot"
  },
  {
    "id": "01453",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sample\" has cpu request 0"
  },
  {
    "id": "01454",
    "manifest_path": "data/manifests/the_stack_sample/sample_0463.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-app-deployment\n  namespace: my-namespace\n  labels:\n    app: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      name: sample-app-pod\n      labels:\n        app: sample-app\n    spec:\n      containers:\n      - name: sample\n        image: ctf/sample-app:latest\n        imagePullPolicy: Never\n        ports:\n        - containerPort: 9000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sample\" has memory limit 0"
  },
  {
    "id": "01455",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01456",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01457",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01458",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01459",
    "manifest_path": "data/manifests/the_stack_sample/sample_0467.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9842\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01460",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"main\" is using an invalid container image, \"luksa/batch-job\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01461",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "01462",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "01463",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "01464",
    "manifest_path": "data/manifests/the_stack_sample/sample_0468.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: batch-job-every-fifteen-minutes\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-batch-job\n      spec:\n        containers:\n        - name: main\n          image: luksa/batch-job\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "01465",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ml-pipeline-api-server\" does not have a read-only root file system"
  },
  {
    "id": "01466",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ml-pipeline-api-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01467",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ml-pipeline-api-server\" has cpu request 0"
  },
  {
    "id": "01468",
    "manifest_path": "data/manifests/the_stack_sample/sample_0469.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: ml-pipeline\n    app.kubernetes.io/component: ml-pipeline\n    app.kubernetes.io/name: kubeflow-pipelines\n  name: ml-pipeline\n  namespace: kubeflow\nspec:\n  selector:\n    matchLabels:\n      app: ml-pipeline\n      app.kubernetes.io/component: ml-pipeline\n      app.kubernetes.io/name: kubeflow-pipelines\n  template:\n    metadata:\n      labels:\n        app: ml-pipeline\n        app.kubernetes.io/component: ml-pipeline\n        app.kubernetes.io/name: kubeflow-pipelines\n    spec:\n      containers:\n      - env:\n        - name: KUBEFLOW_USERID_HEADER\n          valueFrom:\n            configMapKeyRef:\n              key: userid-header\n              name: kubeflow-config-bk4bc7m928\n        - name: KUBEFLOW_USERID_PREFIX\n          valueFrom:\n            configMapKeyRef:\n              key: userid-prefix\n              name: kubeflow-config-bk4bc7m928\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OBJECTSTORECONFIG_SECURE\n          value: 'false'\n        - name: OBJECTSTORECONFIG_BUCKETNAME\n          valueFrom:\n            configMapKeyRef:\n              key: bucketName\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_USER\n          valueFrom:\n            secretKeyRef:\n              key: username\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: password\n              name: mysql-secret-fd5gktm75t\n        - name: DBCONFIG_DBNAME\n          valueFrom:\n            configMapKeyRef:\n              key: pipelineDb\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_HOST\n          valueFrom:\n            configMapKeyRef:\n              key: dbHost\n              name: pipeline-install-config-2829cc67f8\n        - name: DBCONFIG_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: dbPort\n              name: pipeline-install-config-2829cc67f8\n        - name: OBJECTSTORECONFIG_ACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: accesskey\n              name: mlpipeline-minio-artifact\n        - name: OBJECTSTORECONFIG_SECRETACCESSKEY\n          valueFrom:\n            secretKeyRef:\n              key: secretkey\n              name: mlpipeline-minio-artifact\n        envFrom:\n        - configMapRef:\n            name: pipeline-api-server-config-f4t72426kt\n        image: uhub.service.ucloud.cn/a4x-kubeflow/ml-pipeline/api-server:1.0.4\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n        name: ml-pipeline-api-server\n        ports:\n        - containerPort: 8888\n          name: http\n        - containerPort: 8887\n          name: grpc\n        readinessProbe:\n          exec:\n            command:\n            - wget\n            - -q\n            - -S\n            - -O\n            - '-'\n            - http://localhost:8888/apis/v1beta1/healthz\n          initialDelaySeconds: 3\n          periodSeconds: 5\n          timeoutSeconds: 2\n      serviceAccountName: ml-pipeline\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ml-pipeline-api-server\" has memory limit 0"
  },
  {
    "id": "01469",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis-commander\" is using an invalid container image, \"rediscommander/redis-commander\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01470",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis-commander\" does not have a read-only root file system"
  },
  {
    "id": "01471",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis-commander\" has cpu request 0"
  },
  {
    "id": "01472",
    "manifest_path": "data/manifests/the_stack_sample/sample_0470.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-commander\n  annotations:\n    container.apparmor.security.beta.kubernetes.io/redis-commander: runtime/default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-commander\n  template:\n    metadata:\n      labels:\n        app: redis-commander\n        tier: backend\n    spec:\n      containers:\n      - name: redis-commander\n        image: rediscommander/redis-commander\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOSTS\n          value: instance1:redis:6379\n        - name: K8S_SIGTERM\n          value: '1'\n        ports:\n        - name: redis-commander\n          containerPort: 8081\n        livenessProbe:\n          httpGet:\n            path: /favicon.png\n            port: 8081\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n        securityContext:\n          runAsNonRoot: true\n          readOnlyRootFilesystem: false\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis-commander\" has memory limit 0"
  },
  {
    "id": "01473",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01474",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01475",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01476",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01477",
    "manifest_path": "data/manifests/the_stack_sample/sample_0471.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9833\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01478",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "01479",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "01480",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "01481",
    "manifest_path": "data/manifests/the_stack_sample/sample_0473.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hello\nspec:\n  containers:\n  - image: felipeogutierrez/explore-akka:1.1\n    name: hello\n    imagePullPolicy: Always\n    args:\n    - '83.1'\n    ports:\n    - containerPort: 8001\n      name: http\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "01482",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"download-dataset\" is using an invalid container image, \"google/cloud-sdk:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01483",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"download-dataset\" does not have a read-only root file system"
  },
  {
    "id": "01484",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"download-dataset\" is not set to runAsNonRoot"
  },
  {
    "id": "01485",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"download-dataset\" has cpu request 0"
  },
  {
    "id": "01486",
    "manifest_path": "data/manifests/the_stack_sample/sample_0474.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: restore-jenkins\nspec:\n  template:\n    spec:\n      containers:\n      - name: download-dataset\n        image: google/cloud-sdk:latest\n        imagePullPolicy: Always\n        volumeMounts:\n        - mountPath: /certs\n          name: gcloud-certs\n          readOnly: true\n        - mountPath: /transfer\n          name: transfer\n        command:\n        - sh\n        - -c\n        - 'gcloud auth activate-service-account --key-file=/certs/svc_account.json\n\n\n          gsutil cp \"<JENKINS_BACKUP_BUCKET>/jenkins.tar.gz\" /transfer/jenkins.tar.gz\n\n          cd /transfer\n\n          tar -xzvf jenkins.tar.gz\n\n          rm jenkins.tar.gz\n\n          cd jenkins_home\n\n          find . -maxdepth 1 -exec mv {} .. \\;\n\n          cd ..\n\n          rm -rf jenkins_home\n\n          '\n      volumes:\n      - name: gcloud-certs\n        secret:\n          secretName: gcloud-creds\n      - name: transfer\n        persistentVolumeClaim:\n          claimName: <EXISTING_CLAIM>\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"download-dataset\" has memory limit 0"
  },
  {
    "id": "01487",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01488",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01489",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01490",
    "manifest_path": "data/manifests/the_stack_sample/sample_0475.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.7.9\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01491",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"fluentd\" does not have a read-only root file system"
  },
  {
    "id": "01492",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"otel-collector\" does not have a read-only root file system"
  },
  {
    "id": "01493",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prepare-fluentd-config\" does not have a read-only root file system"
  },
  {
    "id": "01494",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"fluentd\" is not set to runAsNonRoot"
  },
  {
    "id": "01495",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"otel-collector\" is not set to runAsNonRoot"
  },
  {
    "id": "01496",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prepare-fluentd-config\" is not set to runAsNonRoot"
  },
  {
    "id": "01497",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"otel-collector\" has cpu request 0"
  },
  {
    "id": "01498",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prepare-fluentd-config\" has cpu request 0"
  },
  {
    "id": "01499",
    "manifest_path": "data/manifests/the_stack_sample/sample_0479.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: default-splunk-otel-collector-agent\n  labels:\n    app: splunk-otel-collector\n    chart: splunk-otel-collector-0.28.0\n    release: default\n    heritage: Helm\n    engine: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: splunk-otel-collector\n      release: default\n  template:\n    metadata:\n      labels:\n        app: splunk-otel-collector\n        release: default\n      annotations:\n        checksum/config: 90df4c65aebdd25fb4df652f9607dd45a6ab63e6fd59170427e0c2f9b68dba85\n    spec:\n      serviceAccountName: default-splunk-otel-collector\n      initContainers:\n      - name: prepare-fluentd-config\n        image: busybox:1.33\n        command:\n        - sh\n        - -c\n        args:\n        - if [ -z \"${LOG_FORMAT_TYPE}\" ]; then if [ \"$(ls /var/lib/docker/containers/*/*json.log\n          2>/dev/null | wc -l)\" != \"0\" ]; then export LOG_FORMAT_TYPE=json; else export\n          LOG_FORMAT_TYPE=cri; fi; fi; cp /fluentd/etc/common/* /fluentd/etc/${LOG_FORMAT_TYPE}/*\n          /fluentd/etc/\n        env:\n        - name: LOG_FORMAT_TYPE\n          value: ''\n        volumeMounts:\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: fluentd-config-common\n          mountPath: /fluentd/etc/common\n        - name: fluentd-config-json\n          mountPath: /fluentd/etc/json\n        - name: fluentd-config-cri\n          mountPath: /fluentd/etc/cri\n      containers:\n      - name: fluentd\n        image: splunk/fluentd-hec:1.2.4\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 0\n        env:\n        - name: SPLUNK_MEMORY_TOTAL_MIB\n          value: '500'\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        resources:\n          limits:\n            cpu: 500m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: varlogdest\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: journallogpath\n          mountPath: /run/log/journal\n          readOnly: true\n        - name: fluentd-config\n          mountPath: /fluentd/etc\n        - name: secrets\n          mountPath: /fluentd/etc/splunk\n          readOnly: true\n      - name: otel-collector\n        command:\n        - /otelcol\n        - --config=/conf/relay.yaml\n        - --metrics-addr=0.0.0.0:8888\n        ports:\n        - name: fluentforward\n          containerPort: 8006\n          hostPort: 8006\n          protocol: TCP\n        - name: jaeger-grpc\n          containerPort: 14250\n          hostPort: 14250\n          protocol: TCP\n        - name: jaeger-thrift\n          containerPort: 14268\n          hostPort: 14268\n          protocol: TCP\n        - name: otlp\n          containerPort: 4317\n          hostPort: 4317\n          protocol: TCP\n        - name: sfx-forwarder\n          containerPort: 9080\n          hostPort: 9080\n          protocol: TCP\n        - name: signalfx\n          containerPort: 9943\n          hostPort: 9943\n          protocol: TCP\n        - name: zipkin\n          containerPort: 9411\n          hostPort: 9411\n          protocol: TCP\n        image: quay.io/signalfx/splunk-otel-collector:0.28.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NODE_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.hostIP\n        - name: K8S_POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        - name: K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: K8S_POD_UID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: SPLUNK_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: splunk-otel-collector\n              key: splunk_access_token\n        - name: HOST_PROC\n          value: /hostfs/proc\n        - name: HOST_SYS\n          value: /hostfs/sys\n        - name: HOST_ETC\n          value: /hostfs/etc\n        - name: HOST_VAR\n          value: /hostfs/var\n        - name: HOST_RUN\n          value: /hostfs/run\n        - name: HOST_DEV\n          value: /hostfs/dev\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 13133\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n        volumeMounts:\n        - mountPath: /conf\n          name: otel-configmap\n        - mountPath: /hostfs\n          name: hostfs\n          readOnly: true\n          mountPropagation: HostToContainer\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: varlogdest\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: journallogpath\n        hostPath:\n          path: /run/log/journal\n      - name: secrets\n        secret:\n          secretName: splunk-otel-collector\n      - name: fluentd-config\n        emptyDir: {}\n      - name: fluentd-config-common\n        configMap:\n          name: default-splunk-otel-collector-fluentd\n      - name: fluentd-config-cri\n        configMap:\n          name: default-splunk-otel-collector-fluentd-cri\n      - name: fluentd-config-json\n        configMap:\n          name: default-splunk-otel-collector-fluentd-json\n      - name: hostfs\n        hostPath:\n          path: /\n      - name: otel-configmap\n        configMap:\n          name: default-splunk-otel-collector-otel-agent\n          items:\n          - key: relay\n            path: relay.yaml\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prepare-fluentd-config\" has memory limit 0"
  },
  {
    "id": "01500",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"failing-pod\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01501",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"failing-pod\" does not have a read-only root file system"
  },
  {
    "id": "01502",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"failing-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "01503",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"failing-pod\" has cpu request 0"
  },
  {
    "id": "01504",
    "manifest_path": "data/manifests/the_stack_sample/sample_0481.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failing-pod\nspec:\n  containers:\n  - args:\n    - /bin/sh\n    - -c\n    - while true; do echo $(date) >> ~/tmp/curr-date.txt; sleep 5; done;\n    image: busybox\n    name: failing-pod\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"failing-pod\" has memory limit 0"
  },
  {
    "id": "01505",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"queue\" does not have a read-only root file system"
  },
  {
    "id": "01506",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"queue\" is not set to runAsNonRoot"
  },
  {
    "id": "01507",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"queue\" has cpu request 0"
  },
  {
    "id": "01508",
    "manifest_path": "data/manifests/the_stack_sample/sample_0484.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  labels:\n    app: work-queue\n    component: queue\n    chapter: jobs\n  name: queue\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: work-queue\n  template:\n    metadata:\n      labels:\n        app: work-queue\n        component: queue\n        chapter: jobs\n    spec:\n      containers:\n      - name: queue\n        image: gcr.io/kuar-demo/kuard-amd64:blue\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"queue\" has memory limit 0"
  },
  {
    "id": "01509",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01510",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01511",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01512",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01513",
    "manifest_path": "data/manifests/the_stack_sample/sample_0487.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-143\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01514",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cloudsql-proxy\" is using an invalid container image, \"gcr.io/cloudsql-docker/gce-proxy:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01515",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"study-builder\" is using an invalid container image, \"gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01516",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cloudsql-proxy\" does not have a read-only root file system"
  },
  {
    "id": "01517",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"study-builder\" does not have a read-only root file system"
  },
  {
    "id": "01518",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cloudsql-proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "01519",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"study-builder\" is not set to runAsNonRoot"
  },
  {
    "id": "01520",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cloudsql-proxy\" has cpu request 0"
  },
  {
    "id": "01521",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cloudsql-proxy\" has memory limit 0"
  },
  {
    "id": "01522",
    "manifest_path": "data/manifests/the_stack_sample/sample_0490.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: study-builder\n  labels:\n    app: study-builder\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: study-builder\n  template:\n    metadata:\n      labels:\n        app: study-builder\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - name: study-builder\n        image: gcr.io/<PREFIX>-<ENV>-apps/study-builder:latest\n        env:\n        - name: DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbusername\n        - name: DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbpassword\n        - name: DB_NAME\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: dbname\n        - name: DB_INSTANCE_URL\n          value: 127.0.0.1\n        - name: FROM_EMAIL_ADDRESS\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_address\n        - name: FROM_EMAIL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: email_password\n        - name: SMTP_HOSTNAME\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_hostname\n        - name: SMTP_USE_IP_ALLOWLIST\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: smtp_use_ip_allowlist\n        - name: FROM_EMAIL_DOMAIN\n          valueFrom:\n            secretKeyRef:\n              name: email-credentials\n              key: from_email_domain\n        - name: CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: client_id\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: study-builder-credentials\n              key: secret_key\n        - name: STUDY_BUILDER_BASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: studies_base_url\n        - name: PARTICIPANT_USER_DATASTORE_URL\n          value: http://participant-user-datastore-np:50000/participant-user-datastore\n        - name: RESPONSE_DATASTORE_URL\n          value: http://response-datastore-np:50000/response-datastore\n        - name: SCIM_AUTH_URL\n          value: http://auth-server-np:50000/auth-server\n        - name: GCP_BUCKET_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: study_resources_bucket_name\n        - name: ORG_NAME\n          valueFrom:\n            secretKeyRef:\n              name: shared-secrets\n              key: org_name\n        - name: CATALINA_OPTS\n          value: -Duser.timezone=America/New_York\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /secrets/gcloud_key/key.json\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /studybuilder/healthCheck.do\n            port: 8080\n          initialDelaySeconds: 180\n          periodSeconds: 20\n        resources:\n          requests:\n            memory: 500Mi\n            cpu: 50m\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      - name: cloudsql-proxy\n        image: gcr.io/cloudsql-docker/gce-proxy:latest\n        command:\n        - /cloud_sql_proxy\n        - -instances=<PREFIX>-<ENV>-data:<LOCATION>:mystudies=tcp:3306\n        - -credential_file=/secrets/gcloud_key/key.json\n        volumeMounts:\n        - name: gcloud-key-volume\n          mountPath: /secrets/gcloud_key\n          readOnly: true\n      volumes:\n      - name: gcloud-key-volume\n        secret:\n          secretName: study-builder-gke-sa-gcloud-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"study-builder\" has memory limit 0"
  },
  {
    "id": "01523",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"pod-identity-webhook\" is using an invalid container image, \"IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01524",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod-identity-webhook\" does not have a read-only root file system"
  },
  {
    "id": "01525",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod-identity-webhook\" is not set to runAsNonRoot"
  },
  {
    "id": "01526",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pod-identity-webhook\" has cpu request 0"
  },
  {
    "id": "01527",
    "manifest_path": "data/manifests/the_stack_sample/sample_0493.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pod-identity-webhook\n  namespace: pod-identity\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: pod-identity-webhook\n  template:\n    metadata:\n      labels:\n        app: pod-identity-webhook\n    spec:\n      serviceAccountName: pod-identity-webhook\n      containers:\n      - name: pod-identity-webhook\n        image: IMAGE\n        imagePullPolicy: Always\n        command:\n        - /webhook\n        - --in-cluster\n        - --namespace=pod-identity\n        - --service-name=pod-identity-webhook\n        - --tls-secret=pod-identity-webhook\n        - --annotation-prefix=eks.amazonaws.com\n        - --token-audience=sts.amazonaws.com\n        - --logtostderr\n        volumeMounts:\n        - name: webhook-certs\n          mountPath: /var/run/app/certs\n          readOnly: false\n      volumes:\n      - name: webhook-certs\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pod-identity-webhook\" has memory limit 0"
  },
  {
    "id": "01528",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"branchprotector\" does not have a read-only root file system"
  },
  {
    "id": "01529",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"branchprotector\" is not set to runAsNonRoot"
  },
  {
    "id": "01530",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"branchprotector\" has cpu request 0"
  },
  {
    "id": "01531",
    "manifest_path": "data/manifests/the_stack_sample/sample_0494.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: default\n  name: branchprotector\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: branchprotector\n          image: gcr.io/k8s-prow/branchprotector:v20200529-d374280316\n          args:\n          - --config-path=/etc/config/config.yaml\n          - --job-config-path=/etc/job-config\n          - --github-token-path=/etc/github/oauth\n          - --confirm\n          - --github-endpoint=http://ghproxy\n          - --github-endpoint=https://api.github.com\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n          - name: job-config\n            mountPath: /etc/job-config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: config\n        - name: job-config\n          configMap:\n            name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"branchprotector\" has memory limit 0"
  },
  {
    "id": "01532",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sample-app\" does not have a read-only root file system"
  },
  {
    "id": "01533",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sample-app\" is not set to runAsNonRoot"
  },
  {
    "id": "01534",
    "manifest_path": "data/manifests/the_stack_sample/sample_0499.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: sample-app\n  name: sample-app\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: sample-app\n  template:\n    metadata:\n      labels:\n        app: sample-app\n        fluent-pvc-operator.tech.zozo.com/fluent-pvc-name: fluent-pvc-operator-example-log-collection\n    spec:\n      containers:\n      - name: sample-app\n        image: fluent-pvc-operator-sample-app:development\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 100m\n            memory: 50Mi\n        env:\n        - name: BENCHMARK_LOGGING_MAX_LOG_COUNT\n          value: '10000'\n        - name: BENCHMARK_LOGGING_INTERVAL_MILLIS\n          value: '1000'\n        - name: BENCHMARK_LOGGING_EVENT_NAME\n          value: test-event\n        - name: BENCHMARK_LOGGING_PAYLOAD_KEY1\n          value: myKey1\n        - name: BENCHMARK_LOGGING_PAYLOAD_VALUE1\n          value: myValue1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sample-app\" has cpu request 0"
  },
  {
    "id": "01535",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"myjavaapp-container\" does not have a read-only root file system"
  },
  {
    "id": "01536",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"myjavaapp-container\" is not set to runAsNonRoot"
  },
  {
    "id": "01537",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"myjavaapp-container\" has cpu request 0"
  },
  {
    "id": "01538",
    "manifest_path": "data/manifests/the_stack_sample/sample_0505.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myjavaapp-deploy\n  labels:\n    app: myjavaapp\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: myjavaapp\n  template:\n    metadata:\n      labels:\n        app: myjavaapp\n    spec:\n      containers:\n      - name: myjavaapp-container\n        image: nagendra464/deployimage:1\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"myjavaapp-container\" has memory limit 0"
  },
  {
    "id": "01539",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hvpa-manager\" is using an invalid container image, \"ggaurav10/hvpa-controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01540",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hvpa-manager\" does not have a read-only root file system"
  },
  {
    "id": "01541",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hvpa-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "01542",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hvpa-manager\" has cpu request 0"
  },
  {
    "id": "01543",
    "manifest_path": "data/manifests/the_stack_sample/sample_0507.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hvpa-controller\n  namespace: system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      controller: hvpa\n  template:\n    metadata:\n      labels:\n        controller: hvpa\n    spec:\n      containers:\n      - image: ggaurav10/hvpa-controller:latest\n        name: hvpa-manager\n        command:\n        - ./manager\n        - --logtostderr=true\n        - --v=2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hvpa-manager\" has memory limit 0"
  },
  {
    "id": "01544",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app2\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01545",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app2\" does not have a read-only root file system"
  },
  {
    "id": "01546",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app2\" is not set to runAsNonRoot"
  },
  {
    "id": "01547",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app2\" has cpu request 0"
  },
  {
    "id": "01548",
    "manifest_path": "data/manifests/the_stack_sample/sample_0508.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: app2\n  name: app2\n  namespace: app2-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app2\n  template:\n    metadata:\n      labels:\n        app: app2\n    spec:\n      containers:\n      - image: nginx\n        name: app2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app2\" has memory limit 0"
  },
  {
    "id": "01549",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01550",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01551",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01552",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01553",
    "manifest_path": "data/manifests/the_stack_sample/sample_0512.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7005\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01554",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "01555",
    "manifest_path": "data/manifests/the_stack_sample/sample_0514.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: shasbdois.azurecr.io/captureorder:placeholdertag\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "01556",
    "manifest_path": "data/manifests/the_stack_sample/sample_0517.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kuard-a\nspec:\n  volumes:\n  - name: kuard-data\n    nfs:\n      server: ks101\n      path: /var/export\n  containers:\n  - image: gcr.io/kuar-demo/kuard-amd64:1\n    name: kuard-a\n    volumeMounts:\n    - mountPath: /data\n      name: kuard-data\n    resources:\n      requests:\n        cpu: 600m\n        memory: 128Mi\n      limits:\n        cpu: 1000m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthy\n        port: 8080\n      initialDelaySeconds: 5\n      timeoutSeconds: 1\n      periodSeconds: 10\n      failureThreshold: 3\n    ports:\n    - containerPort: 8080\n      name: http\n      protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kuard-a\" does not have a read-only root file system"
  },
  {
    "id": "01557",
    "manifest_path": "data/manifests/the_stack_sample/sample_0517.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kuard-a\nspec:\n  volumes:\n  - name: kuard-data\n    nfs:\n      server: ks101\n      path: /var/export\n  containers:\n  - image: gcr.io/kuar-demo/kuard-amd64:1\n    name: kuard-a\n    volumeMounts:\n    - mountPath: /data\n      name: kuard-data\n    resources:\n      requests:\n        cpu: 600m\n        memory: 128Mi\n      limits:\n        cpu: 1000m\n        memory: 256Mi\n    livenessProbe:\n      httpGet:\n        path: /healthy\n        port: 8080\n      initialDelaySeconds: 5\n      timeoutSeconds: 1\n      periodSeconds: 10\n      failureThreshold: 3\n    ports:\n    - containerPort: 8080\n      name: http\n      protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kuard-a\" is not set to runAsNonRoot"
  },
  {
    "id": "01558",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sidecar-injector\" does not have a read-only root file system"
  },
  {
    "id": "01559",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sidecar-injector\" has cpu request 0"
  },
  {
    "id": "01560",
    "manifest_path": "data/manifests/the_stack_sample/sample_0518.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vault-example-agent-injector\n  labels:\n    app.kubernetes.io/name: vault-example-agent-injector\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-example-agent-injector\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-example-agent-injector\n        component: webhook\n    spec:\n      serviceAccountName: vault-example-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:0.1.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: https://vault-example.vault-example.svc:8200\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: vault:1.3.1\n        - name: AGENT_INJECT_TLS_AUTO\n          value: vault-example-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: vault-example-agent-injector-svc,vault-example-agent-injector-svc.vault-example,vault-example-agent-injector-svc.vault-example.svc\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 1\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 2\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sidecar-injector\" has memory limit 0"
  },
  {
    "id": "01561",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"goapp\" does not have a read-only root file system"
  },
  {
    "id": "01562",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"goapp\" is not set to runAsNonRoot"
  },
  {
    "id": "01563",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"goapp\" has cpu request 0"
  },
  {
    "id": "01564",
    "manifest_path": "data/manifests/the_stack_sample/sample_0521.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: goapp-deployment\nspec:\n  selector:\n    matchLabels:\n      app: goapp\n  template:\n    metadata:\n      labels:\n        app: goapp\n    spec:\n      containers:\n      - name: goapp\n        image: docker.pkg.github.com/kenji-kk/cicd-handson-2021-code/go-image:base\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"goapp\" has memory limit 0"
  },
  {
    "id": "01565",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-udp-ingress-lb\" does not have a read-only root file system"
  },
  {
    "id": "01566",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-udp-ingress-lb\" is not set to runAsNonRoot"
  },
  {
    "id": "01567",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-udp-ingress-lb\" has cpu request 0"
  },
  {
    "id": "01568",
    "manifest_path": "data/manifests/the_stack_sample/sample_0523.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: nginx-udp-ingress-controller\n  labels:\n    k8s-app: nginx-udp-ingress-lb\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    k8s-app: nginx-udp-ingress-lb\n  template:\n    metadata:\n      labels:\n        k8s-app: nginx-udp-ingress-lb\n        name: nginx-udp-ingress-lb\n    spec:\n      containers:\n      - image: gcr.io/google_containers/nginx-ingress-controller:0.9.0-beta.13\n        name: nginx-udp-ingress-lb\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          timeoutSeconds: 1\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 80\n          hostPort: 80\n        - containerPort: 443\n          hostPort: 443\n        - containerPort: 9001\n          hostPort: 9001\n        args:\n        - /nginx-ingress-controller\n        - --default-backend-service=$(POD_NAMESPACE)/default-http-backend\n        - --udp-services-configmap=$(POD_NAMESPACE)/nginx-udp-ingress-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-udp-ingress-lb\" has memory limit 0"
  },
  {
    "id": "01569",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01570",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01571",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01572",
    "manifest_path": "data/manifests/the_stack_sample/sample_0526.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: fungusappdeployment\n  labels:\n    appgungus: api\nspec:\n  selector:\n    matchLabels:\n      octopusexport: OctopusExport\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        appgungus: api\n        octopusexport: OctopusExport\n    spec:\n      containers:\n      - name: nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n      securityContext:\n        runAsNonRoot: true\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - web\n              topologyKey: kubernetes.io/hostname\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01573",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox1\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01574",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox2\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01575",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox1\" does not have a read-only root file system"
  },
  {
    "id": "01576",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox2\" does not have a read-only root file system"
  },
  {
    "id": "01577",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox1\" is not set to runAsNonRoot"
  },
  {
    "id": "01578",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox2\" is not set to runAsNonRoot"
  },
  {
    "id": "01579",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox1\" has cpu request 0"
  },
  {
    "id": "01580",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox2\" has cpu request 0"
  },
  {
    "id": "01581",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox1\" has memory limit 0"
  },
  {
    "id": "01582",
    "manifest_path": "data/manifests/the_stack_sample/sample_0528.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: busybox\nspec:\n  volumes:\n  - name: myvolume1\n    emptyDir: {}\n  - name: myvolume2\n    emptyDir: {}\n  containers:\n  - name: busybox1\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /foo\n      name: myvolume1\n  - name: busybox2\n    image: busybox:latest\n    command:\n    - /bin/sh\n    - -c\n    - sleep 600\n    volumeMounts:\n    - mountPath: /bar\n      name: myvolume2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox2\" has memory limit 0"
  },
  {
    "id": "01583",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"reverseproxy\" is using an invalid container image, \"scheele/reverseproxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01584",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"reverseproxy\" does not have a read-only root file system"
  },
  {
    "id": "01585",
    "manifest_path": "data/manifests/the_stack_sample/sample_0529.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-example\nspec:\n  containers:\n  - image: scheele/reverseproxy\n    name: reverseproxy\n    imagePullPolicy: Always\n    resources:\n      requests:\n        memory: 64Mi\n        cpu: 250m\n      limits:\n        memory: 1024Mi\n        cpu: 500m\n    ports:\n    - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"reverseproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "01586",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "01587",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "01588",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "01589",
    "manifest_path": "data/manifests/the_stack_sample/sample_0531.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210520-56277900f8\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "01590",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01591",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01592",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "01593",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "01594",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "01595",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "01596",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "01597",
    "manifest_path": "data/manifests/the_stack_sample/sample_0532.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces1\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "01598",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nexus3\" is using an invalid container image, \"sonatype/nexus3\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01599",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"volume-mount-uid\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01600",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nexus3\" does not have a read-only root file system"
  },
  {
    "id": "01601",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"volume-mount-uid\" does not have a read-only root file system"
  },
  {
    "id": "01602",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nexus3\" is not set to runAsNonRoot"
  },
  {
    "id": "01603",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"volume-mount-uid\" is not set to runAsNonRoot"
  },
  {
    "id": "01604",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"volume-mount-uid\" has cpu request 0"
  },
  {
    "id": "01605",
    "manifest_path": "data/manifests/the_stack_sample/sample_0533.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nexus3\n  namespace: default\n  labels:\n    app: nexus3\nspec:\n  selector:\n    matchLabels:\n      app: nexus3\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nexus3\n    spec:\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      initContainers:\n      - name: volume-mount-uid\n        image: busybox\n        command:\n        - sh\n        - -c\n        - chown -R 200:200 /nexus-data\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n      containers:\n      - name: nexus3\n        image: sonatype/nexus3\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 60\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          requests:\n            cpu: 500m\n            memory: 1Gi\n          limits:\n            cpu: 8000m\n            memory: 8Gi\n        ports:\n        - name: web\n          containerPort: 8081\n        - name: docker\n          containerPort: 5003\n        volumeMounts:\n        - name: nexus-data\n          mountPath: /nexus-data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"volume-mount-uid\" has memory limit 0"
  },
  {
    "id": "01606",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"default-http-backend\" does not have a read-only root file system"
  },
  {
    "id": "01607",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"l7-lb-controller\" does not have a read-only root file system"
  },
  {
    "id": "01608",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"default-http-backend\" is not set to runAsNonRoot"
  },
  {
    "id": "01609",
    "manifest_path": "data/manifests/the_stack_sample/sample_0535.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: l7-lb-controller\n  namespace: kube-system\n  labels:\n    k8s-app: glbc\n    version: v0.5.1\n    kubernetes.io/cluster-service: 'true'\n    kubernetes.io/name: GLBC\nspec:\n  replicas: 1\n  selector:\n    k8s-app: glbc\n    version: v0.5.1\n  template:\n    metadata:\n      labels:\n        k8s-app: glbc\n        version: v0.5.1\n        name: glbc\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - name: default-http-backend\n        image: gcr.io/google_containers/defaultbackend:1.0\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n      - image: gcr.io/google_containers/glbc:0.5.1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 30\n          timeoutSeconds: 5\n        name: l7-lb-controller\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 100m\n            memory: 50Mi\n        args:\n        - --default-backend-service=kube-system/default-http-backend\n        - --sync-period=300s\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"l7-lb-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "01610",
    "manifest_path": "data/manifests/the_stack_sample/sample_0536.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-core\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/version: 1.4.0\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon1\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/version: v0.5\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon1\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/version: v0.5\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=8443\n        - --create-resources=$(MANAGER_CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: RELATED_IMAGE_STORAGE_INITIALIZER\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_REST\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TENSORFLOW\n          value: ''\n        - name: RELATED_IMAGE_EXPLAINER\n          value: ''\n        - name: RELATED_IMAGE_MOCK_CLASSIFIER\n          value: ''\n        - name: MANAGER_CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/engine:1.4.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: PREDICTIVE_UNIT_METRICS_PORT_NAME\n          value: metrics\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: KEDA_ENABLED\n          value: 'false'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-executor:1.4.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_SERVER_METRICS_PORT_NAME\n          value: metrics\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT\n          value: http://default-broker\n        - name: DEFAULT_USER_ID\n          value: '8888'\n        - name: EXECUTOR_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: EXECUTOR_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        image: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-operator:1.4.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 8443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "01611",
    "manifest_path": "data/manifests/the_stack_sample/sample_0536.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: seldon\n    app.kubernetes.io/component: seldon\n    app.kubernetes.io/instance: seldon-core\n    app.kubernetes.io/name: seldon-core-operator\n    app.kubernetes.io/version: 1.4.0\n    control-plane: seldon-controller-manager\n  name: seldon-controller-manager\n  namespace: kubeflow\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: seldon\n      app.kubernetes.io/component: seldon\n      app.kubernetes.io/instance: seldon1\n      app.kubernetes.io/name: seldon-core-operator\n      app.kubernetes.io/version: v0.5\n      control-plane: seldon-controller-manager\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n      labels:\n        app: seldon\n        app.kubernetes.io/component: seldon\n        app.kubernetes.io/instance: seldon1\n        app.kubernetes.io/name: seldon-core-operator\n        app.kubernetes.io/version: v0.5\n        control-plane: seldon-controller-manager\n    spec:\n      containers:\n      - args:\n        - --enable-leader-election\n        - --webhook-port=8443\n        - --create-resources=$(MANAGER_CREATE_RESOURCES)\n        - ''\n        command:\n        - /manager\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: RELATED_IMAGE_EXECUTOR\n          value: ''\n        - name: RELATED_IMAGE_ENGINE\n          value: ''\n        - name: RELATED_IMAGE_STORAGE_INITIALIZER\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_SKLEARNSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_XGBOOSTSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_REST\n          value: ''\n        - name: RELATED_IMAGE_MLFLOWSERVER_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_REST\n          value: ''\n        - name: RELATED_IMAGE_TFPROXY_GRPC\n          value: ''\n        - name: RELATED_IMAGE_TENSORFLOW\n          value: ''\n        - name: RELATED_IMAGE_EXPLAINER\n          value: ''\n        - name: RELATED_IMAGE_MOCK_CLASSIFIER\n          value: ''\n        - name: MANAGER_CREATE_RESOURCES\n          value: 'false'\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONTROLLER_ID\n          value: ''\n        - name: AMBASSADOR_ENABLED\n          value: 'true'\n        - name: AMBASSADOR_SINGLE_NAMESPACE\n          value: 'false'\n        - name: ENGINE_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/engine:1.4.0\n        - name: ENGINE_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: ENGINE_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: ENGINE_CONTAINER_USER\n          value: '8888'\n        - name: ENGINE_LOG_MESSAGES_EXTERNALLY\n          value: 'false'\n        - name: PREDICTIVE_UNIT_SERVICE_PORT\n          value: '9000'\n        - name: PREDICTIVE_UNIT_DEFAULT_ENV_SECRET_REF_NAME\n          value: ''\n        - name: PREDICTIVE_UNIT_METRICS_PORT_NAME\n          value: metrics\n        - name: ENGINE_SERVER_GRPC_PORT\n          value: '5001'\n        - name: ENGINE_SERVER_PORT\n          value: '8000'\n        - name: ENGINE_PROMETHEUS_PATH\n          value: /prometheus\n        - name: ISTIO_ENABLED\n          value: 'true'\n        - name: KEDA_ENABLED\n          value: 'false'\n        - name: ISTIO_GATEWAY\n          value: kubeflow/kubeflow-gateway\n        - name: ISTIO_TLS_MODE\n          value: ''\n        - name: USE_EXECUTOR\n          value: 'true'\n        - name: EXECUTOR_CONTAINER_IMAGE_AND_VERSION\n          value: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-executor:1.4.0\n        - name: EXECUTOR_CONTAINER_IMAGE_PULL_POLICY\n          value: IfNotPresent\n        - name: EXECUTOR_PROMETHEUS_PATH\n          value: /prometheus\n        - name: EXECUTOR_SERVER_PORT\n          value: '8000'\n        - name: EXECUTOR_CONTAINER_USER\n          value: '8888'\n        - name: EXECUTOR_CONTAINER_SERVICE_ACCOUNT_NAME\n          value: default\n        - name: EXECUTOR_SERVER_METRICS_PORT_NAME\n          value: metrics\n        - name: EXECUTOR_REQUEST_LOGGER_DEFAULT_ENDPOINT\n          value: http://default-broker\n        - name: DEFAULT_USER_ID\n          value: '8888'\n        - name: EXECUTOR_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: EXECUTOR_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: EXECUTOR_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_REQUEST\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_REQUEST\n          value: 512Mi\n        - name: ENGINE_DEFAULT_CPU_LIMIT\n          value: 500m\n        - name: ENGINE_DEFAULT_MEMORY_LIMIT\n          value: 512Mi\n        image: uhub.service.ucloud.cn/a4x-kubeflow/seldonio/seldon-core-operator:1.4.0\n        imagePullPolicy: IfNotPresent\n        name: manager\n        ports:\n        - containerPort: 8443\n          name: webhook-server\n          protocol: TCP\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n        resources:\n          limits:\n            cpu: 500m\n            memory: 300Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n      serviceAccountName: seldon-manager\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: seldon-webhook-server-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "01612",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgres\" does not have a read-only root file system"
  },
  {
    "id": "01613",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgres\" is not set to runAsNonRoot"
  },
  {
    "id": "01614",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgres\" has cpu request 0"
  },
  {
    "id": "01615",
    "manifest_path": "data/manifests/the_stack_sample/sample_0538.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-statefulset\n  namespace: default\n  labels:\n    app: postgres\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres\n  template:\n    metadata:\n      labels:\n        app: postgres\n    spec:\n      containers:\n      - name: postgres\n        image: postgres:12\n        env:\n        - name: POSTGRES_DB\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: db\n        - name: POSTGRES_USER\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: user\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres\n              key: password\n        ports:\n        - containerPort: 5432\n          name: postgresdb\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: postgres\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: postgres-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"postgres\" has memory limit 0"
  },
  {
    "id": "01616",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "01617",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress\" is not set to runAsNonRoot"
  },
  {
    "id": "01618",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress\" has cpu request 0"
  },
  {
    "id": "01619",
    "manifest_path": "data/manifests/the_stack_sample/sample_0539.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress\n  namespace: nginx-ingress\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: nginx-ingress\n  template:\n    metadata:\n      labels:\n        app: nginx-ingress\n    spec:\n      serviceAccountName: nginx-ingress\n      containers:\n      - image: nginx/nginx-ingress:edge\n        imagePullPolicy: Always\n        name: nginx-ingress\n        ports:\n        - name: http\n          containerPort: 80\n        - name: https\n          containerPort: 443\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-configmaps=$(POD_NAMESPACE)/nginx-config\n        - -ingress-class=nginx-ingress\n        - -default-server-tls-secret=$(POD_NAMESPACE)/default-server-secret\n        - -external-service=nginx-ingress\n        - -report-ingress-status\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "01620",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01621",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01622",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01623",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01624",
    "manifest_path": "data/manifests/the_stack_sample/sample_0541.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9975\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01625",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dn\" is using an invalid container image, \"hdfgroup/hsds\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01626",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sn\" is using an invalid container image, \"hdfgroup/hsds\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01627",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dn\" does not have a read-only root file system"
  },
  {
    "id": "01628",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sn\" does not have a read-only root file system"
  },
  {
    "id": "01629",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dn\" is not set to runAsNonRoot"
  },
  {
    "id": "01630",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sn\" is not set to runAsNonRoot"
  },
  {
    "id": "01631",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dn\" has cpu request 0"
  },
  {
    "id": "01632",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sn\" has cpu request 0"
  },
  {
    "id": "01633",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dn\" has memory limit 0"
  },
  {
    "id": "01634",
    "manifest_path": "data/manifests/the_stack_sample/sample_0545.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: hsds\n  name: hsds\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hsds\n  template:\n    metadata:\n      labels:\n        app: hsds\n    spec:\n      containers:\n      - name: sn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: accounts\n          mountPath: /config/passwd.txt\n          subPath: passwd.txt\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 5101\n        env:\n        - name: NODE_TYPE\n          value: sn\n        - name: PASSWORD_FILE\n          value: /config/passwd.txt\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n        - name: AZURE_APP_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_app_id\n        - name: AZURE_RESOURCE_ID\n          valueFrom:\n            secretKeyRef:\n              name: azure-ad-ids\n              key: az_resource_id\n      - name: dn\n        image: hdfgroup/hsds\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: config\n          mountPath: /config/config.yml\n          subPath: config.yml\n        - name: override\n          mountPath: /config/override.yml\n          subPath: override.yml\n        ports:\n        - containerPort: 6101\n        env:\n        - name: NODE_TYPE\n          value: dn\n        - name: AZURE_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: azure-conn-str\n              key: az_conn_str\n      volumes:\n      - name: accounts\n        secret:\n          secretName: user-password\n      - name: config\n        configMap:\n          name: hsds-config\n      - name: override\n        configMap:\n          name: hsds-override\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sn\" has memory limit 0"
  },
  {
    "id": "01635",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"catalog-operator\" does not have a read-only root file system"
  },
  {
    "id": "01636",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"catalog-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "01637",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"catalog-operator\" has cpu request 0"
  },
  {
    "id": "01638",
    "manifest_path": "data/manifests/the_stack_sample/sample_0546.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        - -namespace\n        - operator-lifecycle-manager\n        - -debug\n        image: quay.io/coreos/catalog@sha256:8fc933e660a5b143bce7a5e4cb1606630fa9497cc252a7e47e0def3c18268f45\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"catalog-operator\" has memory limit 0"
  },
  {
    "id": "01639",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-exporter\" does not have a read-only root file system"
  },
  {
    "id": "01640",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-exporter\" is not set to runAsNonRoot"
  },
  {
    "id": "01641",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "01642",
    "manifest_path": "data/manifests/the_stack_sample/sample_0550.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: node-exporter\n  labels:\n    app: node-exporter\nspec:\n  selector:\n    matchLabels:\n      app: node-exporter\n  template:\n    metadata:\n      labels:\n        app: node-exporter\n    spec:\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.0.1\n        args:\n        - --web.listen-address=127.0.0.1:9100\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --no-collector.wifi\n        - --no-collector.hwmon\n        volumeMounts:\n        - mountPath: /host/proc\n          mountPropagation: HostToContainer\n          name: proc\n          readOnly: true\n        - mountPath: /host/sys\n          mountPropagation: HostToContainer\n          name: sys\n          readOnly: true\n        - mountPath: /host/root\n          mountPropagation: HostToContainer\n          name: root\n          readOnly: true\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "01643",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "01644",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "01645",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "01646",
    "manifest_path": "data/manifests/the_stack_sample/sample_0551.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  selector:\n    matchLabels:\n      app: sinker\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        image: gcr.io/k8s-prow/sinker:v20211206-64485af39f\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "01647",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01648",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01649",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01650",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01651",
    "manifest_path": "data/manifests/the_stack_sample/sample_0553.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-499\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01652",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"web-server\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01653",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"web-server\" does not have a read-only root file system"
  },
  {
    "id": "01654",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"web-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01655",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"web-server\" has cpu request 0"
  },
  {
    "id": "01656",
    "manifest_path": "data/manifests/the_stack_sample/sample_0558.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: csi-scale-staticdemo-pod\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: web-server\n    image: nginx\n    volumeMounts:\n    - name: mypvc\n      mountPath: /usr/share/nginx/html/scale\n    ports:\n    - containerPort: 80\n  volumes:\n  - name: mypvc\n    persistentVolumeClaim:\n      claimName: scale-static-pvc\n      readOnly: false\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"web-server\" has memory limit 0"
  },
  {
    "id": "01657",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "01658",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "01659",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "01660",
    "manifest_path": "data/manifests/the_stack_sample/sample_0560.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20201001-0240871903\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "01661",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01662",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01663",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01664",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01665",
    "manifest_path": "data/manifests/the_stack_sample/sample_0562.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-716\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01666",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sampleapp\" is using an invalid container image, \"k8sexamplesacr.azurecr.io/sampleapp\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01667",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sampleapp\" does not have a read-only root file system"
  },
  {
    "id": "01668",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sampleapp\" is not set to runAsNonRoot"
  },
  {
    "id": "01669",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sampleapp\" has cpu request 0"
  },
  {
    "id": "01670",
    "manifest_path": "data/manifests/the_stack_sample/sample_0563.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sampleapp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sampleapp\n  template:\n    metadata:\n      labels:\n        app: sampleapp\n    spec:\n      containers:\n      - name: sampleapp\n        image: k8sexamplesacr.azurecr.io/sampleapp\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sampleapp\" has memory limit 0"
  },
  {
    "id": "01671",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01672",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "01673",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "01674",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "01675",
    "manifest_path": "data/manifests/the_stack_sample/sample_0564.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: fluid-copy-test\nspec:\n  template:\n    spec:\n      containers:\n      - name: busybox\n        image: busybox\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - set -x; time cp -r /data/hbase ./\n        volumeMounts:\n        - mountPath: /data\n          name: hbase-vol\n      volumes:\n      - name: hbase-vol\n        persistentVolumeClaim:\n          claimName: hbase\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "01676",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "01677",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "01678",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "01679",
    "manifest_path": "data/manifests/the_stack_sample/sample_0566.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: tuntsov/hipster-payment:v0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "01680",
    "manifest_path": "data/manifests/the_stack_sample/sample_0568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app-api\n        image: gcr.io/google-samples/node-hello:1.0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app-api\" does not have a read-only root file system"
  },
  {
    "id": "01681",
    "manifest_path": "data/manifests/the_stack_sample/sample_0568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app-api\n        image: gcr.io/google-samples/node-hello:1.0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app-api\" is not set to runAsNonRoot"
  },
  {
    "id": "01682",
    "manifest_path": "data/manifests/the_stack_sample/sample_0568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app-api\n        image: gcr.io/google-samples/node-hello:1.0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app-api\" has cpu request 0"
  },
  {
    "id": "01683",
    "manifest_path": "data/manifests/the_stack_sample/sample_0568.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  labels:\n    app: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      labels:\n        app: app\n    spec:\n      containers:\n      - name: app-api\n        image: gcr.io/google-samples/node-hello:1.0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app-api\" has memory limit 0"
  },
  {
    "id": "01684",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init\" does not have a read-only root file system"
  },
  {
    "id": "01685",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "01686",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init\" is not set to runAsNonRoot"
  },
  {
    "id": "01687",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "01688",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init\" has cpu request 0"
  },
  {
    "id": "01689",
    "manifest_path": "data/manifests/the_stack_sample/sample_0570.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: podinfo\n  namespace: demo\n  labels:\n    app: podinfo\n  annotations:\n    flux.weave.works/automated: 'true'\n    flux.weave.works/tag.init: glob:1.4.*\n    flux.weave.works/tag.podinfod: glob:1.4.*\n    flux.weave.works/locked: 'true'\n    flux.weave.works/locked_msg: 1.4.2 does not work for us\n    flux.weave.works/locked_user: \"Changyu Seon (\\u1109\\u1165\\u11AB\\u110E\\u1161\\u11AB\\\n      \\u1100\\u1172) <changyu.seon@bespinglobal.com>\"\nspec:\n  selector:\n    matchLabels:\n      app: podinfo\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n      labels:\n        app: podinfo\n    spec:\n      initContainers:\n      - name: init\n        image: alpine:3.10\n        command:\n        - sleep\n        - '1'\n      containers:\n      - name: podinfod\n        image: stefanprodan/podinfo:1.4.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9898\n          name: http\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        env:\n        - name: PODINFO_UI_COLOR\n          value: green\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 9898\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 9898\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init\" has memory limit 0"
  },
  {
    "id": "01690",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubernetes-client\" is using an invalid container image, \"<KUBERNETES_CLIENT_IMAGE>\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01691",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubernetes-client\" does not have a read-only root file system"
  },
  {
    "id": "01692",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubernetes-client\" is not set to runAsNonRoot"
  },
  {
    "id": "01693",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubernetes-client\" has cpu request 0"
  },
  {
    "id": "01694",
    "manifest_path": "data/manifests/the_stack_sample/sample_0572.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: <ACUMOS_NAMESPACE>\n  name: kubernetes-client\n  labels:\n    app: kubernetes-client\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kubernetes-client\n  template:\n    metadata:\n      labels:\n        app: kubernetes-client\n        <ACUMOS_SERVICE_LABEL_KEY>: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: <ACUMOS_SERVICE_LABEL_KEY>\n                operator: NotIn\n                values:\n                - <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n            topologyKey: kubernetes.io/node\n      containers:\n      - name: kubernetes-client\n        image: <KUBERNETES_CLIENT_IMAGE>\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - set -x; cd maven; java $JAVA_OPTS -Dhttp.proxyHost=$ACUMOS_HTTP_PROXY_HOST\n          -Dhttp.proxyPort=$ACUMOS_HTTP_PROXY_PORT -Dhttp.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS\n          -Dhttps.proxyHost=$ACUMOS_HTTP_PROXY_HOST -Dhttps.proxyPort=$ACUMOS_HTTP_PROXY_PORT\n          -Dhttps.nonProxyHosts=$ACUMOS_HTTP_NON_PROXY_HOSTS -Djava.security.egd=file:/dev/./urandom\n          -jar *.jar\n        env:\n        - name: ACUMOS_HTTP_NON_PROXY_HOSTS\n          value: <ACUMOS_HTTP_NON_PROXY_HOSTS>|cds-service\n        - name: ACUMOS_HTTP_PROXY_HOST\n          value: <ACUMOS_HTTP_PROXY_HOST>\n        - name: ACUMOS_HTTP_PROXY_PORT\n          value: <ACUMOS_HTTP_PROXY_PORT>\n        - name: JAVA_OPTS\n          value: -Xms128m -Xmx512m\n        - name: SPRING_APPLICATION_JSON\n          value: '{ \"logging\": { \"level\": { \"root\": \"INFO\" } }, \"kube\" : { \"incrementPort\":\n            \"8557\", \"singleModelPort\": \"8556\", \"folderPath\": \"/maven/home\", \"singleNodePort\":\n            \"30333\", \"singleTargetPort\": \"8061\", \"dataBrokerModelPort\": \"8556\", \"dataBrokerNodePort\":\n            \"30556\", \"dataBrokerTargetPort\": \"8556\", \"mlTargetPort\": \"8061\", \"nginxImageName\":\n            \"nginx\", \"nexusEndPointURL\": \"http://localhost:80\" }, \"dockerproxy\": {\n            \"host\": \"<ACUMOS_DOCKER_PROXY_HOST>\", \"port\": \"<ACUMOS_DOCKER_PROXY_PORT>\"\n            }, \"blueprint\": { \"ImageName\": \"<BLUEPRINT_ORCHESTRATOR_IMAGE>\", \"name\":\n            \"blueprint-orchestrator\", \"nodePort\": \"30555\", \"port\": \"8061\" }, \"nexus\":\n            { \"url\": \"http://<ACUMOS_NEXUS_HOST>:<ACUMOS_NEXUS_API_PORT>/<ACUMOS_NEXUS_MAVEN_REPO_PATH>/<ACUMOS_NEXUS_MAVEN_REPO>/\",\n            \"password\": \"<ACUMOS_NEXUS_RW_USER_PASSWORD>\", \"username\": \"<ACUMOS_NEXUS_RW_USER>\",\n            \"groupid\": \"<ACUMOS_NEXUS_GROUP>\" }, \"cmndatasvc\": { \"cmndatasvcendpointurl\":\n            \"http://<ACUMOS_CDS_HOST>:<ACUMOS_CDS_PORT>/ccds\", \"cmndatasvcuser\": \"<ACUMOS_CDS_USER>\",\n            \"cmndatasvcpwd\": \"<ACUMOS_CDS_PASSWORD>\" }, \"probe\": { \"probeImageName\":\n            \"<PROTO_VIEWER_IMAGE>\", \"probeImagePORT\": \"5006\", \"probeModelPort\": \"5006\",\n            \"probeNodePort\": \"30800\", \"probeTargetPort\": \"5006\", \"probeApiPort\": \"5006\",\n            \"probeExternalPort\": \"30800\", \"probeSchemaPort\": \"80\" }, \"logstash\": {\n            \"host\": \"<ACUMOS_ELK_HOST>\", \"ip\": \"<ACUMOS_ELK_HOST_IP>\", \"port\": \"<ACUMOS_ELK_LOGSTASH_PORT>\"\n            }, \"server\": { \"port\": \"8082\" } }'\n        ports:\n        - containerPort: 8082\n        volumeMounts:\n        - mountPath: /maven/logs\n          name: logs\n      volumes:\n      - name: logs\n        persistentVolumeClaim:\n          claimName: <ACUMOS_KUBERNETES_CLIENT_SERVICE_LABEL>\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubernetes-client\" has memory limit 0"
  },
  {
    "id": "01695",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"hotel-fahmi\" is using an invalid container image, \"localhost:5000/hotel-fahmi\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01696",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hotel-fahmi\" does not have a read-only root file system"
  },
  {
    "id": "01697",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hotel-fahmi\" is not set to runAsNonRoot"
  },
  {
    "id": "01698",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hotel-fahmi\" has cpu request 0"
  },
  {
    "id": "01699",
    "manifest_path": "data/manifests/the_stack_sample/sample_0574.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: hotel-fahmi-server-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      framework: codeigniter-4\n      language: php\n      type: server\n      app: hotel-fahmi-server\n  template:\n    metadata:\n      name: hotel-fahmi-server\n      labels:\n        framework: codeigniter-4\n        language: php\n        type: server\n        app: hotel-fahmi-server\n      annotations:\n        description: merupakan aplikasi hotel fahmi yang bersifat server untuk berintaraksi\n          dengan user\n    spec:\n      containers:\n      - name: hotel-fahmi\n        image: localhost:5000/hotel-fahmi\n        ports:\n        - containerPort: 80\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hotel-fahmi\" has memory limit 0"
  },
  {
    "id": "01700",
    "manifest_path": "data/manifests/the_stack_sample/sample_0575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-refiner-test\n  labels:\n    app: node-refiner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-refiner\n  template:\n    metadata:\n      labels:\n        app: node-refiner\n    spec:\n      serviceAccountName: node-refiner-sa\n      containers:\n      - name: application\n        image: alisoliman/node-refiner:${IMAGE_TAGGED}\n        imagePullPolicy: Always\n        ports:\n        - name: health\n          containerPort: 9102\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /alive\n            port: health\n        env:\n        - name: LISTENING_PORT\n          value: '8080'\n        resources:\n          requests:\n            cpu: 50m\n            memory: 256Mi\n          limits:\n            cpu: 200m\n            memory: 512Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"application\" does not have a read-only root file system"
  },
  {
    "id": "01701",
    "manifest_path": "data/manifests/the_stack_sample/sample_0575.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: node-refiner-test\n  labels:\n    app: node-refiner\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: node-refiner\n  template:\n    metadata:\n      labels:\n        app: node-refiner\n    spec:\n      serviceAccountName: node-refiner-sa\n      containers:\n      - name: application\n        image: alisoliman/node-refiner:${IMAGE_TAGGED}\n        imagePullPolicy: Always\n        ports:\n        - name: health\n          containerPort: 9102\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /alive\n            port: health\n        env:\n        - name: LISTENING_PORT\n          value: '8080'\n        resources:\n          requests:\n            cpu: 50m\n            memory: 256Mi\n          limits:\n            cpu: 200m\n            memory: 512Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"application\" is not set to runAsNonRoot"
  },
  {
    "id": "01702",
    "manifest_path": "data/manifests/the_stack_sample/sample_0582.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mattermost\n    role: mattermost-worker\n  name: mattermost-worker\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: mattermost-worker\n  template:\n    metadata:\n      labels:\n        app: mattermost\n        role: mattermost-worker\n    spec:\n      containers:\n      - image: __REGISTRY_IP__/mattermost-worker:5.21.0\n        name: mattermost-worker\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        volumeMounts:\n        - name: config-volume\n          mountPath: /var/mattermost/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: mattermost-v1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mattermost-worker\" does not have a read-only root file system"
  },
  {
    "id": "01703",
    "manifest_path": "data/manifests/the_stack_sample/sample_0582.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mattermost\n    role: mattermost-worker\n  name: mattermost-worker\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: mattermost-worker\n  template:\n    metadata:\n      labels:\n        app: mattermost\n        role: mattermost-worker\n    spec:\n      containers:\n      - image: __REGISTRY_IP__/mattermost-worker:5.21.0\n        name: mattermost-worker\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        volumeMounts:\n        - name: config-volume\n          mountPath: /var/mattermost/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: mattermost-v1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mattermost-worker\" is not set to runAsNonRoot"
  },
  {
    "id": "01704",
    "manifest_path": "data/manifests/the_stack_sample/sample_0582.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mattermost\n    role: mattermost-worker\n  name: mattermost-worker\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: mattermost-worker\n  template:\n    metadata:\n      labels:\n        app: mattermost\n        role: mattermost-worker\n    spec:\n      containers:\n      - image: __REGISTRY_IP__/mattermost-worker:5.21.0\n        name: mattermost-worker\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        volumeMounts:\n        - name: config-volume\n          mountPath: /var/mattermost/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: mattermost-v1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mattermost-worker\" has cpu request 0"
  },
  {
    "id": "01705",
    "manifest_path": "data/manifests/the_stack_sample/sample_0582.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mattermost\n    role: mattermost-worker\n  name: mattermost-worker\n  namespace: default\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      role: mattermost-worker\n  template:\n    metadata:\n      labels:\n        app: mattermost\n        role: mattermost-worker\n    spec:\n      containers:\n      - image: __REGISTRY_IP__/mattermost-worker:5.21.0\n        name: mattermost-worker\n        ports:\n        - containerPort: 80\n          protocol: TCP\n        volumeMounts:\n        - name: config-volume\n          mountPath: /var/mattermost/config\n      volumes:\n      - name: config-volume\n        configMap:\n          name: mattermost-v1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mattermost-worker\" has memory limit 0"
  },
  {
    "id": "01706",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"aws-efa-k8s-device-plugin\" does not have a read-only root file system"
  },
  {
    "id": "01707",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"aws-efa-k8s-device-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "01708",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"aws-efa-k8s-device-plugin\" has cpu request 0"
  },
  {
    "id": "01709",
    "manifest_path": "data/manifests/the_stack_sample/sample_0583.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: aws-efa-k8s-device-plugin-daemonset\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      name: aws-efa-k8s-device-plugin\n  template:\n    metadata:\n      annotations:\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n      labels:\n        name: aws-efa-k8s-device-plugin\n    spec:\n      serviceAccount: default\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: beta.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n            - matchExpressions:\n              - key: node.kubernetes.io/instance-type\n                operator: In\n                values:\n                - c5n.18xlarge\n                - c5n.metal\n                - g4dn.metal\n                - i3en.24xlarge\n                - i3en.metal\n                - inf1.24xlarge\n                - m5dn.24xlarge\n                - m5n.24xlarge\n                - p3dn.24xlarge\n                - r5dn.24xlarge\n                - r5n.24xlarge\n                - p4d.24xlarge\n      containers:\n      - image: '%s.dkr.ecr.%s.%s/eks/aws-efa-k8s-device-plugin:v0.3.3'\n        name: aws-efa-k8s-device-plugin\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: device-plugin\n          mountPath: /var/lib/kubelet/device-plugins\n      volumes:\n      - name: device-plugin\n        hostPath:\n          path: /var/lib/kubelet/device-plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"aws-efa-k8s-device-plugin\" has memory limit 0"
  },
  {
    "id": "01710",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "01711",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "01712",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "01713",
    "manifest_path": "data/manifests/the_stack_sample/sample_0584.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20210421-8709509fc9\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubernetes-blob-storage-workers=1\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "01714",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx/alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01715",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01716",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01717",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01718",
    "manifest_path": "data/manifests/the_stack_sample/sample_0587.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\n  labels:\n    app: nginx\nspec:\n  containers:\n  - name: nginx\n    image: nginx/alpine\n    ports:\n    - containerPort: 80\n    readinessProbe:\n      httpGet:\n        path: /\n        port: 80\n      initialDelaySeconds: 5\n      periodSeconds: 10\n    livenessProbe:\n      tcpSocket:\n        port: 80\n      initialDelaySeconds: 15\n      periodSeconds: 20\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01719",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01720",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01721",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01722",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01723",
    "manifest_path": "data/manifests/the_stack_sample/sample_0588.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-flexvolume-driver-disallowed\n  labels:\n    app: nginx-flexvolume-driver\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n    volumeMounts:\n    - mountPath: /test\n      name: test\n      readOnly: true\n  volumes:\n  - name: test\n    flexVolume:\n      driver: example/customdriver\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01724",
    "manifest_path": "data/manifests/the_stack_sample/sample_0590.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200812-8936af3bd4\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "01725",
    "manifest_path": "data/manifests/the_stack_sample/sample_0590.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200812-8936af3bd4\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "01726",
    "manifest_path": "data/manifests/the_stack_sample/sample_0590.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200812-8936af3bd4\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "01727",
    "manifest_path": "data/manifests/the_stack_sample/sample_0590.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200812-8936af3bd4\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "01728",
    "manifest_path": "data/manifests/the_stack_sample/sample_0592.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo-posts\n  name: mongo-posts\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo-posts\n  template:\n    metadata:\n      labels:\n        app: mongo-posts\n    spec:\n      containers:\n      - image: mongo:4.4.6\n        name: mongo-posts\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir-posts\n          mountPath: /data/db\n      volumes:\n      - name: mongo-data-dir-posts\n        persistentVolumeClaim:\n          claimName: mongo-data-posts\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mongo-posts\" does not have a read-only root file system"
  },
  {
    "id": "01729",
    "manifest_path": "data/manifests/the_stack_sample/sample_0592.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo-posts\n  name: mongo-posts\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo-posts\n  template:\n    metadata:\n      labels:\n        app: mongo-posts\n    spec:\n      containers:\n      - image: mongo:4.4.6\n        name: mongo-posts\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir-posts\n          mountPath: /data/db\n      volumes:\n      - name: mongo-data-dir-posts\n        persistentVolumeClaim:\n          claimName: mongo-data-posts\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mongo-posts\" is not set to runAsNonRoot"
  },
  {
    "id": "01730",
    "manifest_path": "data/manifests/the_stack_sample/sample_0592.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo-posts\n  name: mongo-posts\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo-posts\n  template:\n    metadata:\n      labels:\n        app: mongo-posts\n    spec:\n      containers:\n      - image: mongo:4.4.6\n        name: mongo-posts\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir-posts\n          mountPath: /data/db\n      volumes:\n      - name: mongo-data-dir-posts\n        persistentVolumeClaim:\n          claimName: mongo-data-posts\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mongo-posts\" has cpu request 0"
  },
  {
    "id": "01731",
    "manifest_path": "data/manifests/the_stack_sample/sample_0592.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: mongo-posts\n  name: mongo-posts\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mongo-posts\n  template:\n    metadata:\n      labels:\n        app: mongo-posts\n    spec:\n      containers:\n      - image: mongo:4.4.6\n        name: mongo-posts\n        args:\n        - --dbpath\n        - /data/db\n        livenessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - mongo\n            - --disableImplicitSessions\n            - --eval\n            - db.adminCommand('ping')\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        env:\n        - name: MONGO_INITDB_ROOT_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: username\n        - name: MONGO_INITDB_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongo-creds\n              key: password\n        volumeMounts:\n        - name: mongo-data-dir-posts\n          mountPath: /data/db\n      volumes:\n      - name: mongo-data-dir-posts\n        persistentVolumeClaim:\n          claimName: mongo-data-posts\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mongo-posts\" has memory limit 0"
  },
  {
    "id": "01732",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"camel-k-operator\" does not have a read-only root file system"
  },
  {
    "id": "01733",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"camel-k-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "01734",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"camel-k-operator\" has cpu request 0"
  },
  {
    "id": "01735",
    "manifest_path": "data/manifests/the_stack_sample/sample_0593.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: camel-k-operator\n  labels:\n    app: camel-k\n    camel.apache.org/component: operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: camel-k-operator\n  template:\n    metadata:\n      labels:\n        name: camel-k-operator\n        camel.apache.org/component: operator\n        app: camel-k\n    spec:\n      serviceAccountName: camel-k-operator\n      containers:\n      - name: camel-k-operator\n        image: docker.io/apache/camel-k:1.9.0-SNAPSHOT\n        imagePullPolicy: IfNotPresent\n        command:\n        - kamel\n        - operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: OPERATOR_NAME\n          value: camel-k\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 20\n          periodSeconds: 10\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"camel-k-operator\" has memory limit 0"
  },
  {
    "id": "01736",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01737",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01738",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01739",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01740",
    "manifest_path": "data/manifests/the_stack_sample/sample_0594.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2987\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01741",
    "manifest_path": "data/manifests/the_stack_sample/sample_0596.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-slave\n  namespace: guestbook\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: slave\n      tier: backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: slave\n        tier: backend\n    spec:\n      containers:\n      - name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"slave\" does not have a read-only root file system"
  },
  {
    "id": "01742",
    "manifest_path": "data/manifests/the_stack_sample/sample_0596.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-slave\n  namespace: guestbook\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: slave\n      tier: backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: slave\n        tier: backend\n    spec:\n      containers:\n      - name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"slave\" is not set to runAsNonRoot"
  },
  {
    "id": "01743",
    "manifest_path": "data/manifests/the_stack_sample/sample_0596.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-slave\n  namespace: guestbook\nspec:\n  selector:\n    matchLabels:\n      app: redis\n      role: slave\n      tier: backend\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: slave\n        tier: backend\n    spec:\n      containers:\n      - name: slave\n        image: gcr.io/google_samples/gb-redisslave:v1\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        env:\n        - name: GET_HOSTS_FROM\n          value: dns\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"slave\" has memory limit 0"
  },
  {
    "id": "01744",
    "manifest_path": "data/manifests/the_stack_sample/sample_0599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lhci-server\nspec:\n  selector:\n    matchLabels:\n      name: lhci-server\n  template:\n    metadata:\n      name: lhci-pod\n      labels:\n        name: lhci-server\n    spec:\n      containers:\n      - name: lhci-server\n        image: docker.io/patrickhulce/lhci-server:0.6.1\n        volumeMounts:\n        - mountPath: /data\n          name: lhci-data-volume\n        ports:\n        - containerPort: 9001\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 9001\n      volumes:\n      - name: lhci-data-volume\n        persistentVolumeClaim:\n          claimName: lhci-data-claim\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lhci-server\" does not have a read-only root file system"
  },
  {
    "id": "01745",
    "manifest_path": "data/manifests/the_stack_sample/sample_0599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lhci-server\nspec:\n  selector:\n    matchLabels:\n      name: lhci-server\n  template:\n    metadata:\n      name: lhci-pod\n      labels:\n        name: lhci-server\n    spec:\n      containers:\n      - name: lhci-server\n        image: docker.io/patrickhulce/lhci-server:0.6.1\n        volumeMounts:\n        - mountPath: /data\n          name: lhci-data-volume\n        ports:\n        - containerPort: 9001\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 9001\n      volumes:\n      - name: lhci-data-volume\n        persistentVolumeClaim:\n          claimName: lhci-data-claim\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lhci-server\" is not set to runAsNonRoot"
  },
  {
    "id": "01746",
    "manifest_path": "data/manifests/the_stack_sample/sample_0599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lhci-server\nspec:\n  selector:\n    matchLabels:\n      name: lhci-server\n  template:\n    metadata:\n      name: lhci-pod\n      labels:\n        name: lhci-server\n    spec:\n      containers:\n      - name: lhci-server\n        image: docker.io/patrickhulce/lhci-server:0.6.1\n        volumeMounts:\n        - mountPath: /data\n          name: lhci-data-volume\n        ports:\n        - containerPort: 9001\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 9001\n      volumes:\n      - name: lhci-data-volume\n        persistentVolumeClaim:\n          claimName: lhci-data-claim\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"lhci-server\" has cpu request 0"
  },
  {
    "id": "01747",
    "manifest_path": "data/manifests/the_stack_sample/sample_0599.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lhci-server\nspec:\n  selector:\n    matchLabels:\n      name: lhci-server\n  template:\n    metadata:\n      name: lhci-pod\n      labels:\n        name: lhci-server\n    spec:\n      containers:\n      - name: lhci-server\n        image: docker.io/patrickhulce/lhci-server:0.6.1\n        volumeMounts:\n        - mountPath: /data\n          name: lhci-data-volume\n        ports:\n        - containerPort: 9001\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 9001\n      volumes:\n      - name: lhci-data-volume\n        persistentVolumeClaim:\n          claimName: lhci-data-claim\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"lhci-server\" has memory limit 0"
  },
  {
    "id": "01748",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"still\" is using an invalid container image, \"hdghg/still:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01749",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"still\" does not have a read-only root file system"
  },
  {
    "id": "01750",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"still\" is not set to runAsNonRoot"
  },
  {
    "id": "01751",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"still\" has cpu request 0"
  },
  {
    "id": "01752",
    "manifest_path": "data/manifests/the_stack_sample/sample_0601.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: still-deployment\n  labels:\n    app: still\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: still\n  template:\n    metadata:\n      labels:\n        app: still\n    spec:\n      containers:\n      - name: still\n        image: hdghg/still:latest\n        env:\n        - name: BOOTSTRAP_SERVERS\n          value: kafka-0.kafka-headless.default.svc.cluster.local:9092\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"still\" has memory limit 0"
  },
  {
    "id": "01753",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rbac-permissions-operator\" is using an invalid container image, \"quay.io/app-sre/rbac-permissions-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01754",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rbac-permissions-operator\" does not have a read-only root file system"
  },
  {
    "id": "01755",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rbac-permissions-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "01756",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rbac-permissions-operator\" has cpu request 0"
  },
  {
    "id": "01757",
    "manifest_path": "data/manifests/the_stack_sample/sample_0602.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rbac-permissions-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: rbac-permissions-operator\n  template:\n    metadata:\n      labels:\n        name: rbac-permissions-operator\n    spec:\n      serviceAccountName: rbac-permissions-operator\n      containers:\n      - name: rbac-permissions-operator\n        image: quay.io/app-sre/rbac-permissions-operator:latest\n        command:\n        - rbac-permissions-operator\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: rbac-permissions-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rbac-permissions-operator\" has memory limit 0"
  },
  {
    "id": "01758",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mysql\" is using an invalid container image, \"mysql\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01759",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mysql\" does not have a read-only root file system"
  },
  {
    "id": "01760",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mysql\" is not set to runAsNonRoot"
  },
  {
    "id": "01761",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mysql\" has cpu request 0"
  },
  {
    "id": "01762",
    "manifest_path": "data/manifests/the_stack_sample/sample_0607.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-server\n  labels:\n    app: python\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: python\n      tier: mysql\n  template:\n    metadata:\n      labels:\n        app: python\n        tier: mysql\n    spec:\n      containers:\n      - image: mysql\n        name: mysql\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysecret\n              key: key\n        ports:\n        - containerPort: 3306\n          name: mysql\n        volumeMounts:\n        - name: mysql-persistent-storage\n          mountPath: /var/lib/mysql\n      volumes:\n      - name: mysql-persistent-storage\n        persistentVolumeClaim:\n          claimName: mysql-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mysql\" has memory limit 0"
  },
  {
    "id": "01763",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"consul\" does not have a read-only root file system"
  },
  {
    "id": "01764",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"consul-exporter\" does not have a read-only root file system"
  },
  {
    "id": "01765",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"consul\" is not set to runAsNonRoot"
  },
  {
    "id": "01766",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"consul-exporter\" is not set to runAsNonRoot"
  },
  {
    "id": "01767",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"consul-exporter\" has cpu request 0"
  },
  {
    "id": "01768",
    "manifest_path": "data/manifests/the_stack_sample/sample_0610.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: consul\n  namespace: consul-back\nspec:\n  selector:\n    matchLabels:\n      app: consul\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: consul\n        storageClassName: rook-ceph-block\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - consul\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: consul\n      securityContext:\n        fsGroup: 1000\n      containers:\n      - name: consul-exporter\n        args:\n        - --consul.server=127.0.0.1:8500\n        - --consul.ca-file=/etc/tls/ca.pem\n        - --consul.cert-file=/etc/tls/consul-back.pem\n        - --consul.key-file=/etc/tls/consul-back-key.pem\n        image: prom/consul-exporter:v0.4.0\n        ports:\n        - name: metrics\n          containerPort: 9107\n        volumeMounts:\n        - mountPath: /etc/tls\n          name: tls\n      - name: consul\n        image: consul:1.6.1\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GOSSIP_ENCRYPTION_KEY\n          valueFrom:\n            secretKeyRef:\n              name: consul\n              key: gossip-encryption-key\n        - name: CONSUL_HTTP_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: tokens\n              key: acl.tokens.agent\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        args:\n        - consul\n        - agent\n        - -advertise=$(POD_IP)\n        - -bind=0.0.0.0\n        - -bootstrap-expect=3\n        - -retry-join\n        - provider=k8s namespace=$(NAMESPACE) label_selector=\"app=consul\"\n        - -client=0.0.0.0\n        - -config-file=/consul/myconfig/config.json\n        - -datacenter=dc1\n        - -data-dir=/consul/data\n        - -domain=cluster.local\n        - -encrypt=$(GOSSIP_ENCRYPTION_KEY)\n        - -server\n        - -ui\n        - -disable-host-node-id\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - 'curl http://127.0.0.1:8500/v1/status/leader 2>/dev/null | \\\n\n              grep -E ''\".+\"''\n\n              '\n          failureThreshold: 2\n          initialDelaySeconds: 120\n          periodSeconds: 5\n          successThreshold: 2\n          timeoutSeconds: 5\n        livenessProbe:\n          exec:\n            command:\n            - consul\n            - members\n            - -http-addr=http://127.0.0.1:8500\n          initialDelaySeconds: 300\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: data\n          mountPath: /consul/data\n        - name: tls\n          mountPath: /etc/tls\n        - name: config\n          mountPath: /consul/myconfig\n        resources:\n          requests:\n            memory: 128Mi\n            cpu: 250m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n        ports:\n        - containerPort: 8500\n          name: ui-port\n        - containerPort: 8400\n          name: alt-port\n        - containerPort: 53\n          name: udp-port\n        - containerPort: 8443\n          name: https-port\n        - containerPort: 8080\n          name: http-port\n        - containerPort: 8301\n          name: serflan\n        - containerPort: 8302\n          name: serfwan\n        - containerPort: 8600\n          name: consuldns\n        - containerPort: 8300\n          name: server\n      volumes:\n      - name: tls\n        secret:\n          secretName: consul\n      - name: tokens\n        secret:\n          secretName: tokens\n      - name: config\n        configMap:\n          name: consul\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"consul-exporter\" has memory limit 0"
  },
  {
    "id": "01769",
    "manifest_path": "data/manifests/the_stack_sample/sample_0611.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-serving-install\n    app.kubernetes.io/instance: knative-serving-install-v0.11.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: knative-serving-install\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: v0.11.1\n    kustomize.component: knative\n    serving.knative.dev/release: v0.11.1\n  name: activator\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      app: activator\n      app.kubernetes.io/component: knative-serving-install\n      app.kubernetes.io/instance: knative-serving-install-v0.11.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: knative-serving-install\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: v0.11.1\n      kustomize.component: knative\n      role: activator\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'true'\n      labels:\n        app: activator\n        app.kubernetes.io/component: knative-serving-install\n        app.kubernetes.io/instance: knative-serving-install-v0.11.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: knative-serving-install\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: v0.11.1\n        kustomize.component: knative\n        role: activator\n        serving.knative.dev/release: v0.11.1\n    spec:\n      containers:\n      - env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/internal/serving\n        image: gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:8e606671215cc029683e8cd633ec5de9eabeaa6e9a4392ff289883304be1f418\n        livenessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        name: activator\n        ports:\n        - containerPort: 8012\n          name: http1\n        - containerPort: 8013\n          name: h2c\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        readinessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 600Mi\n          requests:\n            cpu: 300m\n            memory: 60Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: controller\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"activator\" does not have a read-only root file system"
  },
  {
    "id": "01770",
    "manifest_path": "data/manifests/the_stack_sample/sample_0611.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: knative-serving-install\n    app.kubernetes.io/instance: knative-serving-install-v0.11.1\n    app.kubernetes.io/managed-by: kfctl\n    app.kubernetes.io/name: knative-serving-install\n    app.kubernetes.io/part-of: kubeflow\n    app.kubernetes.io/version: v0.11.1\n    kustomize.component: knative\n    serving.knative.dev/release: v0.11.1\n  name: activator\n  namespace: knative-serving\nspec:\n  selector:\n    matchLabels:\n      app: activator\n      app.kubernetes.io/component: knative-serving-install\n      app.kubernetes.io/instance: knative-serving-install-v0.11.1\n      app.kubernetes.io/managed-by: kfctl\n      app.kubernetes.io/name: knative-serving-install\n      app.kubernetes.io/part-of: kubeflow\n      app.kubernetes.io/version: v0.11.1\n      kustomize.component: knative\n      role: activator\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n        sidecar.istio.io/inject: 'true'\n      labels:\n        app: activator\n        app.kubernetes.io/component: knative-serving-install\n        app.kubernetes.io/instance: knative-serving-install-v0.11.1\n        app.kubernetes.io/managed-by: kfctl\n        app.kubernetes.io/name: knative-serving-install\n        app.kubernetes.io/part-of: kubeflow\n        app.kubernetes.io/version: v0.11.1\n        kustomize.component: knative\n        role: activator\n        serving.knative.dev/release: v0.11.1\n    spec:\n      containers:\n      - env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: knative.dev/internal/serving\n        image: gcr.io/knative-releases/knative.dev/serving/cmd/activator@sha256:8e606671215cc029683e8cd633ec5de9eabeaa6e9a4392ff289883304be1f418\n        livenessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        name: activator\n        ports:\n        - containerPort: 8012\n          name: http1\n        - containerPort: 8013\n          name: h2c\n        - containerPort: 9090\n          name: metrics\n        - containerPort: 8008\n          name: profiling\n        readinessProbe:\n          httpGet:\n            httpHeaders:\n            - name: k-kubelet-probe\n              value: activator\n            path: /healthz\n            port: 8012\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 600Mi\n          requests:\n            cpu: 300m\n            memory: 60Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      serviceAccountName: controller\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"activator\" is not set to runAsNonRoot"
  },
  {
    "id": "01771",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"conbot\" is using an invalid container image, \"conbot/conbot:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01772",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"conbot\" does not have a read-only root file system"
  },
  {
    "id": "01773",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"conbot\" is not set to runAsNonRoot"
  },
  {
    "id": "01774",
    "manifest_path": "data/manifests/the_stack_sample/sample_0612.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: demojob2-workerpool\n  labels:\n    poolname: workerpool\nspec:\n  template:\n    metadata:\n      labels:\n        poolname: workerpool\n    spec:\n      containers:\n      - name: conbot\n        resources:\n          requests:\n            cpu: '6'\n        image: conbot/conbot:latest\n        imagePullPolicy: Always\n        args:\n        - convert\n        - -source\n        - gs://conbot-data/source/10m.csv.gz\n        - -target\n        - /output\n        - -consumer\n        - csv\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"conbot\" has memory limit 0"
  },
  {
    "id": "01775",
    "manifest_path": "data/manifests/the_stack_sample/sample_0614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 1.9.5\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 1.9.5\n    spec:\n      containers:\n      - image: quay.io/coreos/kube-state-metrics:v1.9.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "01776",
    "manifest_path": "data/manifests/the_stack_sample/sample_0614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 1.9.5\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 1.9.5\n    spec:\n      containers:\n      - image: quay.io/coreos/kube-state-metrics:v1.9.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "01777",
    "manifest_path": "data/manifests/the_stack_sample/sample_0614.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: 1.9.5\n  name: kube-state-metrics\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: 1.9.5\n    spec:\n      containers:\n      - image: quay.io/coreos/kube-state-metrics:v1.9.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        name: kube-state-metrics\n        ports:\n        - containerPort: 8080\n          name: http-metrics\n        - containerPort: 8081\n          name: telemetry\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8081\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        securityContext:\n          runAsUser: 65534\n      serviceAccountName: kube-state-metrics\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "01778",
    "manifest_path": "data/manifests/the_stack_sample/sample_0616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: ubuntu:16.04\n    args:\n    - sh\n    - -c\n    - sleep 10; true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"pod2\" does not have a read-only root file system"
  },
  {
    "id": "01779",
    "manifest_path": "data/manifests/the_stack_sample/sample_0616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: ubuntu:16.04\n    args:\n    - sh\n    - -c\n    - sleep 10; true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"pod2\" is not set to runAsNonRoot"
  },
  {
    "id": "01780",
    "manifest_path": "data/manifests/the_stack_sample/sample_0616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: ubuntu:16.04\n    args:\n    - sh\n    - -c\n    - sleep 10; true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"pod2\" has cpu request 0"
  },
  {
    "id": "01781",
    "manifest_path": "data/manifests/the_stack_sample/sample_0616.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod2\nspec:\n  containers:\n  - name: pod2\n    image: ubuntu:16.04\n    args:\n    - sh\n    - -c\n    - sleep 10; true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"pod2\" has memory limit 0"
  },
  {
    "id": "01782",
    "manifest_path": "data/manifests/the_stack_sample/sample_0621.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: hello\n  name: hello\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: hello\n  template:\n    metadata:\n      labels:\n        run: hello\n    spec:\n      containers:\n      - image: k8s.gcr.io/echoserver:1.9\n        name: hello\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "01783",
    "manifest_path": "data/manifests/the_stack_sample/sample_0621.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: hello\n  name: hello\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: hello\n  template:\n    metadata:\n      labels:\n        run: hello\n    spec:\n      containers:\n      - image: k8s.gcr.io/echoserver:1.9\n        name: hello\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "01784",
    "manifest_path": "data/manifests/the_stack_sample/sample_0621.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: hello\n  name: hello\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: hello\n  template:\n    metadata:\n      labels:\n        run: hello\n    spec:\n      containers:\n      - image: k8s.gcr.io/echoserver:1.9\n        name: hello\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"hello\" has cpu request 0"
  },
  {
    "id": "01785",
    "manifest_path": "data/manifests/the_stack_sample/sample_0621.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    run: hello\n  name: hello\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: hello\n  template:\n    metadata:\n      labels:\n        run: hello\n    spec:\n      containers:\n      - image: k8s.gcr.io/echoserver:1.9\n        name: hello\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"hello\" has memory limit 0"
  },
  {
    "id": "01786",
    "manifest_path": "data/manifests/the_stack_sample/sample_0623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubeecho-folder\n  labels:\n    app: kubeecho-folder\nspec:\n  selector:\n    matchLabels:\n      app: kubeecho-folder\n  template:\n    metadata:\n      labels:\n        app: kubeecho-folder\n    spec:\n      containers:\n      - name: kubeecho-folder\n        image: emrahkk/kube-echo:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3000\n          name: http\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 3000\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kubeecho-folder\" is using an invalid container image, \"emrahkk/kube-echo:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01787",
    "manifest_path": "data/manifests/the_stack_sample/sample_0623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubeecho-folder\n  labels:\n    app: kubeecho-folder\nspec:\n  selector:\n    matchLabels:\n      app: kubeecho-folder\n  template:\n    metadata:\n      labels:\n        app: kubeecho-folder\n    spec:\n      containers:\n      - name: kubeecho-folder\n        image: emrahkk/kube-echo:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3000\n          name: http\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 3000\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubeecho-folder\" does not have a read-only root file system"
  },
  {
    "id": "01788",
    "manifest_path": "data/manifests/the_stack_sample/sample_0623.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kubeecho-folder\n  labels:\n    app: kubeecho-folder\nspec:\n  selector:\n    matchLabels:\n      app: kubeecho-folder\n  template:\n    metadata:\n      labels:\n        app: kubeecho-folder\n    spec:\n      containers:\n      - name: kubeecho-folder\n        image: emrahkk/kube-echo:latest\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 3000\n          name: http\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 3000\n        readinessProbe:\n          httpGet:\n            path: /readyz\n            port: 3000\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubeecho-folder\" is not set to runAsNonRoot"
  },
  {
    "id": "01789",
    "manifest_path": "data/manifests/the_stack_sample/sample_0626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-recommender\n  namespace: kube-system\n  labels:\n    application: vpa-recommender\n    version: patched-0.5.1-master-16\n    component: vpa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: vpa-recommender\n  template:\n    metadata:\n      labels:\n        application: vpa-recommender\n        version: patched-0.5.1-master-16\n        component: vpa\n    spec:\n      serviceAccountName: system\n      containers:\n      - name: recommender\n        image: registry.opensource.zalan.do/teapot/vpa-recommender:patched-0.5.1-master-16\n        args:\n        - --stderrthreshold=info\n        - --v=5\n        - --memory-saver\n        - --pod-recommendation-min-memory-mb=50\n        command:\n        - /recommender\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"recommender\" does not have a read-only root file system"
  },
  {
    "id": "01790",
    "manifest_path": "data/manifests/the_stack_sample/sample_0626.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vpa-recommender\n  namespace: kube-system\n  labels:\n    application: vpa-recommender\n    version: patched-0.5.1-master-16\n    component: vpa\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: vpa-recommender\n  template:\n    metadata:\n      labels:\n        application: vpa-recommender\n        version: patched-0.5.1-master-16\n        component: vpa\n    spec:\n      serviceAccountName: system\n      containers:\n      - name: recommender\n        image: registry.opensource.zalan.do/teapot/vpa-recommender:patched-0.5.1-master-16\n        args:\n        - --stderrthreshold=info\n        - --v=5\n        - --memory-saver\n        - --pod-recommendation-min-memory-mb=50\n        command:\n        - /recommender\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 200m\n            memory: 500Mi\n        ports:\n        - containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"recommender\" is not set to runAsNonRoot"
  },
  {
    "id": "01791",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01792",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01793",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01794",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01795",
    "manifest_path": "data/manifests/the_stack_sample/sample_0630.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: devopsgirls-pod\n  labels:\n    name: devopsgirls-pod\nspec:\n  containers:\n  - image: nginx:latest\n    name: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01796",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"default-demo-ctr\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01797",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"default-demo-ctr\" does not have a read-only root file system"
  },
  {
    "id": "01798",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"default-demo-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "01799",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"default-demo-ctr\" has cpu request 0"
  },
  {
    "id": "01800",
    "manifest_path": "data/manifests/the_stack_sample/sample_0633.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: default-demo\nspec:\n  containers:\n  - name: default-demo-ctr\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"default-demo-ctr\" has memory limit 0"
  },
  {
    "id": "01801",
    "manifest_path": "data/manifests/the_stack_sample/sample_0635.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: controller\n  namespace: maths\n  labels:\n    maths.tableflip.dev/release: devel\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: controller\n  template:\n    metadata:\n      labels:\n        app: controller\n        maths.tableflip.dev/release: devel\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: controller\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      serviceAccountName: controller\n      containers:\n      - name: controller\n        image: ko://tableflip.dev/maths/cmd/controller\n        resources:\n          requests:\n            cpu: 100m\n            memory: 100Mi\n          limits:\n            cpu: 1000m\n            memory: 1000Mi\n        ports:\n        - name: metrics\n          containerPort: 9090\n        env:\n        - name: SYSTEM_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CONFIG_LOGGING_NAME\n          value: config-logging\n        - name: CONFIG_OBSERVABILITY_NAME\n          value: config-observability\n        - name: METRICS_DOMAIN\n          value: tableflip.dev/maths\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - all\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"controller\" is using an invalid container image, \"ko://tableflip.dev/maths/cmd/controller\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01802",
    "manifest_path": "data/manifests/the_stack_sample/sample_0638.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: magazines\n  annotations:\n    sidecar.jaegertracing.io/inject: 'true'\n  labels:\n    app: swir\n    swir: magazines\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: swir\n  template:\n    metadata:\n      labels:\n        app: swir\n    spec:\n      containers:\n      - name: client\n        image: swir/swir-example-si-python-http-server:v0.3.2\n        env:\n        - name: ip\n          value: 127.0.0.1\n        - name: port\n          value: '8090'\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"client\" does not have a read-only root file system"
  },
  {
    "id": "01803",
    "manifest_path": "data/manifests/the_stack_sample/sample_0638.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: magazines\n  annotations:\n    sidecar.jaegertracing.io/inject: 'true'\n  labels:\n    app: swir\n    swir: magazines\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: swir\n  template:\n    metadata:\n      labels:\n        app: swir\n    spec:\n      containers:\n      - name: client\n        image: swir/swir-example-si-python-http-server:v0.3.2\n        env:\n        - name: ip\n          value: 127.0.0.1\n        - name: port\n          value: '8090'\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"client\" is not set to runAsNonRoot"
  },
  {
    "id": "01804",
    "manifest_path": "data/manifests/the_stack_sample/sample_0638.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: magazines\n  annotations:\n    sidecar.jaegertracing.io/inject: 'true'\n  labels:\n    app: swir\n    swir: magazines\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: swir\n  template:\n    metadata:\n      labels:\n        app: swir\n    spec:\n      containers:\n      - name: client\n        image: swir/swir-example-si-python-http-server:v0.3.2\n        env:\n        - name: ip\n          value: 127.0.0.1\n        - name: port\n          value: '8090'\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"client\" has cpu request 0"
  },
  {
    "id": "01805",
    "manifest_path": "data/manifests/the_stack_sample/sample_0638.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: magazines\n  annotations:\n    sidecar.jaegertracing.io/inject: 'true'\n  labels:\n    app: swir\n    swir: magazines\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: swir\n  template:\n    metadata:\n      labels:\n        app: swir\n    spec:\n      containers:\n      - name: client\n        image: swir/swir-example-si-python-http-server:v0.3.2\n        env:\n        - name: ip\n          value: 127.0.0.1\n        - name: port\n          value: '8090'\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"client\" has memory limit 0"
  },
  {
    "id": "01806",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "01807",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "01808",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "01809",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "01810",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "01811",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "01812",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "01813",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "01814",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "01815",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "01816",
    "manifest_path": "data/manifests/the_stack_sample/sample_0644.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-nightly-maskrcnn-func-v3-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: nightly\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/detection/main.py\n          - --model=mask_rcnn\n          - \"--params_override=\\\"architecture\\\":\\n  \\\"use_bfloat16\\\": true\\n\\\"eval\\\"\\\n            :\\n  \\\"batch_size\\\": 40\\n  \\\"eval_file_pattern\\\": \\\"$(COCO_DIR)/val*\\\"\\\n            \\n  \\\"val_json_file\\\": \\\"$(COCO_DIR)/instances_val2017.json\\\"\\n\\\"postprocess\\\"\\\n            :\\n  \\\"pre_nms_num_boxes\\\": 1000\\n\\\"predict\\\":\\n  \\\"batch_size\\\": 40\\n\\\n            \\\"train\\\":\\n  \\\"batch_size\\\": 64\\n  \\\"checkpoint\\\":\\n    \\\"path\\\": \\\"\\\n            $(RESNET_PRETRAIN_DIR)/resnet50-checkpoint-2018-02-07\\\"\\n    \\\"prefix\\\"\\\n            : \\\"resnet50/\\\"\\n  \\\"iterations_per_loop\\\": 5000\\n  \\\"total_steps\\\": 1000\\n\\\n            \\  \\\"train_file_pattern\\\": \\\"$(COCO_DIR)/train*\\\"\\n\"\n          - --model_dir=$(MODEL_DIR)\n          - --mode=train\n          - --strategy_type=tpu\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:nightly\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/preemptible-v3: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-nightly/maskrcnn/func/v3-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 4\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-nightly-maskrcnn-func-v3-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "01817",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"job\" is using an invalid container image, \"luksa/batch-job\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01818",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job\" does not have a read-only root file system"
  },
  {
    "id": "01819",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job\" is not set to runAsNonRoot"
  },
  {
    "id": "01820",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"job\" has cpu request 0"
  },
  {
    "id": "01821",
    "manifest_path": "data/manifests/the_stack_sample/sample_0645.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: example-job-every-quarter\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          app: periodic-job\n      spec:\n        containers:\n        - name: job\n          image: luksa/batch-job\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"job\" has memory limit 0"
  },
  {
    "id": "01822",
    "manifest_path": "data/manifests/the_stack_sample/sample_0647.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.5.10\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 6d3939d6f6959c729794e49116e266b7584fcbf15dbabd0db30f725dad149252\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.5.10\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: dineshgitbot\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.10\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 8f23a5446f6f026f5395d53298638faec245136e\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "01823",
    "manifest_path": "data/manifests/the_stack_sample/sample_0647.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.5.10\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: 6d3939d6f6959c729794e49116e266b7584fcbf15dbabd0db30f725dad149252\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.5.10\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: dineshgitbot\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.10\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 8f23a5446f6f026f5395d53298638faec245136e\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "01824",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"prometheus\" is using an invalid container image, \"prom/prometheus\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01825",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus\" does not have a read-only root file system"
  },
  {
    "id": "01826",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"prometheus\" is not set to runAsNonRoot"
  },
  {
    "id": "01827",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"prometheus\" has cpu request 0"
  },
  {
    "id": "01828",
    "manifest_path": "data/manifests/the_stack_sample/sample_0650.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  labels:\n    app: prometheus\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: prometheus\n  template:\n    metadata:\n      labels:\n        app: prometheus\n        version: '3'\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9090'\n    spec:\n      containers:\n      - name: prometheus\n        image: prom/prometheus\n        args:\n        - --storage.tsdb.retention=6h\n        - --storage.tsdb.path=/prometheus\n        - --config.file=/etc/prometheus/prometheus.yml\n        ports:\n        - name: web\n          containerPort: 9090\n        volumeMounts:\n        - name: prometheus-config-volume\n          mountPath: /etc/prometheus\n        - name: prometheus-storage-volume\n          mountPath: /prometheus\n      volumes:\n      - name: prometheus-config-volume\n        configMap:\n          defaultMode: 420\n          name: prometheus-config\n      - name: prometheus-storage-volume\n        persistentVolumeClaim:\n          claimName: pvc-nfs-data\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"prometheus\" has memory limit 0"
  },
  {
    "id": "01829",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01830",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "01831",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "01832",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "01833",
    "manifest_path": "data/manifests/the_stack_sample/sample_0653.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6008\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "01834",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kubernetes-dashboard-not-secure\" does not have a read-only root file system"
  },
  {
    "id": "01835",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kubernetes-dashboard-not-secure\" is not set to runAsNonRoot"
  },
  {
    "id": "01836",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kubernetes-dashboard-not-secure\" has cpu request 0"
  },
  {
    "id": "01837",
    "manifest_path": "data/manifests/the_stack_sample/sample_0655.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: kubernetes-dashboard-not-secure\n  name: kubernetes-dashboard-not-secure\n  namespace: kubernetes-dashboard\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: kubernetes-dashboard-not-secure\n  template:\n    metadata:\n      labels:\n        k8s-app: kubernetes-dashboard-not-secure\n    spec:\n      containers:\n      - name: kubernetes-dashboard-not-secure\n        image: kubernetesui/dashboard:v2.0.0-beta8\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 9090\n          protocol: TCP\n        args:\n        - --namespace=kubernetes-dashboard\n        - --enable-insecure-login=true\n        volumeMounts:\n        - name: kubernetes-dashboard-certs\n          mountPath: /certs\n        - mountPath: /tmp\n          name: tmp-volume\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /\n            port: 9090\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      volumes:\n      - name: kubernetes-dashboard-certs\n        secret:\n          secretName: kubernetes-dashboard-certs\n      - name: tmp-volume\n        emptyDir: {}\n      serviceAccountName: kubernetes-dashboard\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kubernetes-dashboard-not-secure\" has memory limit 0"
  },
  {
    "id": "01838",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01839",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "01840",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "01841",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "01842",
    "manifest_path": "data/manifests/the_stack_sample/sample_0657.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  namespace: voting-app\n  labels:\n    app: voting-app\n    name: redis-deployment\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: voting-app\n      name: redis-pod\n  template:\n    metadata:\n      labels:\n        app: voting-app\n        name: redis-pod\n    spec:\n      containers:\n      - image: redis\n        name: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "01843",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dispatchers\" is using an invalid container image, \"cycoresystems/dispatchers\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01844",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kamailio\" is using an invalid container image, \"atsip/kamailio\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01845",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"rtpproxy\" is using an invalid container image, \"atsip/rtpproxy\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "01846",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dispatchers\" does not have a read-only root file system"
  },
  {
    "id": "01847",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kamailio\" does not have a read-only root file system"
  },
  {
    "id": "01848",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rtpproxy\" does not have a read-only root file system"
  },
  {
    "id": "01849",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dispatchers\" is not set to runAsNonRoot"
  },
  {
    "id": "01850",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kamailio\" is not set to runAsNonRoot"
  },
  {
    "id": "01851",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rtpproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "01852",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"dispatchers\" has cpu request 0"
  },
  {
    "id": "01853",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kamailio\" has cpu request 0"
  },
  {
    "id": "01854",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rtpproxy\" has cpu request 0"
  },
  {
    "id": "01855",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dispatchers\" has memory limit 0"
  },
  {
    "id": "01856",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kamailio\" has memory limit 0"
  },
  {
    "id": "01857",
    "manifest_path": "data/manifests/the_stack_sample/sample_0659.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kamailio\n  namespace: voip\n  labels:\n    component: kamailio\nspec:\n  selector:\n    matchLabels:\n      component: kamailio\n  template:\n    metadata:\n      name: kamailio\n      labels:\n        component: kamailio\n    spec:\n      volumes:\n      - name: config\n      containers:\n      - name: kamailio\n        image: atsip/kamailio\n        env:\n        - name: CLOUD\n          value: gcp\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n      - name: rtpproxy\n        image: atsip/rtpproxy\n        env:\n        - name: CLOUD\n          value: gcp\n      - name: dispatchers\n        image: cycoresystems/dispatchers\n        command:\n        - /app\n        - -set\n        - voip:asterisk=1:5080\n        volumeMounts:\n        - name: config\n          mountPath: /data/kamailio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rtpproxy\" has memory limit 0"
  },
  {
    "id": "01858",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"bootstrap\" does not have a read-only root file system"
  },
  {
    "id": "01859",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cockroachdb\" does not have a read-only root file system"
  },
  {
    "id": "01860",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"bootstrap\" is not set to runAsNonRoot"
  },
  {
    "id": "01861",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cockroachdb\" is not set to runAsNonRoot"
  },
  {
    "id": "01862",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"bootstrap\" has cpu request 0"
  },
  {
    "id": "01863",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cockroachdb\" has cpu request 0"
  },
  {
    "id": "01864",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"bootstrap\" has memory limit 0"
  },
  {
    "id": "01865",
    "manifest_path": "data/manifests/the_stack_sample/sample_0663.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app: cockroachdb\n  name: cockroachdb\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: cockroachdb\n  template:\n    metadata:\n      labels:\n        app: cockroachdb\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - cockroachdb\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      containers:\n      - command:\n        - /bin/bash\n        - -ecx\n        - \"# The use of qualified `hostname -f` is crucial:\\n# Other nodes aren't\\\n          \\ able to look up the unqualified hostname.\\nCRARGS=(\\\"start\\\" \\\"--logtostderr\\\"\\\n          \\ \\\"--insecure\\\" \\\"--host\\\" \\\"$(hostname -f)\\\" \\\"--http-host\\\" \\\"0.0.0.0\\\"\\\n          )\\n# We only want to initialize a new cluster (by omitting the join flag)\\n\\\n          # if we're sure that we're the first node (i.e. index 0) and that\\n# there\\\n          \\ aren't any other nodes running as part of the cluster that\\n# this is\\\n          \\ supposed to be a part of (which indicates that a cluster\\n# already exists\\\n          \\ and we should make sure not to create a new one).\\n# It's fine to run\\\n          \\ without --join on a restart if there aren't any\\n# other nodes.\\nif [\\\n          \\ ! \\\"$(hostname)\\\" == \\\"cockroachdb-0\\\" ] || \\\\\\n   [ -e \\\"/cockroach/cockroach-data/cluster_exists_marker\\\"\\\n          \\ ]\\nthen\\n  # We don't join cockroachdb in order to avoid a node attempting\\n\\\n          \\  # to join itself, which currently doesn't work\\n  # (https://github.com/cockroachdb/cockroach/issues/9625).\\n\\\n          \\  CRARGS+=(\\\"--join\\\" \\\"cockroachdb-public\\\")\\nfi\\nexec /cockroach/cockroach\\\n          \\ ${CRARGS[*]}\\n\"\n        image: cockroachdb/cockroach:v1.1.0\n        imagePullPolicy: IfNotPresent\n        name: cockroachdb\n        ports:\n        - containerPort: 26257\n          name: grpc\n        - containerPort: 8080\n          name: http\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      initContainers:\n      - args:\n        - -on-start=/on-start.sh\n        - -service=cockroachdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        image: cockroachdb/cockroach-k8s-init:0.2\n        imagePullPolicy: IfNotPresent\n        name: bootstrap\n        volumeMounts:\n        - mountPath: /cockroach/cockroach-data\n          name: datadir\n      volumes:\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: datadir\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cockroachdb\" has memory limit 0"
  },
  {
    "id": "01866",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable JX_SECRET_SIDECAR in container \"job\" found"
  },
  {
    "id": "01867",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable JX_SECRET_TMP_DIR in container \"job\" found"
  },
  {
    "id": "01868",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"git-clone\" does not have a read-only root file system"
  },
  {
    "id": "01869",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"gsm\" does not have a read-only root file system"
  },
  {
    "id": "01870",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"job\" does not have a read-only root file system"
  },
  {
    "id": "01871",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"git-clone\" is not set to runAsNonRoot"
  },
  {
    "id": "01872",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"gsm\" is not set to runAsNonRoot"
  },
  {
    "id": "01873",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"job\" is not set to runAsNonRoot"
  },
  {
    "id": "01874",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"git-clone\" has cpu request 0"
  },
  {
    "id": "01875",
    "manifest_path": "data/manifests/the_stack_sample/sample_0664.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - make apply || make failed\n        command:\n        - /bin/sh\n        - -c\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        - name: JX_SECRET_SIDECAR\n          value: gsm\n        - name: JX_SECRET_TMP_DIR\n          value: /workspace/source/.jx-secrets\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        image: gcr.io/jenkinsxio/jx-boot:3.1.118\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      - command:\n        - versionStream/src/wait-for-complete.sh\n        image: google/cloud-sdk:slim\n        imagePullPolicy: Always\n        name: gsm\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      serviceAccountName: jx-boot-job\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"gsm\" has cpu request 0"
  }
]