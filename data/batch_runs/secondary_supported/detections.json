[
  {
    "id": "001",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n      ports:\n        - containerPort: 80\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "002",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n      ports:\n        - containerPort: 80\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "003",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n      ports:\n        - containerPort: 80\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "004",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n      ports:\n        - containerPort: 80\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "005",
    "manifest_path": "data/manifests/001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-latest\n  namespace: default\nspec:\n  containers:\n    - name: nginx\n      image: nginx:latest\n      ports:\n        - containerPort: 80\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "006",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu:22.04\n      securityContext:\n        privileged: true\n      command: [\"sleep\", \"3600\"]\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"ubuntu\" does not have a read-only root file system"
  },
  {
    "id": "007",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu:22.04\n      securityContext:\n        privileged: true\n      command: [\"sleep\", \"3600\"]\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"ubuntu\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "008",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu:22.04\n      securityContext:\n        privileged: true\n      command: [\"sleep\", \"3600\"]\n",
    "policy_id": "no_privileged",
    "violation_text": "container \"ubuntu\" is privileged"
  },
  {
    "id": "009",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu:22.04\n      securityContext:\n        privileged: true\n      command: [\"sleep\", \"3600\"]\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"ubuntu\" is not set to runAsNonRoot"
  },
  {
    "id": "010",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu:22.04\n      securityContext:\n        privileged: true\n      command: [\"sleep\", \"3600\"]\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"ubuntu\" has cpu request 0"
  },
  {
    "id": "011",
    "manifest_path": "data/manifests/002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: privileged-pod\n  namespace: default\nspec:\n  containers:\n    - name: ubuntu\n      image: ubuntu:22.04\n      securityContext:\n        privileged: true\n      command: [\"sleep\", \"3600\"]\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"ubuntu\" has memory limit 0"
  },
  {
    "id": "012",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n    - name: web\n      image: nginx:latest\n      ports:\n        - containerPort: 8080\n          hostPort: 30080\n      securityContext:\n        capabilities:\n          add:\n            - NET_RAW\n            - SYS_ADMIN\n  volumes:\n    - name: host-data\n      hostPath:\n        path: /var/lib/data\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"web\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "013",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n    - name: web\n      image: nginx:latest\n      ports:\n        - containerPort: 8080\n          hostPort: 30080\n      securityContext:\n        capabilities:\n          add:\n            - NET_RAW\n            - SYS_ADMIN\n  volumes:\n    - name: host-data\n      hostPath:\n        path: /var/lib/data\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"web\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "014",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n    - name: web\n      image: nginx:latest\n      ports:\n        - containerPort: 8080\n          hostPort: 30080\n      securityContext:\n        capabilities:\n          add:\n            - NET_RAW\n            - SYS_ADMIN\n  volumes:\n    - name: host-data\n      hostPath:\n        path: /var/lib/data\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"web\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "015",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n    - name: web\n      image: nginx:latest\n      ports:\n        - containerPort: 8080\n          hostPort: 30080\n      securityContext:\n        capabilities:\n          add:\n            - NET_RAW\n            - SYS_ADMIN\n  volumes:\n    - name: host-data\n      hostPath:\n        path: /var/lib/data\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"web\" does not have a read-only root file system"
  },
  {
    "id": "016",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n    - name: web\n      image: nginx:latest\n      ports:\n        - containerPort: 8080\n          hostPort: 30080\n      securityContext:\n        capabilities:\n          add:\n            - NET_RAW\n            - SYS_ADMIN\n  volumes:\n    - name: host-data\n      hostPath:\n        path: /var/lib/data\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"web\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "017",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n    - name: web\n      image: nginx:latest\n      ports:\n        - containerPort: 8080\n          hostPort: 30080\n      securityContext:\n        capabilities:\n          add:\n            - NET_RAW\n            - SYS_ADMIN\n  volumes:\n    - name: host-data\n      hostPath:\n        path: /var/lib/data\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"web\" is not set to runAsNonRoot"
  },
  {
    "id": "018",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n    - name: web\n      image: nginx:latest\n      ports:\n        - containerPort: 8080\n          hostPort: 30080\n      securityContext:\n        capabilities:\n          add:\n            - NET_RAW\n            - SYS_ADMIN\n  volumes:\n    - name: host-data\n      hostPath:\n        path: /var/lib/data\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"web\" has cpu request 0"
  },
  {
    "id": "019",
    "manifest_path": "data/manifests/003.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: security-risk\n  namespace: default\nspec:\n  containers:\n    - name: web\n      image: nginx:latest\n      ports:\n        - containerPort: 8080\n          hostPort: 30080\n      securityContext:\n        capabilities:\n          add:\n            - NET_RAW\n            - SYS_ADMIN\n  volumes:\n    - name: host-data\n      hostPath:\n        path: /var/lib/data\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"web\" has memory limit 0"
  },
  {
    "id": "020",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/020_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "021",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/021_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "022",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/022_service_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  type: ClusterIP\n  selector:\n    tier: airflow\n    component: api-server\n    release: release-name\n  ports:\n  - name: api-server\n    port: 8080\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:api-server release:release-name tier:airflow])"
  },
  {
    "id": "023",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/023_service_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  type: ClusterIP\n  selector:\n    tier: airflow\n    component: redis\n    release: release-name\n  ports:\n  - name: redis-db\n    protocol: TCP\n    port: 6379\n    targetPort: 6379\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:redis release:release-name tier:airflow])"
  },
  {
    "id": "024",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/024_service_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9102'\nspec:\n  type: ClusterIP\n  selector:\n    tier: airflow\n    component: statsd\n    release: release-name\n  ports:\n  - name: statsd-ingest\n    protocol: UDP\n    port: 9125\n    targetPort: 9125\n  - name: statsd-scrape\n    protocol: TCP\n    port: 9102\n    targetPort: 9102\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:statsd release:release-name tier:airflow])"
  },
  {
    "id": "025",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/025_service_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  clusterIP: None\n  selector:\n    tier: airflow\n    component: triggerer\n    release: release-name\n  ports:\n  - name: triggerer-logs\n    protocol: TCP\n    port: 8794\n    targetPort: 8794\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:triggerer release:release-name tier:airflow])"
  },
  {
    "id": "026",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/026_service_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  clusterIP: None\n  selector:\n    tier: airflow\n    component: worker\n    release: release-name\n  ports:\n  - name: worker-logs\n    protocol: TCP\n    port: 8793\n    targetPort: 8793\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:worker release:release-name tier:airflow])"
  },
  {
    "id": "027",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"api-server\" does not have a read-only root file system"
  },
  {
    "id": "028",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "029",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-api-server\" not found"
  },
  {
    "id": "030",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"api-server\" has cpu request 0"
  },
  {
    "id": "031",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "032",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"api-server\" has memory limit 0"
  },
  {
    "id": "033",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/027_deployment_release-name-api-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-api-server\n  labels:\n    tier: airflow\n    component: api-server\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  selector:\n    matchLabels:\n      tier: airflow\n      component: api-server\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: api-server\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n    spec:\n      serviceAccountName: release-name-airflow-api-server\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: api-server\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: api-server\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow api-server\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        ports:\n        - name: api-server\n          containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 5\n          failureThreshold: 5\n          periodSeconds: 10\n        startupProbe:\n          httpGet:\n            path: /api/v2/version\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "034",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"dag-processor\" does not have a read-only root file system"
  },
  {
    "id": "035",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"dag-processor-log-groomer\" does not have a read-only root file system"
  },
  {
    "id": "036",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "037",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-dag-processor\" not found"
  },
  {
    "id": "038",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"dag-processor\" has cpu request 0"
  },
  {
    "id": "039",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"dag-processor-log-groomer\" has cpu request 0"
  },
  {
    "id": "040",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "041",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"dag-processor\" has memory limit 0"
  },
  {
    "id": "042",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"dag-processor-log-groomer\" has memory limit 0"
  },
  {
    "id": "043",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/028_deployment_release-name-dag-processor.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-dag-processor\n  labels:\n    tier: airflow\n    component: dag-processor\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: dag-processor\n      release: release-name\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 50%\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: dag-processor\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: dag-processor\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-dag-processor\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: dag-processor\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow dag-processor\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --local --job-type DagProcessorJob\n\n              '\n      - name: dag-processor-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "044",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"scheduler\" does not have a read-only root file system"
  },
  {
    "id": "045",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"scheduler-log-groomer\" does not have a read-only root file system"
  },
  {
    "id": "046",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "047",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-scheduler\" not found"
  },
  {
    "id": "048",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"scheduler\" has cpu request 0"
  },
  {
    "id": "049",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"scheduler-log-groomer\" has cpu request 0"
  },
  {
    "id": "050",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "051",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"scheduler\" has memory limit 0"
  },
  {
    "id": "052",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"scheduler-log-groomer\" has memory limit 0"
  },
  {
    "id": "053",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/029_deployment_release-name-scheduler.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-scheduler\n  labels:\n    tier: airflow\n    component: scheduler\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n    executor: CeleryExecutor\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: scheduler\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: scheduler\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: scheduler\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      restartPolicy: Always\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-airflow-scheduler\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: scheduler\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow scheduler\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        startupProbe:\n          initialDelaySeconds: 0\n          timeoutSeconds: 20\n          failureThreshold: 6\n          periodSeconds: 10\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type SchedulerJob --local\n\n              '\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/pod_templates/pod_template_file.yaml\n          subPath: pod_template_file.yaml\n          readOnly: true\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      - name: scheduler-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n      - name: logs\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "054",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/030_deployment_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"statsd\" does not have a read-only root file system"
  },
  {
    "id": "055",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/030_deployment_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-statsd\" not found"
  },
  {
    "id": "056",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/030_deployment_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"statsd\" has cpu request 0"
  },
  {
    "id": "057",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/030_deployment_release-name-statsd.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-statsd\n  labels:\n    tier: airflow\n    component: statsd\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: statsd\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: statsd\n        release: release-name\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-airflow-statsd\n      securityContext:\n        runAsUser: 65534\n      restartPolicy: Always\n      containers:\n      - name: statsd\n        image: quay.io/prometheus/statsd-exporter:v0.28.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - --statsd.mapping-config=/etc/statsd-exporter/mappings.yml\n        resources: {}\n        ports:\n        - name: statsd-ingest\n          protocol: UDP\n          containerPort: 9125\n        - name: statsd-scrape\n          containerPort: 9102\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: 9102\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: config\n          mountPath: /etc/statsd-exporter/mappings.yml\n          subPath: mappings.yml\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-statsd\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"statsd\" has memory limit 0"
  },
  {
    "id": "058",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/031_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 16.1.0\n        helm.sh/chart: postgresql-13.2.24\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:16.1.0-debian-11-r15\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n          runAsUser: 1001\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits: {}\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"postgresql\" does not have a read-only root file system"
  },
  {
    "id": "059",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/031_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 16.1.0\n    helm.sh/chart: postgresql-13.2.24\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 16.1.0\n        helm.sh/chart: postgresql-13.2.24\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: default\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:16.1.0-debian-11-r15\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n          runAsUser: 1001\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits: {}\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"postgresql\" has memory limit 0"
  },
  {
    "id": "060",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "061",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-redis\" not found"
  },
  {
    "id": "062",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "063",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "064",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/032_statefulset_release-name-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis\n  labels:\n    tier: airflow\n    component: redis\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-redis\n  selector:\n    matchLabels:\n      tier: airflow\n      component: redis\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: redis\n        release: release-name\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      serviceAccountName: release-name-airflow-redis\n      securityContext:\n        runAsUser: 0\n      containers:\n      - name: redis\n        image: redis:7.2-bookworm\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        command:\n        - /bin/sh\n        resources: {}\n        args:\n        - -c\n        - redis-server --requirepass ${REDIS_PASSWORD}\n        ports:\n        - name: redis-db\n          containerPort: 6379\n        volumeMounts:\n        - name: redis-db\n          mountPath: /data\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-redis-password\n              key: password\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-db\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "065",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"triggerer\" does not have a read-only root file system"
  },
  {
    "id": "066",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"triggerer-log-groomer\" does not have a read-only root file system"
  },
  {
    "id": "067",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "068",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-triggerer\" not found"
  },
  {
    "id": "069",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"triggerer\" has cpu request 0"
  },
  {
    "id": "070",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"triggerer-log-groomer\" has cpu request 0"
  },
  {
    "id": "071",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "072",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"triggerer\" has memory limit 0"
  },
  {
    "id": "073",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"triggerer-log-groomer\" has memory limit 0"
  },
  {
    "id": "074",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/033_statefulset_release-name-triggerer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-triggerer\n  labels:\n    tier: airflow\n    component: triggerer\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-triggerer\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: triggerer\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: triggerer\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: triggerer\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 60\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-triggerer\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: triggerer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - exec airflow triggerer\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - 'CONNECTION_CHECK_MAX_COUNT=0 AIRFLOW__LOGGING__LOGGING_LEVEL=ERROR\n              exec /entrypoint \\\n\n              airflow jobs check --job-type TriggererJob --local\n\n              '\n        ports:\n        - name: triggerer-logs\n          containerPort: 8794\n      - name: triggerer-log-groomer\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "075",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wait-for-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "076",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"worker\" does not have a read-only root file system"
  },
  {
    "id": "077",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"worker-log-groomer\" does not have a read-only root file system"
  },
  {
    "id": "078",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-worker\" not found"
  },
  {
    "id": "079",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "080",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"worker\" has cpu request 0"
  },
  {
    "id": "081",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"worker-log-groomer\" has cpu request 0"
  },
  {
    "id": "082",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-for-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "083",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"worker\" has memory limit 0"
  },
  {
    "id": "084",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/034_statefulset_release-name-worker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-worker\n  labels:\n    tier: airflow\n    component: worker\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\nspec:\n  serviceName: release-name-worker\n  replicas: 1\n  selector:\n    matchLabels:\n      tier: airflow\n      component: worker\n      release: release-name\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: worker\n        release: release-name\n      annotations:\n        checksum/metadata-secret: 327e091234a14bd54fe663b0f2eb6d8f7b817c2dcfa8b6fc2311334f6a68faf1\n        checksum/result-backend-secret: 98a68f230007cfa8f5d3792e1aff843a76b0686409e4a46ab2f092f6865a1b71\n        checksum/pgbouncer-config-secret: 1dae2adc757473469686d37449d076b0c82404f61413b58ae68b3c5e99527688\n        checksum/webserver-secret-key: 97ccb485bcb4d6b5d625617645f8cda75f82731afbd1fa58b643a3bd927c7026\n        checksum/kerberos-keytab: 80979996aa3c1f48c95dfbe9bb27191e71f12442a08c0ed834413da9d430fd0e\n        checksum/airflow-config: 8280f1a548335eb4fa2dadaae8a78f009bb408ea8725a2363cc9533a176ec5db\n        checksum/extra-configmaps: e862ea47e13e634cf17d476323784fa27dac20015550c230953b526182f5cac8\n        checksum/extra-secrets: e9582fdd622296c976cbc10a5ba7d6702c28a24fe80795ea5b84ba443a56c827\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'false'\n    spec:\n      nodeSelector: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  component: worker\n              topologyKey: kubernetes.io/hostname\n            weight: 100\n      tolerations: []\n      topologySpreadConstraints: []\n      terminationGracePeriodSeconds: 600\n      restartPolicy: Always\n      serviceAccountName: release-name-airflow-worker\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      initContainers:\n      - name: wait-for-airflow-migrations\n        resources: {}\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        args:\n        - airflow\n        - db\n        - check-migrations\n        - --migration-wait-timeout=60\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      containers:\n      - name: worker\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow celery worker'\n        resources: {}\n        livenessProbe:\n          initialDelaySeconds: 10\n          timeoutSeconds: 20\n          failureThreshold: 5\n          periodSeconds: 60\n          exec:\n            command:\n            - sh\n            - -c\n            - CONNECTION_CHECK_MAX_COUNT=0 exec /entrypoint python -m celery --app\n              airflow.providers.celery.executors.celery_executor.app inspect ping\n              -d celery@$(hostname)\n        ports:\n        - name: worker-logs\n          containerPort: 8793\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n        envFrom: []\n        env:\n        - name: DUMB_INIT_SETSID\n          value: '0'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n      - name: worker-log-groomer\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - /clean-logs\n        env:\n        - name: AIRFLOW__LOG_RETENTION_DAYS\n          value: '15'\n        - name: AIRFLOW__LOG_CLEANUP_FREQUENCY_MINUTES\n          value: '15'\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        resources: {}\n        volumeMounts:\n        - name: logs\n          mountPath: /opt/airflow/logs\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: logs\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 100Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"worker-log-groomer\" has memory limit 0"
  },
  {
    "id": "085",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/038_job_release-name-create-user.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"create-user\" does not have a read-only root file system"
  },
  {
    "id": "086",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/038_job_release-name-create-user.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-create-user-job\" not found"
  },
  {
    "id": "087",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/038_job_release-name-create-user.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"create-user\" has cpu request 0"
  },
  {
    "id": "088",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/038_job_release-name-create-user.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-create-user\n  labels:\n    tier: airflow\n    component: create-user-job\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '2'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: create-user-job\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-create-user-job\n      containers:\n      - name: create-user\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow users create \"$@\"'\n        - --\n        - -r\n        - Admin\n        - -u\n        - admin\n        - -e\n        - admin@example.com\n        - -f\n        - admin\n        - -l\n        - user\n        - -p\n        - admin\n        envFrom: []\n        env:\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"create-user\" has memory limit 0"
  },
  {
    "id": "089",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/039_job_release-name-run-airflow-migrations.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"run-airflow-migrations\" does not have a read-only root file system"
  },
  {
    "id": "090",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/039_job_release-name-run-airflow-migrations.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-airflow-migrate-database-job\" not found"
  },
  {
    "id": "091",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/039_job_release-name-run-airflow-migrations.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"run-airflow-migrations\" has cpu request 0"
  },
  {
    "id": "092",
    "manifest_path": "data/manifests/artifacthub/apache-airflow/airflow/039_job_release-name-run-airflow-migrations.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-run-airflow-migrations\n  labels:\n    tier: airflow\n    component: run-airflow-migrations\n    release: release-name\n    chart: airflow-1.18.0\n    heritage: Helm\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  ttlSecondsAfterFinished: 300\n  template:\n    metadata:\n      labels:\n        tier: airflow\n        component: run-airflow-migrations\n        release: release-name\n    spec:\n      securityContext:\n        runAsUser: 50000\n        fsGroup: 0\n      restartPolicy: OnFailure\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      topologySpreadConstraints: []\n      serviceAccountName: release-name-airflow-migrate-database-job\n      containers:\n      - name: run-airflow-migrations\n        image: apache/airflow:3.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        args:\n        - bash\n        - -c\n        - 'exec \\\n\n          airflow db migrate'\n        envFrom: []\n        env:\n        - name: PYTHONUNBUFFERED\n          value: '1'\n        - name: AIRFLOW__CORE__FERNET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-fernet-key\n              key: fernet-key\n        - name: AIRFLOW_HOME\n          value: /opt/airflow\n        - name: AIRFLOW__CORE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW_CONN_AIRFLOW_DB\n          valueFrom:\n            secretKeyRef:\n              name: release-name-metadata\n              key: connection\n        - name: AIRFLOW__API__SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-api-secret-key\n              key: api-secret-key\n        - name: AIRFLOW__API_AUTH__JWT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-jwt-secret\n              key: jwt-secret\n        - name: AIRFLOW__CELERY__BROKER_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-broker-url\n              key: connection\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /opt/airflow/airflow.cfg\n          subPath: airflow.cfg\n          readOnly: true\n        - name: config\n          mountPath: /opt/airflow/config/airflow_local_settings.py\n          subPath: airflow_local_settings.py\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"run-airflow-migrations\" has memory limit 0"
  },
  {
    "id": "093",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/038_service_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  type: ClusterIP\n  ports:\n  - name: http-webhook\n    port: 7000\n    targetPort: webhook\n  selector:\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-applicationset-controller])"
  },
  {
    "id": "094",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/039_service_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\n  name: release-name-argocd-repo-server\n  namespace: default\nspec:\n  ports:\n  - name: tcp-repo-server\n    protocol: TCP\n    port: 8081\n    targetPort: repo-server\n  selector:\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-repo-server])"
  },
  {
    "id": "095",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/040_service_release-name-argocd-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 8080\n  - name: https\n    protocol: TCP\n    port: 443\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-server])"
  },
  {
    "id": "096",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/041_service_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ports:\n  - name: http\n    protocol: TCP\n    port: 5556\n    targetPort: http\n  - name: grpc\n    protocol: TCP\n    port: 5557\n    targetPort: grpc\n  selector:\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-dex-server])"
  },
  {
    "id": "097",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/042_service_release-name-argocd-redis.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ports:\n  - name: redis\n    port: 6379\n    targetPort: redis\n  selector:\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argocd-redis])"
  },
  {
    "id": "098",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"argocd-applicationset-controller\" not found"
  },
  {
    "id": "099",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"applicationset-controller\" has cpu request 0"
  },
  {
    "id": "100",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/043_deployment_release-name-argocd-applicationset-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-applicationset-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-applicationset-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: applicationset-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-applicationset-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-applicationset-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: applicationset-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-applicationset-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: applicationset-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-applicationset-controller\n        - --metrics-addr=:8080\n        - --probe-addr=:8081\n        - --webhook-addr=:7000\n        env:\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_ANNOTATIONS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.annotations\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_GLOBAL_PRESERVED_LABELS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.global.preserved.labels\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_LEADER_ELECTION\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.leader.election\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              key: repo.server\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_POLICY\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.policy\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_POLICY_OVERRIDE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.policy.override\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.debug\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_DRY_RUN\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.dryrun\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_PROGRESSIVE_SYNCS\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.progressive.syncs\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_TOKENREF_STRICT_MODE\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.tokenref.strict.mode\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_CONCURRENT_RECONCILIATIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.concurrent.reconciliations.max\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_REQUEUE_AFTER\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.requeue.after\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_MAX_RESOURCES_STATUS_COUNT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.status.max.resources.count\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 8080\n          protocol: TCP\n        - name: probe\n          containerPort: 8081\n          protocol: TCP\n        - name: webhook\n          containerPort: 7000\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /tmp\n          name: tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-applicationset-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"applicationset-controller\" has memory limit 0"
  },
  {
    "id": "101",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/044_deployment_release-name-argocd-notifications-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-notifications-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"argocd-notifications-controller\" not found"
  },
  {
    "id": "102",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/044_deployment_release-name-argocd-notifications-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-notifications-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"notifications-controller\" has cpu request 0"
  },
  {
    "id": "103",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/044_deployment_release-name-argocd-notifications-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-notifications-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-notifications-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: notifications-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-notifications-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-notifications-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: notifications-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-notifications-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: notifications-controller\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-notifications\n        - --metrics-port=9001\n        - --namespace=default\n        - --argocd-repo-server=release-name-argocd-repo-server:8081\n        - --secret-name=argocd-notifications-secret\n        env:\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATIONS_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              key: application.namespaces\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_SELF_SERVICE_NOTIFICATION_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.selfservice.enabled\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_NOTIFICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              key: notificationscontroller.repo.server.plaintext\n              name: argocd-cmd-params-cm\n              optional: true\n        ports:\n        - name: metrics\n          containerPort: 9001\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /app\n        volumeMounts:\n        - name: tls-certs\n          mountPath: /app/config/tls\n        - name: argocd-repo-server-tls\n          mountPath: /app/config/reposerver/tls\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-notifications-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"notifications-controller\" has memory limit 0"
  },
  {
    "id": "104",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-argocd-repo-server\" not found"
  },
  {
    "id": "105",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"copyutil\" has cpu request 0"
  },
  {
    "id": "106",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"repo-server\" has cpu request 0"
  },
  {
    "id": "107",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"copyutil\" has memory limit 0"
  },
  {
    "id": "108",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/045_deployment_release-name-argocd-repo-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-repo-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-repo-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: repo-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-repo-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-repo-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: repo-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: release-name-argocd-repo-server\n      automountServiceAccountToken: true\n      containers:\n      - name: repo-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-repo-server\n        - --port=8081\n        - --metrics-port=8084\n        env:\n        - name: ARGOCD_REPO_SERVER_NAME\n          value: release-name-argocd-repo-server\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.format\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.parallelism.limit\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_LISTEN_METRICS_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.metrics.listen.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.tls.ciphers\n              optional: true\n        - name: ARGOCD_REPO_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.repo.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.default.cache.expiration\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_REPO_SERVER_MAX_COMBINED_DIRECTORY_MANIFESTS_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.max.combined.directory.manifests.size\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_TAR_EXCLUSIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.plugin.tar.exclusions\n              optional: true\n        - name: ARGOCD_REPO_SERVER_PLUGIN_USE_MANIFEST_GENERATE_PATHS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.plugin.use.manifest.generate.paths\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_ALLOW_OUT_OF_BOUNDS_SYMLINKS\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.allow.oob.symlinks\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_TAR_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.tar.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_STREAMED_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.streamed.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.helm.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_HELM_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: reposerver.disable.helm.manifest.max.extracted.size\n              optional: true\n        - name: ARGOCD_GIT_MODULES_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.enable.git.submodule\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_LS_REMOTE_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.lsremote.parallelism.limit\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_GIT_REQUEST_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.git.request.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_DISABLE_OCI_MANIFEST_MAX_EXTRACTED_SIZE\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.disable.oci.manifest.max.extracted.size\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_OCI_LAYER_MEDIA_TYPES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.oci.layer.media.types\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REVISION_CACHE_LOCK_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.revision.cache.lock.timeout\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_REPO_SERVER_INCLUDE_HIDDEN_DIRECTORIES\n          valueFrom:\n            configMapKeyRef:\n              key: reposerver.include.hidden.directories\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: HELM_CACHE_HOME\n          value: /helm-working-dir\n        - name: HELM_CONFIG_HOME\n          value: /helm-working-dir\n        - name: HELM_DATA_HOME\n          value: /helm-working-dir\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/gpg/source\n          name: gpg-keys\n        - mountPath: /app/config/gpg/keys\n          name: gpg-keyring\n        - mountPath: /app/config/reposerver/tls\n          name: argocd-repo-server-tls\n        - mountPath: /helm-working-dir\n          name: helm-working-dir\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: tmp\n        ports:\n        - name: repo-server\n          containerPort: 8081\n          protocol: TCP\n        - name: metrics\n          containerPort: 8084\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      initContainers:\n      - command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /var/run/argocd/argocd-cmp-server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: copyutil\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-repo-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: helm-working-dir\n        emptyDir: {}\n      - name: plugins\n        emptyDir: {}\n      - name: var-files\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: gpg-keys\n        configMap:\n          name: argocd-gpg-keys-cm\n      - name: gpg-keyring\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"repo-server\" has memory limit 0"
  },
  {
    "id": "109",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/046_deployment_release-name-argocd-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-server\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"argocd-server\" not found"
  },
  {
    "id": "110",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/046_deployment_release-name-argocd-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-server\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "111",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/046_deployment_release-name-argocd-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-server\n      automountServiceAccountToken: true\n      containers:\n      - name: server\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        args:\n        - /usr/local/bin/argocd-server\n        - --port=8080\n        - --metrics-port=8083\n        env:\n        - name: ARGOCD_SERVER_NAME\n          value: release-name-argocd-server\n        - name: ARGOCD_SERVER_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.insecure\n              optional: true\n        - name: ARGOCD_SERVER_BASEHREF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.basehref\n              optional: true\n        - name: ARGOCD_SERVER_ROOTPATH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.rootpath\n              optional: true\n        - name: ARGOCD_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.format\n              optional: true\n        - name: ARGOCD_SERVER_LOG_LEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.log.level\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server\n              optional: true\n        - name: ARGOCD_SERVER_DISABLE_AUTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.disable.auth\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_GZIP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.gzip\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_SERVER_X_FRAME_OPTIONS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.x.frame.options\n              optional: true\n        - name: ARGOCD_SERVER_CONTENT_SECURITY_POLICY\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.content.security.policy\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.plaintext\n              optional: true\n        - name: ARGOCD_SERVER_DEX_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.dex.server.strict.tls\n              optional: true\n        - name: ARGOCD_TLS_MIN_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.minversion\n              optional: true\n        - name: ARGOCD_TLS_MAX_VERSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.maxversion\n              optional: true\n        - name: ARGOCD_TLS_CIPHERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.tls.ciphers\n              optional: true\n        - name: ARGOCD_SERVER_CONNECTION_STATUS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.connection.status.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_OIDC_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.oidc.cache.expiration\n              optional: true\n        - name: ARGOCD_SERVER_STATIC_ASSETS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.staticassets\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.default.cache.expiration\n              optional: true\n        - name: ARGOCD_MAX_COOKIE_NUMBER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.http.cookie.maxnumber\n              optional: true\n        - name: ARGOCD_SERVER_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_METRICS_LISTEN_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.metrics.listen.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_SERVER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_SERVER_ENABLE_PROXY_EXTENSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.enable.proxy.extension\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_API_CONTENT_TYPES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.api.content.types\n              optional: true\n        - name: ARGOCD_SERVER_WEBHOOK_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.webhook.parallelism.limit\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_NEW_GIT_FILE_GLOBBING\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.enable.new.git.file.globbing\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_SCM_ROOT_CA_PATH\n          valueFrom:\n            configMapKeyRef:\n              key: applicationsetcontroller.scm.root.ca.path\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ALLOWED_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.allowed.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_SCM_PROVIDERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.scm.providers\n              optional: true\n        - name: ARGOCD_APPLICATIONSET_CONTROLLER_ENABLE_GITHUB_API_METRICS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: applicationsetcontroller.enable.github.api.metrics\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_SYNC_WITH_REPLACE_ALLOWED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: server.sync.replace.allowed\n              optional: true\n        volumeMounts:\n        - mountPath: /app/config/ssh\n          name: ssh-known-hosts\n        - mountPath: /app/config/tls\n          name: tls-certs\n        - mountPath: /app/config/server/tls\n          name: argocd-repo-server-tls\n        - mountPath: /app/config/dex/tls\n          name: argocd-dex-server-tls\n        - mountPath: /home/argocd\n          name: plugins-home\n        - mountPath: /shared/app/custom\n          name: styles\n        - mountPath: /tmp\n          name: tmp\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        ports:\n        - name: server\n          containerPort: 8080\n          protocol: TCP\n        - name: metrics\n          containerPort: 8083\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz?full=true\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: server\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: plugins-home\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: ssh-known-hosts\n        configMap:\n          name: argocd-ssh-known-hosts-cm\n      - name: tls-certs\n        configMap:\n          name: argocd-tls-certs-cm\n      - name: styles\n        configMap:\n          name: argocd-styles-cm\n          optional: true\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: server.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "112",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"argocd-dex-server\" not found"
  },
  {
    "id": "113",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"copyutil\" has cpu request 0"
  },
  {
    "id": "114",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"dex-server\" has cpu request 0"
  },
  {
    "id": "115",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"copyutil\" has memory limit 0"
  },
  {
    "id": "116",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/047_deployment_release-name-argocd-dex-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-dex-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-dex-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: dex-server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-dex-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-dex-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: dex-server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-dex-server\n      automountServiceAccountToken: true\n      containers:\n      - name: dex-server\n        image: ghcr.io/dexidp/dex:v2.44.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /shared/argocd-dex\n        args:\n        - rundex\n        env:\n        - name: ARGOCD_DEX_SERVER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.format\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_DEX_SERVER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              key: dexserver.log.level\n              name: argocd-cmd-params-cm\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_DEX_SERVER_DISABLE_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: dexserver.disable.tls\n              optional: true\n        ports:\n        - name: http\n          containerPort: 5556\n          protocol: TCP\n        - name: grpc\n          containerPort: 5557\n          protocol: TCP\n        - name: metrics\n          containerPort: 5558\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: static-files\n          mountPath: /shared\n        - name: dexconfig\n          mountPath: /tmp\n        - name: argocd-dex-server-tls\n          mountPath: /tls\n      initContainers:\n      - name: copyutil\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/cp\n        - -n\n        - /usr/local/bin/argocd\n        - /shared/argocd-dex\n        volumeMounts:\n        - mountPath: /shared\n          name: static-files\n        - mountPath: /tmp\n          name: dexconfig\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-dex-server\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: static-files\n        emptyDir: {}\n      - name: dexconfig\n        emptyDir: {}\n      - name: argocd-dex-server-tls\n        secret:\n          secretName: argocd-dex-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"dex-server\" has memory limit 0"
  },
  {
    "id": "117",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/048_deployment_release-name-argocd-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-redis\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: redis\n        image: ecr-public.aws.com/docker/library/redis:7.2.8-alpine\n        imagePullPolicy: IfNotPresent\n        args:\n        - --save\n        - ''\n        - --appendonly\n        - 'no'\n        - --requirepass $(REDIS_PASSWORD)\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n        ports:\n        - name: redis\n          containerPort: 6379\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /health\n          name: health\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: health\n        configMap:\n          name: release-name-argocd-redis-health-configmap\n          defaultMode: 493\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "118",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/048_deployment_release-name-argocd-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argocd-redis\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-redis\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: default\n      automountServiceAccountToken: true\n      containers:\n      - name: redis\n        image: ecr-public.aws.com/docker/library/redis:7.2.8-alpine\n        imagePullPolicy: IfNotPresent\n        args:\n        - --save\n        - ''\n        - --appendonly\n        - 'no'\n        - --requirepass $(REDIS_PASSWORD)\n        env:\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n        ports:\n        - name: redis\n          containerPort: 6379\n          protocol: TCP\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - mountPath: /health\n          name: health\n      nodeSelector:\n        kubernetes.io/os: linux\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis\n              topologyKey: kubernetes.io/hostname\n      volumes:\n      - name: health\n        configMap:\n          name: release-name-argocd-redis-health-configmap\n          defaultMode: 493\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "119",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/049_statefulset_release-name-argocd-application-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-application-controller\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"argocd-application-controller\" not found"
  },
  {
    "id": "120",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/049_statefulset_release-name-argocd-application-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-application-controller\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"application-controller\" has cpu request 0"
  },
  {
    "id": "121",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/049_statefulset_release-name-argocd-application-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-argocd-application-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-application-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: application-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  replicas: 1\n  revisionHistoryLimit: 5\n  serviceName: release-name-argocd-application-controller\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argocd-application-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/cmd-params: 2668ced57bd7b8a4198bc09a7505497f26b422d12a957a08b048317dda050991\n        checksum/cm: 96ecfb88b6c2d1b7b1b033e1c466cb5b41411e6d7280e8c87ae5a02343d5079a\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-application-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: application-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      terminationGracePeriodSeconds: 30\n      serviceAccountName: argocd-application-controller\n      automountServiceAccountToken: true\n      containers:\n      - args:\n        - /usr/local/bin/argocd-application-controller\n        - --metrics-port=8082\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: application-controller\n        env:\n        - name: ARGOCD_CONTROLLER_REPLICAS\n          value: '1'\n        - name: ARGOCD_APPLICATION_CONTROLLER_NAME\n          value: release-name-argocd-application-controller\n        - name: ARGOCD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.reconciliation\n              optional: true\n        - name: ARGOCD_HARD_RECONCILIATION_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cm\n              key: timeout.hard.reconciliation\n              optional: true\n        - name: ARGOCD_RECONCILIATION_JITTER\n          valueFrom:\n            configMapKeyRef:\n              key: timeout.reconciliation.jitter\n              name: argocd-cm\n              optional: true\n        - name: ARGOCD_REPO_ERROR_GRACE_PERIOD_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.error.grace.period.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: repo.server\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_STATUS_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.status.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OPERATION_PROCESSORS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.operation.processors\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGFORMAT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.format\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_LOGLEVEL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.log.level\n              optional: true\n        - name: ARGOCD_LOG_FORMAT_TIMESTAMP\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: log.format.timestamp\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_METRICS_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.metrics.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_TIMEOUT_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_FACTOR\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.factor\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_CAP_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cap.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SELF_HEAL_BACKOFF_COOLDOWN_SECONDS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.self.heal.backoff.cooldown.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SYNC_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sync.timeout.seconds\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_PLAINTEXT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.plaintext\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_REPO_SERVER_STRICT_TLS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.repo.server.strict.tls\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_PERSIST_RESOURCE_HEALTH\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.resource.health.persist\n              optional: true\n        - name: ARGOCD_APP_STATE_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.app.state.cache.expiration\n              optional: true\n        - name: REDIS_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.server\n              optional: true\n        - name: REDIS_COMPRESSION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.compression\n              optional: true\n        - name: REDISDB\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: redis.db\n              optional: true\n        - name: REDIS_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: redis-username\n              optional: true\n        - name: REDIS_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: argocd-redis\n              key: auth\n              optional: false\n        - name: REDIS_SENTINEL_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-username\n              optional: true\n        - name: REDIS_SENTINEL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-argocd-redis\n              key: redis-sentinel-password\n              optional: true\n        - name: ARGOCD_DEFAULT_CACHE_EXPIRATION\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.default.cache.expiration\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ADDRESS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.address\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_INSECURE\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.insecure\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_HEADERS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.headers\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_OTLP_ATTRS\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: otlp.attrs\n              optional: true\n        - name: ARGOCD_APPLICATION_NAMESPACES\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: application.namespaces\n              optional: true\n        - name: ARGOCD_CONTROLLER_SHARDING_ALGORITHM\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.sharding.algorithm\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_KUBECTL_PARALLELISM_LIMIT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.kubectl.parallelism.limit\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_MAX\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.max\n              optional: true\n        - name: ARGOCD_K8SCLIENT_RETRY_BASE_BACKOFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.k8sclient.retry.base.backoff\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_SERVER_SIDE_DIFF\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.diff.server.side\n              optional: true\n        - name: ARGOCD_IGNORE_NORMALIZER_JQ_TIMEOUT\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.ignore.normalizer.jq.timeout\n              optional: true\n        - name: ARGOCD_HYDRATOR_ENABLED\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: hydrator.enabled\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_BATCH_EVENTS_PROCESSING\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.batch.events.processing\n              optional: true\n        - name: ARGOCD_CLUSTER_CACHE_EVENTS_PROCESSING_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: controller.cluster.cache.events.processing.interval\n              optional: true\n        - name: ARGOCD_APPLICATION_CONTROLLER_COMMIT_SERVER\n          valueFrom:\n            configMapKeyRef:\n              name: argocd-cmd-params-cm\n              key: commit.server\n              optional: true\n        - name: KUBECACHEDIR\n          value: /tmp/kubecache\n        ports:\n        - name: metrics\n          containerPort: 8082\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        workingDir: /home/argocd\n        volumeMounts:\n        - mountPath: /app/config/controller/tls\n          name: argocd-repo-server-tls\n        - mountPath: /home/argocd\n          name: argocd-home\n        - name: argocd-cmd-params-cm\n          mountPath: /home/argocd/params\n        - name: argocd-application-controller-tmp\n          mountPath: /tmp\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-application-controller\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: argocd-home\n        emptyDir: {}\n      - name: argocd-application-controller-tmp\n        emptyDir: {}\n      - name: argocd-repo-server-tls\n        secret:\n          secretName: argocd-repo-server-tls\n          optional: true\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n          - key: ca.crt\n            path: ca.crt\n      - name: argocd-cmd-params-cm\n        configMap:\n          optional: true\n          name: argocd-cmd-params-cm\n          items:\n          - key: controller.profile.enabled\n            path: profiler.enabled\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"application-controller\" has memory limit 0"
  },
  {
    "id": "122",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/053_job_release-name-argocd-redis-secret-init.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-argocd-redis-secret-init\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-argocd-redis-secret-init\" not found"
  },
  {
    "id": "123",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/053_job_release-name-argocd-redis-secret-init.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-argocd-redis-secret-init\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"secret-init\" has cpu request 0"
  },
  {
    "id": "124",
    "manifest_path": "data/manifests/artifacthub/argo/argo-cd/053_job_release-name-argocd-redis-secret-init.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-argocd-redis-secret-init\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation\n  labels:\n    helm.sh/chart: argo-cd-8.5.9\n    app.kubernetes.io/name: argocd-redis-secret-init\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: redis-secret-init\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argocd\n    app.kubernetes.io/version: v3.1.8\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-cd-8.5.9\n        app.kubernetes.io/name: argocd-redis-secret-init\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: redis-secret-init\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argocd\n        app.kubernetes.io/version: v3.1.8\n    spec:\n      containers:\n      - command:\n        - argocd\n        - admin\n        - redis-initial-password\n        image: quay.io/argoproj/argocd:v3.1.8\n        imagePullPolicy: IfNotPresent\n        name: secret-init\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/name: argocd-redis-secret-init\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-argocd-redis-secret-init\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"secret-init\" has memory limit 0"
  },
  {
    "id": "125",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/025_service_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  ports:\n  - port: 2746\n    targetPort: 2746\n  selector:\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:argo-workflows-server])"
  },
  {
    "id": "126",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/026_deployment_release-name-argo-workflows-workflow-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: release-name-argo-workflows-workflow-controller\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources: {}\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-argo-workflows-workflow-controller\" not found"
  },
  {
    "id": "127",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/026_deployment_release-name-argo-workflows-workflow-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: release-name-argo-workflows-workflow-controller\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources: {}\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"controller\" has cpu request 0"
  },
  {
    "id": "128",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/026_deployment_release-name-argo-workflows-workflow-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-workflow-controller\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-workflow-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: workflow-controller\n    app: workflow-controller\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-workflow-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-workflow-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: workflow-controller\n        app: workflow-controller\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n    spec:\n      serviceAccountName: release-name-argo-workflows-workflow-controller\n      containers:\n      - name: controller\n        image: quay.io/argoproj/workflow-controller:v3.7.2\n        imagePullPolicy: Always\n        command:\n        - workflow-controller\n        args:\n        - --configmap\n        - release-name-argo-workflows-workflow-controller-configmap\n        - --executor-image\n        - quay.io/argoproj/argoexec:v3.7.2\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        env:\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LEADER_ELECTION_IDENTITY\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: LEADER_ELECTION_DISABLE\n          value: 'true'\n        resources: {}\n        ports:\n        - name: metrics\n          containerPort: 9090\n        - containerPort: 6060\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 6060\n          initialDelaySeconds: 90\n          periodSeconds: 60\n          timeoutSeconds: 30\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "129",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/027_deployment_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"argo-server\" does not have a read-only root file system"
  },
  {
    "id": "130",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/027_deployment_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-argo-workflows-server\" not found"
  },
  {
    "id": "131",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/027_deployment_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"argo-server\" has cpu request 0"
  },
  {
    "id": "132",
    "manifest_path": "data/manifests/artifacthub/argo/argo-workflows/027_deployment_release-name-argo-workflows-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-argo-workflows-server\n  namespace: default\n  labels:\n    helm.sh/chart: argo-workflows-0.45.26\n    app.kubernetes.io/name: argo-workflows-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: server\n    app: server\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: argo-workflows\n    app.kubernetes.io/version: v3.7.2\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: argo-workflows-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: argo-workflows-0.45.26\n        app.kubernetes.io/name: argo-workflows-server\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: server\n        app: server\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: argo-workflows\n        app.kubernetes.io/version: v3.7.2\n      annotations:\n        checksum/cm: bfd06978cb1b4ebca21cb5b49988153e6e6824da4db346a3a9b3ed6b802bb083\n    spec:\n      serviceAccountName: release-name-argo-workflows-server\n      containers:\n      - name: argo-server\n        image: quay.io/argoproj/argocli:v3.7.2\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        args:\n        - server\n        - --configmap=release-name-argo-workflows-workflow-controller-configmap\n        - --secure=false\n        - --loglevel\n        - info\n        - --gloglevel\n        - '0'\n        - --log-format\n        - text\n        ports:\n        - name: web\n          containerPort: 2746\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 2746\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 20\n        env:\n        - name: IN_CLUSTER\n          value: 'true'\n        - name: ARGO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: ARGO_BASE_HREF\n          value: /\n        resources: {}\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"argo-server\" has memory limit 0"
  },
  {
    "id": "133",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/009_service_release-name-postgresql-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-headless\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app: postgresql\n    release: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:postgresql release:release-name])"
  },
  {
    "id": "134",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/010_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app: postgresql\n    release: release-name\n    role: master\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:postgresql release:release-name role:master])"
  },
  {
    "id": "135",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/011_service_hub.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: NodePort\n  ports:\n  - port: 80\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/component: hub\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:hub app.kubernetes.io/instance:release-name app.kubernetes.io/name:artifact-hub])"
  },
  {
    "id": "136",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/012_service_trivy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8081\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/component: trivy\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:trivy app.kubernetes.io/instance:release-name app.kubernetes.io/name:artifact-hub])"
  },
  {
    "id": "137",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "138",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"check-db-migrator-run\" does not have a read-only root file system"
  },
  {
    "id": "139",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "140",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"hub\" does not have a read-only root file system"
  },
  {
    "id": "141",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"hub\" not found"
  },
  {
    "id": "142",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"check-db-migrator-run\" is not set to runAsNonRoot"
  },
  {
    "id": "143",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "144",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"hub\" is not set to runAsNonRoot"
  },
  {
    "id": "145",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-migrator-run\" has cpu request 0"
  },
  {
    "id": "146",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "147",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"hub\" has cpu request 0"
  },
  {
    "id": "148",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-migrator-run\" has memory limit 0"
  },
  {
    "id": "149",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "150",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/013_deployment_hub.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hub\n  labels:\n    app.kubernetes.io/component: hub\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: hub\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8001'\n      labels:\n        app.kubernetes.io/component: hub\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: hub\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      - name: check-db-migrator-run\n        image: bitnami/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - kubectl\n        - wait\n        - --namespace=default\n        - --for=condition=complete\n        - job/db-migrator-install\n        - --timeout=60s\n      containers:\n      - name: hub\n        image: artifacthub/hub:v1.21.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: hub-config\n          mountPath: /home/hub/.cfg\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8000\n          protocol: TCP\n      volumes:\n      - name: hub-config\n        secret:\n          secretName: hub-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"hub\" has memory limit 0"
  },
  {
    "id": "151",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/014_deployment_trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"trivy\" does not have a read-only root file system"
  },
  {
    "id": "152",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/014_deployment_trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"trivy\" is not set to runAsNonRoot"
  },
  {
    "id": "153",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/014_deployment_trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"trivy\" has cpu request 0"
  },
  {
    "id": "154",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/014_deployment_trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: trivy\n  labels:\n    app.kubernetes.io/component: trivy\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: trivy\n      app.kubernetes.io/name: artifact-hub\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: trivy\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n    spec:\n      containers:\n      - name: trivy\n        image: aquasec/trivy:0.56.1\n        command:\n        - trivy\n        - --debug\n        - --cache-dir\n        - /trivy\n        - server\n        - --listen\n        - 0.0.0.0:8081\n        volumeMounts:\n        - name: trivy\n          mountPath: /trivy\n        ports:\n        - name: http\n          containerPort: 8081\n          protocol: TCP\n      volumes:\n      - name: trivy\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"trivy\" has memory limit 0"
  },
  {
    "id": "155",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"release-name-postgresql\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "156",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"init-chmod-data\" does not have a read-only root file system"
  },
  {
    "id": "157",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"release-name-postgresql\" does not have a read-only root file system"
  },
  {
    "id": "158",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"init-chmod-data\" is not set to runAsNonRoot"
  },
  {
    "id": "159",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"init-chmod-data\" has memory limit 0"
  },
  {
    "id": "160",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/015_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  labels:\n    app: postgresql\n    chart: postgresql-8.2.1\n    release: release-name\n    heritage: Helm\nspec:\n  serviceName: release-name-postgresql-headless\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: postgresql\n      release: release-name\n      role: master\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app: postgresql\n        chart: postgresql-8.2.1\n        release: release-name\n        heritage: Helm\n        role: master\n    spec:\n      securityContext:\n        fsGroup: 1001\n      initContainers:\n      - name: init-chmod-data\n        image: docker.io/bitnami/minideb:stretch\n        imagePullPolicy: Always\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        command:\n        - /bin/sh\n        - -c\n        - \"mkdir -p /data/data\\nchmod 700 /data/data\\nfind /data -mindepth 0 -maxdepth\\\n          \\ 1 -not -name \\\".snapshot\\\" -not -name \\\"lost+found\\\" | \\\\\\n  xargs chown\\\n          \\ -R 1001:1001\\nchmod -R 777 /dev/shm\\n\"\n        securityContext:\n          runAsUser: 0\n        volumeMounts:\n        - name: data\n          mountPath: /data\n          subPath: null\n        - name: dshm\n          mountPath: /dev/shm\n      containers:\n      - name: release-name-postgresql\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          requests:\n            cpu: 250m\n            memory: 256Mi\n        securityContext:\n          runAsUser: 1001\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /data\n        - name: PGDATA\n          value: /data/pgdata\n        - name: POSTGRES_USER\n          value: postgres\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-postgresql\n              key: postgresql-password\n        - name: POSTGRES_DB\n          value: hub\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -d \"hub\" -h 127.0.0.1 -p 5432\n\n              '\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        volumeMounts:\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /data\n          subPath: null\n      volumes:\n      - name: dshm\n        emptyDir:\n          medium: Memory\n          sizeLimit: 1Gi\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-postgresql\" has memory limit 0"
  },
  {
    "id": "161",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "162",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "163",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "164",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"db-migrator\" does not have a read-only root file system"
  },
  {
    "id": "165",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "166",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"db-migrator\" is not set to runAsNonRoot"
  },
  {
    "id": "167",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "168",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"db-migrator\" has cpu request 0"
  },
  {
    "id": "169",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "170",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/016_job_db-migrator-install.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrator-install\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ttlSecondsAfterFinished: null\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: artifact-hub-1.21.0\n        app.kubernetes.io/name: artifact-hub\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.21.0\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: default\n      restartPolicy: Never\n      initContainers:\n      - name: check-db-ready\n        image: docker.io/artifacthub/postgres:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: PGHOST\n          value: release-name-postgresql.default\n        - name: PGPORT\n          value: '5432'\n        - name: PGUSER\n          value: postgres\n        command:\n        - sh\n        - -c\n        - until pg_isready; do echo waiting for database; sleep 2; done;\n      containers:\n      - name: db-migrator\n        image: artifacthub/db-migrator:v1.21.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: TERN_CONF\n          value: /home/db-migrator/.cfg/tern.conf\n        volumeMounts:\n        - name: db-migrator-config\n          mountPath: /home/db-migrator/.cfg\n          readOnly: true\n        command:\n        - ./migrate.sh\n      volumes:\n      - name: db-migrator-config\n        secret:\n          secretName: db-migrator-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"db-migrator\" has memory limit 0"
  },
  {
    "id": "171",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "172",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "173",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"scanner\" does not have a read-only root file system"
  },
  {
    "id": "174",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "175",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"scanner\" is not set to runAsNonRoot"
  },
  {
    "id": "176",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "177",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"scanner\" has cpu request 0"
  },
  {
    "id": "178",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "179",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/017_cronjob_scanner.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: scanner\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 15,45 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: scanner\n            image: artifacthub/scanner:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: scanner-config\n              mountPath: /home/scanner/.cfg\n              readOnly: true\n          volumes:\n          - name: scanner-config\n            secret:\n              secretName: scanner-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"scanner\" has memory limit 0"
  },
  {
    "id": "180",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"check-db-ready\" is using an invalid container image, \"docker.io/artifacthub/postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "181",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"check-db-ready\" does not have a read-only root file system"
  },
  {
    "id": "182",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"tracker\" does not have a read-only root file system"
  },
  {
    "id": "183",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"check-db-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "184",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"tracker\" is not set to runAsNonRoot"
  },
  {
    "id": "185",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-ready\" has cpu request 0"
  },
  {
    "id": "186",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"tracker\" has cpu request 0"
  },
  {
    "id": "187",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"check-db-ready\" has memory limit 0"
  },
  {
    "id": "188",
    "manifest_path": "data/manifests/artifacthub/artifact-hub/artifact-hub/018_cronjob_tracker.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tracker\n  labels:\n    helm.sh/chart: artifact-hub-1.21.0\n    app.kubernetes.io/name: artifact-hub\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.21.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  schedule: 1,30 * * * *\n  successfulJobsHistoryLimit: 1\n  failedJobsHistoryLimit: 1\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            helm.sh/chart: artifact-hub-1.21.0\n            app.kubernetes.io/name: artifact-hub\n            app.kubernetes.io/instance: release-name\n            app.kubernetes.io/version: 1.21.0\n            app.kubernetes.io/managed-by: Helm\n        spec:\n          serviceAccountName: default\n          restartPolicy: Never\n          initContainers:\n          - name: check-db-ready\n            image: docker.io/artifacthub/postgres:latest\n            imagePullPolicy: IfNotPresent\n            env:\n            - name: PGHOST\n              value: release-name-postgresql.default\n            - name: PGPORT\n              value: '5432'\n            - name: PGUSER\n              value: postgres\n            command:\n            - sh\n            - -c\n            - until pg_isready; do echo waiting for database; sleep 2; done;\n          containers:\n          - name: tracker\n            image: artifacthub/tracker:v1.21.0\n            imagePullPolicy: IfNotPresent\n            volumeMounts:\n            - name: tracker-config\n              mountPath: /home/tracker/.cfg\n              readOnly: true\n          volumes:\n          - name: tracker-config\n            secret:\n              secretName: tracker-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"tracker\" has memory limit 0"
  },
  {
    "id": "189",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/008_service_release-name-sealed-secrets.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 8080\n    targetPort: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/name: sealed-secrets\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:sealed-secrets])"
  },
  {
    "id": "190",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/009_service_release-name-sealed-secrets-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-sealed-secrets-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n    app.kubernetes.io/component: metrics\nspec:\n  type: ClusterIP\n  ports:\n  - name: metrics\n    port: 8081\n    targetPort: metrics\n    nodePort: null\n  selector:\n    app.kubernetes.io/name: sealed-secrets\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:sealed-secrets])"
  },
  {
    "id": "191",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/010_deployment_release-name-sealed-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: release-name-sealed-secrets\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits: {}\n          requests: {}\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-sealed-secrets\" not found"
  },
  {
    "id": "192",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/010_deployment_release-name-sealed-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: release-name-sealed-secrets\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits: {}\n          requests: {}\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"controller\" has cpu request 0"
  },
  {
    "id": "193",
    "manifest_path": "data/manifests/artifacthub/bitnami-labs/sealed-secrets/010_deployment_release-name-sealed-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-sealed-secrets\n  namespace: default\n  labels:\n    app.kubernetes.io/name: sealed-secrets\n    helm.sh/chart: sealed-secrets-2.17.7\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 0.32.2\n    app.kubernetes.io/part-of: sealed-secrets\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: sealed-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: sealed-secrets\n        app.kubernetes.io/instance: release-name\n    spec:\n      securityContext:\n        fsGroup: 65534\n      serviceAccountName: release-name-sealed-secrets\n      containers:\n      - name: controller\n        command:\n        - controller\n        args:\n        - --update-status\n        - --key-prefix\n        - sealed-secrets-key\n        - --listen-addr\n        - :8080\n        - --listen-metrics-addr\n        - :8081\n        image: docker.io/bitnami/sealed-secrets-controller:0.32.2\n        imagePullPolicy: IfNotPresent\n        env: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: metrics\n          containerPort: 8081\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /healthz\n            port: http\n        resources:\n          limits: {}\n          requests: {}\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1001\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "194",
    "manifest_path": "data/manifests/artifacthub/bitnami/external-dns/002_poddisruptionbudget_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: external-dns\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "195",
    "manifest_path": "data/manifests/artifacthub/bitnami/external-dns/006_service_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: http\n    port: 7979\n    protocol: TCP\n    targetPort: http\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: external-dns\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:external-dns])"
  },
  {
    "id": "196",
    "manifest_path": "data/manifests/artifacthub/bitnami/external-dns/007_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/version: 0.18.0\n    helm.sh/chart: external-dns-9.0.3\nspec:\n  revisionHistoryLimit: 10\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: external-dns\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/version: 0.18.0\n        helm.sh/chart: external-dns-9.0.3\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: external-dns\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      containers:\n      - name: external-dns\n        image: docker.io/bitnami/external-dns:0.18.0-debian-12-r4\n        imagePullPolicy: IfNotPresent\n        args:\n        - --metrics-address=:7979\n        - --log-level=info\n        - --log-format=text\n        - --policy=upsert-only\n        - --provider=aws\n        - --registry=txt\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --aws-api-retries=3\n        - --aws-zone-type=\n        - --aws-batch-change-size=1000\n        env:\n        - name: AWS_DEFAULT_REGION\n          value: us-east-1\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 7979\n        livenessProbe:\n          tcpSocket:\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 2\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 6\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-external-dns\" not found"
  },
  {
    "id": "197",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/002_poddisruptionbudget_release-name-kafka-broker.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-kafka-broker\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: broker\n    app.kubernetes.io/part-of: kafka\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: broker\n      app.kubernetes.io/part-of: kafka\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "198",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/003_poddisruptionbudget_release-name-kafka-controller.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-kafka-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: controller-eligible\n      app.kubernetes.io/part-of: kafka\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "199",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/009_service_release-name-kafka-controller-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kafka-controller-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-interbroker\n    port: 9094\n    protocol: TCP\n    targetPort: interbroker\n  - name: tcp-client\n    port: 9092\n    protocol: TCP\n    targetPort: client\n  - name: tcp-controller\n    protocol: TCP\n    port: 9093\n    targetPort: controller\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller-eligible app.kubernetes.io/instance:release-name app.kubernetes.io/name:kafka app.kubernetes.io/part-of:kafka])"
  },
  {
    "id": "200",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/010_service_release-name-kafka.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kafka\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: kafka\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-client\n    port: 9092\n    protocol: TCP\n    targetPort: client\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/part-of: kafka\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kafka app.kubernetes.io/part-of:kafka])"
  },
  {
    "id": "201",
    "manifest_path": "data/manifests/artifacthub/bitnami/kafka/011_statefulset_release-name-kafka-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-kafka-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: kafka\n    app.kubernetes.io/version: 4.0.0\n    helm.sh/chart: kafka-32.4.3\n    app.kubernetes.io/component: controller-eligible\n    app.kubernetes.io/part-of: kafka\nspec:\n  podManagementPolicy: Parallel\n  replicas: 3\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: kafka\n      app.kubernetes.io/component: controller-eligible\n      app.kubernetes.io/part-of: kafka\n  serviceName: release-name-kafka-controller-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: kafka\n        app.kubernetes.io/version: 4.0.0\n        helm.sh/chart: kafka-32.4.3\n        app.kubernetes.io/component: controller-eligible\n        app.kubernetes.io/part-of: kafka\n      annotations:\n        checksum/configuration: d96601aff02b0e88a9bc5c8593a6d0446462e05650b1eb84af185e551160d1c8\n        checksum/secret: dba2cdb043e84e63d768aad3292379237050a24915b1b3e70c1f2408e8cd76e8\n    spec:\n      automountServiceAccountToken: false\n      hostNetwork: false\n      hostIPC: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: kafka\n                  app.kubernetes.io/component: controller-eligible\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        seccompProfile:\n          type: RuntimeDefault\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-kafka\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-config\n        image: docker.io/bitnami/kafka:4.0.0-debian-12-r10\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            add: []\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \". /opt/bitnami/scripts/libkafka.sh\\nconfigure_kafka_sasl() {\\n    # Replace\\\n          \\ placeholders with passwords\\n    replace_in_file \\\"$KAFKA_CONF_FILE\\\"\\\n          \\ \\\"interbroker-password-placeholder\\\" \\\"$KAFKA_INTER_BROKER_PASSWORD\\\"\\n\\\n          \\    replace_in_file \\\"$KAFKA_CONF_FILE\\\" \\\"controller-password-placeholder\\\"\\\n          \\ \\\"$KAFKA_CONTROLLER_PASSWORD\\\"\\n    read -r -a passwords <<< \\\"$(tr ',;'\\\n          \\ ' ' <<<\\\"${KAFKA_CLIENT_PASSWORDS:-}\\\")\\\"\\n    for ((i = 0; i < ${#passwords[@]};\\\n          \\ i++)); do\\n        replace_in_file \\\"$KAFKA_CONF_FILE\\\" \\\"password-placeholder-${i}\\\\\\\n          \\\"\\\" \\\"${passwords[i]}\\\\\\\"\\\"\\n    done\\n}\\n\\ncp /configmaps/server.properties\\\n          \\ $KAFKA_CONF_FILE\\n\\n# Get pod ID and role, last and second last fields\\\n          \\ in the pod name respectively\\nPOD_ID=\\\"${MY_POD_NAME##*-}\\\"\\nPOD_ROLE=\\\"\\\n          ${MY_POD_NAME%-*}\\\"; POD_ROLE=\\\"${POD_ROLE##*-}\\\"\\n\\n# Configure node.id\\n\\\n          ID=$((POD_ID + KAFKA_MIN_ID))\\n[[ -f \\\"/bitnami/kafka/data/meta.properties\\\"\\\n          \\ ]] && ID=\\\"$(grep \\\"node.id\\\" /bitnami/kafka/data/meta.properties | awk\\\n          \\ -F '=' '{print $2}')\\\"\\nkafka_server_conf_set \\\"node.id\\\" \\\"$ID\\\"\\n# Configure\\\n          \\ initial controllers\\nif [[ \\\"controller\\\" =~ \\\"$POD_ROLE\\\" ]]; then\\n\\\n          \\    INITIAL_CONTROLLERS=()\\n    for ((i = 0; i < 3; i++)); do\\n       \\\n          \\ var=\\\"KAFKA_CONTROLLER_${i}_DIR_ID\\\"; DIR_ID=\\\"${!var}\\\"\\n        [[ $i\\\n          \\ -eq $POD_ID ]] && [[ -f \\\"/bitnami/kafka/data/meta.properties\\\" ]] &&\\\n          \\ DIR_ID=\\\"$(grep \\\"directory.id\\\" /bitnami/kafka/data/meta.properties |\\\n          \\ awk -F '=' '{print $2}')\\\"\\n        INITIAL_CONTROLLERS+=(\\\"${i}@${KAFKA_FULLNAME}-${POD_ROLE}-${i}.${KAFKA_CONTROLLER_SVC_NAME}.${MY_POD_NAMESPACE}.svc.${CLUSTER_DOMAIN}:${KAFKA_CONTROLLER_PORT}:${DIR_ID}\\\"\\\n          )\\n    done\\n    echo \\\"${INITIAL_CONTROLLERS[*]}\\\" | awk -v OFS=',' '{$1=$1}1'\\\n          \\ > /shared/initial-controllers.txt\\nfi\\nreplace_in_file \\\"$KAFKA_CONF_FILE\\\"\\\n          \\ \\\"advertised-address-placeholder\\\" \\\"${MY_POD_NAME}.${KAFKA_FULLNAME}-${POD_ROLE}-headless.${MY_POD_NAMESPACE}.svc.${CLUSTER_DOMAIN}\\\"\\\n          \\nsasl_env_vars=(\\n  KAFKA_CLIENT_PASSWORDS\\n  KAFKA_INTER_BROKER_PASSWORD\\n\\\n          \\  KAFKA_INTER_BROKER_CLIENT_SECRET\\n  KAFKA_CONTROLLER_PASSWORD\\n  KAFKA_CONTROLLER_CLIENT_SECRET\\n\\\n          )\\nfor env_var in \\\"${sasl_env_vars[@]}\\\"; do\\n    file_env_var=\\\"${env_var}_FILE\\\"\\\n          \\n    if [[ -n \\\"${!file_env_var:-}\\\" ]]; then\\n        if [[ -r \\\"${!file_env_var:-}\\\"\\\n          \\ ]]; then\\n            export \\\"${env_var}=$(< \\\"${!file_env_var}\\\")\\\"\\n\\\n          \\            unset \\\"${file_env_var}\\\"\\n        else\\n            warn \\\"\\\n          Skipping export of '${env_var}'. '${!file_env_var:-}' is not readable.\\\"\\\n          \\n        fi\\n    fi\\ndone\\nconfigure_kafka_sasl\\nif [[ -f /secret-config/server-secret.properties\\\n          \\ ]]; then\\n    cat /secret-config/server-secret.properties >> $KAFKA_CONF_FILE\\n\\\n          fi\\n\"\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KAFKA_FULLNAME\n          value: release-name-kafka\n        - name: CLUSTER_DOMAIN\n          value: cluster.local\n        - name: KAFKA_VOLUME_DIR\n          value: /bitnami/kafka\n        - name: KAFKA_CONF_FILE\n          value: /config/server.properties\n        - name: KAFKA_MIN_ID\n          value: '0'\n        - name: KAFKA_CONTROLLER_SVC_NAME\n          value: release-name-kafka-controller-headless\n        - name: KAFKA_CONTROLLER_PORT\n          value: '9093'\n        - name: KAFKA_CONTROLLER_0_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-0-id\n        - name: KAFKA_CONTROLLER_1_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-1-id\n        - name: KAFKA_CONTROLLER_2_DIR_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: controller-2-id\n        - name: KAFKA_CLIENT_USERS\n          value: user1\n        - name: KAFKA_CLIENT_PASSWORDS_FILE\n          value: /opt/bitnami/kafka/config/secrets/client-passwords\n        - name: KAFKA_INTER_BROKER_USER\n          value: inter_broker_user\n        - name: KAFKA_INTER_BROKER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/inter-broker-password\n        - name: KAFKA_CONTROLLER_USER\n          value: controller_user\n        - name: KAFKA_CONTROLLER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/controller-password\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/kafka\n        - name: kafka-config\n          mountPath: /config\n        - name: kafka-configmaps\n          mountPath: /configmaps\n        - name: kafka-secret-config\n          mountPath: /secret-config\n        - name: tmp\n          mountPath: /tmp\n        - name: init-shared\n          mountPath: /shared\n        - name: kafka-sasl\n          mountPath: /opt/bitnami/kafka/config/secrets\n          readOnly: true\n      containers:\n      - name: kafka\n        image: docker.io/bitnami/kafka:4.0.0-debian-12-r10\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n        env:\n        - name: KAFKA_HEAP_OPTS\n          value: -XX:InitialRAMPercentage=75 -XX:MaxRAMPercentage=75\n        - name: KAFKA_CFG_PROCESS_ROLES\n          value: controller,broker\n        - name: KAFKA_INITIAL_CONTROLLERS_FILE\n          value: /shared/initial-controllers.txt\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: KAFKA_KRAFT_CLUSTER_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kafka-kraft\n              key: cluster-id\n        - name: KAFKA_KRAFT_BOOTSTRAP_SCRAM_USERS\n          value: 'true'\n        - name: KAFKA_CLIENT_USERS\n          value: user1\n        - name: KAFKA_CLIENT_PASSWORDS_FILE\n          value: /opt/bitnami/kafka/config/secrets/client-passwords\n        - name: KAFKA_INTER_BROKER_USER\n          value: inter_broker_user\n        - name: KAFKA_INTER_BROKER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/inter-broker-password\n        - name: KAFKA_CONTROLLER_USER\n          value: controller_user\n        - name: KAFKA_CONTROLLER_PASSWORD_FILE\n          value: /opt/bitnami/kafka/config/secrets/controller-password\n        ports:\n        - name: controller\n          containerPort: 9093\n        - name: client\n          containerPort: 9092\n        - name: interbroker\n          containerPort: 9094\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - pgrep\n            - -f\n            - kafka\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: controller\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/kafka\n        - name: logs\n          mountPath: /opt/bitnami/kafka/logs\n        - name: kafka-config\n          mountPath: /opt/bitnami/kafka/config/server.properties\n          subPath: server.properties\n        - name: tmp\n          mountPath: /tmp\n        - name: init-shared\n          mountPath: /shared\n        - name: kafka-sasl\n          mountPath: /opt/bitnami/kafka/config/secrets\n          readOnly: true\n      volumes:\n      - name: kafka-configmaps\n        configMap:\n          name: release-name-kafka-controller-configuration\n      - name: kafka-secret-config\n        emptyDir: {}\n      - name: kafka-config\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      - name: init-shared\n        emptyDir: {}\n      - name: kafka-sasl\n        projected:\n          sources:\n          - secret:\n              name: release-name-kafka-user-passwords\n      - name: logs\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kafka\" not found"
  },
  {
    "id": "202",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/003_poddisruptionbudget_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "203",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/004_poddisruptionbudget_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: keycloak\n      app.kubernetes.io/component: keycloak\n      app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "204",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/010_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "205",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/011_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "206",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/012_service_release-name-keycloak-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-keycloak-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: http\n    port: 8080\n    protocol: TCP\n    targetPort: http\n  publishNotReadyAddresses: true\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:keycloak app.kubernetes.io/instance:release-name app.kubernetes.io/name:keycloak app.kubernetes.io/part-of:keycloak])"
  },
  {
    "id": "207",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/013_service_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:keycloak app.kubernetes.io/instance:release-name app.kubernetes.io/name:keycloak app.kubernetes.io/part-of:keycloak])"
  },
  {
    "id": "208",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/014_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-16.7.26\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 17.6.0\n        helm.sh/chart: postgresql-16.7.26\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnami/postgresql:17.6.0-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_USER\n          value: bn_keycloak\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/password\n        - name: POSTGRES_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRES_DATABASE\n          value: bitnami_keycloak\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"bn_keycloak\" -d \"dbname=bitnami_keycloak\" -h 127.0.0.1\n              -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"bn_keycloak\" -d \"dbname=bitnami_keycloak\" -h 127.0.0.1\n              -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-postgresql\" not found"
  },
  {
    "id": "209",
    "manifest_path": "data/manifests/artifacthub/bitnami/keycloak/015_statefulset_release-name-keycloak.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-keycloak\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: keycloak\n    app.kubernetes.io/version: 26.3.3\n    helm.sh/chart: keycloak-25.2.0\n    app.kubernetes.io/component: keycloak\n    app.kubernetes.io/part-of: keycloak\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  podManagementPolicy: Parallel\n  serviceName: release-name-keycloak-headless\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: keycloak\n      app.kubernetes.io/component: keycloak\n      app.kubernetes.io/part-of: keycloak\n  template:\n    metadata:\n      annotations:\n        checksum/configmap-env-vars: 32b97b2f95a4b4c37d1e7ba71916ca4c1f73d024fd8a3e077f2c86fba821b469\n        checksum/secrets: 04a6ccee11f98c05bad75a87ca3f46ce99e265b4b5d35d83a1a541641c64d855\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: keycloak\n        app.kubernetes.io/version: 26.3.3\n        helm.sh/chart: keycloak-25.2.0\n        app.kubernetes.io/component: keycloak\n        app.kubernetes.io/part-of: keycloak\n    spec:\n      serviceAccountName: release-name-keycloak\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: keycloak\n                  app.kubernetes.io/component: keycloak\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-write-dirs\n        image: docker.io/bitnami/keycloak:26.3.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - '. /opt/bitnami/scripts/liblog.sh\n\n\n          info \"Copying writable dirs to empty dir\"\n\n          # In order to not break the application functionality we need to make some\n\n          # directories writable, so we need to copy it to an empty dir volume\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/lib/quarkus /emptydir/app-quarkus-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/data /emptydir/app-data-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/providers /emptydir/app-providers-dir\n\n          cp -r --preserve=mode,timestamps /opt/bitnami/keycloak/themes /emptydir/app-themes-dir\n\n          info \"Copy operation completed\"\n\n          '\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: keycloak\n        image: docker.io/bitnami/keycloak:26.3.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KUBERNETES_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        envFrom:\n        - configMapRef:\n            name: release-name-keycloak-env-vars\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        - name: discovery\n          containerPort: 7800\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 1\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          httpGet:\n            path: /realms/master\n            port: http\n            scheme: HTTP\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /bitnami/keycloak\n          subPath: app-volume-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/lib/quarkus\n          subPath: app-quarkus-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/data\n          subPath: app-data-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/providers\n          subPath: app-providers-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/keycloak/themes\n          subPath: app-themes-dir\n        - name: keycloak-secrets\n          mountPath: /opt/bitnami/keycloak/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: keycloak-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-keycloak\n          - secret:\n              name: release-name-postgresql\n              items:\n              - key: password\n                path: db-password\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-keycloak\" not found"
  },
  {
    "id": "210",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/002_poddisruptionbudget_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "211",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/006_service_release-name-mariadb-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\nspec:\n  type: ClusterIP\n  publishNotReadyAddresses: true\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/part-of: mariadb\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb app.kubernetes.io/part-of:mariadb])"
  },
  {
    "id": "212",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/007_service_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb])"
  },
  {
    "id": "213",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "214",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "215",
    "manifest_path": "data/manifests/artifacthub/bitnami/mariadb/008_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-23.0.4\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: a73d0da9839c0886aa9e36d38eecc9587f379829e4b835933fcf64c0eba3b1f5\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-23.0.4\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_DATABASE\n          value: my_database\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-mariadb\" not found"
  },
  {
    "id": "216",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/003_poddisruptionbudget_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: console\n      app.kubernetes.io/part-of: minio\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "217",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/004_poddisruptionbudget_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: minio\n      app.kubernetes.io/part-of: minio\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "218",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/008_service_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    port: 9090\n    targetPort: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:console app.kubernetes.io/instance:release-name app.kubernetes.io/name:minio app.kubernetes.io/part-of:minio])"
  },
  {
    "id": "219",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/009_service_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  type: ClusterIP\n  ports:\n  - name: tcp-api\n    port: 9000\n    targetPort: api\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:minio app.kubernetes.io/instance:release-name app.kubernetes.io/name:minio app.kubernetes.io/part-of:minio])"
  },
  {
    "id": "220",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/010_deployment_release-name-minio.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-minio\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2025.7.23\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: minio\n    app.kubernetes.io/part-of: minio\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: minio\n      app.kubernetes.io/part-of: minio\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: minio\n        app.kubernetes.io/version: 2025.7.23\n        helm.sh/chart: minio-17.0.21\n        app.kubernetes.io/component: minio\n        app.kubernetes.io/part-of: minio\n      annotations:\n        checksum/credentials-secret: d7f9b363039fe1a5911ad90a3de83dbcfe0eba441592f6815c91be1861016459\n    spec:\n      serviceAccountName: release-name-minio\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: minio\n                  app.kubernetes.io/component: minio\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: OnRootMismatch\n        supplementalGroups: []\n        sysctls: []\n      initContainers: null\n      containers:\n      - name: minio\n        image: docker.io/bitnami/minio:2025.7.23-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MINIO_DISTRIBUTED_MODE_ENABLED\n          value: 'no'\n        - name: MINIO_SCHEME\n          value: http\n        - name: MINIO_FORCE_NEW_KEYS\n          value: 'no'\n        - name: MINIO_ROOT_USER_FILE\n          value: /opt/bitnami/minio/secrets/root-user\n        - name: MINIO_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/minio/secrets/root-password\n        - name: MINIO_SKIP_CLIENT\n          value: 'yes'\n        - name: MINIO_API_PORT_NUMBER\n          value: '9000'\n        - name: MINIO_BROWSER\n          value: 'off'\n        - name: MINIO_PROMETHEUS_AUTH_TYPE\n          value: public\n        - name: MINIO_DATA_DIR\n          value: /bitnami/minio/data\n        ports:\n        - name: api\n          containerPort: 9000\n        livenessProbe:\n          httpGet:\n            path: /minio/health/live\n            port: api\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 5\n        readinessProbe:\n          tcpSocket:\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/minio/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /.mc\n          subPath: app-mc-dir\n        - name: minio-credentials\n          mountPath: /opt/bitnami/minio/secrets/\n        - name: data\n          mountPath: /bitnami/minio/data\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: minio-credentials\n        secret:\n          secretName: release-name-minio\n      - name: data\n        persistentVolumeClaim:\n          claimName: release-name-minio\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-minio\" not found"
  },
  {
    "id": "221",
    "manifest_path": "data/manifests/artifacthub/bitnami/minio/011_deployment_release-name-minio-console.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-minio-console\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: minio\n    app.kubernetes.io/version: 2.0.2\n    helm.sh/chart: minio-17.0.21\n    app.kubernetes.io/component: console\n    app.kubernetes.io/part-of: minio\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: minio\n      app.kubernetes.io/component: console\n      app.kubernetes.io/part-of: minio\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: minio\n        app.kubernetes.io/version: 2025.7.23\n        helm.sh/chart: minio-17.0.21\n        app.kubernetes.io/component: console\n        app.kubernetes.io/part-of: minio\n    spec:\n      serviceAccountName: release-name-minio\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: minio\n                  app.kubernetes.io/component: console\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: console\n        image: docker.io/bitnami/minio-object-browser:2.0.2-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - server\n        - --host\n        - 0.0.0.0\n        - --port\n        - '9090'\n        env:\n        - name: CONSOLE_MINIO_SERVER\n          value: http://release-name-minio:9000\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        ports:\n        - name: http\n          containerPort: 9090\n        livenessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          httpGet:\n            path: /minio\n            port: http\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /.console\n          subPath: app-console-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-minio\" not found"
  },
  {
    "id": "222",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/002_poddisruptionbudget_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "223",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/007_service_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  publishNotReadyAddresses: false\n  ports:\n  - name: mongodb\n    port: 27017\n    targetPort: mongodb\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/component: mongodb\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:mongodb app.kubernetes.io/instance:release-name app.kubernetes.io/name:mongodb])"
  },
  {
    "id": "224",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"log-dir\" is using an invalid container image, \"registry-1.docker.io/bitnami/mongodb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "225",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"mongodb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mongodb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "226",
    "manifest_path": "data/manifests/artifacthub/bitnami/mongodb/008_deployment_release-name-mongodb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-mongodb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mongodb\n    app.kubernetes.io/version: 8.2.1\n    helm.sh/chart: mongodb-18.0.5\n    app.kubernetes.io/component: mongodb\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mongodb\n      app.kubernetes.io/component: mongodb\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mongodb\n        app.kubernetes.io/version: 8.2.1\n        helm.sh/chart: mongodb-18.0.5\n        app.kubernetes.io/component: mongodb\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mongodb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mongodb\n                  app.kubernetes.io/component: mongodb\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      enableServiceLinks: true\n      initContainers:\n      - name: log-dir\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - 'ln -sf /dev/stdout \"/opt/bitnami/mongodb/logs/mongodb.log\"\n\n          '\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n      containers:\n      - name: mongodb\n        image: registry-1.docker.io/bitnami/mongodb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MONGODB_ROOT_USER\n          value: root\n        - name: MONGODB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mongodb/secrets/mongodb-root-password\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: MONGODB_SYSTEM_LOG_VERBOSITY\n          value: '0'\n        - name: MONGODB_DISABLE_SYSTEM_LOG\n          value: 'no'\n        - name: MONGODB_DISABLE_JAVASCRIPT\n          value: 'no'\n        - name: MONGODB_ENABLE_JOURNAL\n          value: 'yes'\n        - name: MONGODB_PORT_NUMBER\n          value: '27017'\n        - name: MONGODB_ENABLE_IPV6\n          value: 'no'\n        - name: MONGODB_ENABLE_DIRECTORY_PER_DB\n          value: 'no'\n        ports:\n        - name: mongodb\n          containerPort: 27017\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          successThreshold: 1\n          timeoutSeconds: 10\n          exec:\n            command:\n            - /bitnami/scripts/ping-mongodb.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bitnami/scripts/readiness-probe.sh\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mongodb/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /.mongodb\n          subPath: mongosh-home\n        - name: datadir\n          mountPath: /bitnami/mongodb\n          subPath: null\n        - name: common-scripts\n          mountPath: /bitnami/scripts\n        - name: mongodb-secrets\n          mountPath: /opt/bitnami/mongodb/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: common-scripts\n        configMap:\n          name: release-name-mongodb-common-scripts\n          defaultMode: 360\n      - name: mongodb-secrets\n        secret:\n          secretName: release-name-mongodb\n      - name: datadir\n        persistentVolumeClaim:\n          claimName: release-name-mongodb\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-mongodb\" not found"
  },
  {
    "id": "227",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/002_poddisruptionbudget_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mysql\n      app.kubernetes.io/part-of: mysql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "228",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/006_service_release-name-mysql-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mysql-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: mysql\n    port: 3306\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mysql])"
  },
  {
    "id": "229",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/007_service_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mysql app.kubernetes.io/part-of:mysql])"
  },
  {
    "id": "230",
    "manifest_path": "data/manifests/artifacthub/bitnami/mysql/008_statefulset_release-name-mysql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mysql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mysql\n    app.kubernetes.io/version: 9.4.0\n    helm.sh/chart: mysql-14.0.3\n    app.kubernetes.io/part-of: mysql\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  podManagementPolicy: ''\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mysql\n      app.kubernetes.io/part-of: mysql\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mysql-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 0aa4c7bb029f4871ca0cdece35adf5a5caaf6f2e016ef25c8cae18901c047e48\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mysql\n        app.kubernetes.io/version: 9.4.0\n        helm.sh/chart: mysql-14.0.3\n        app.kubernetes.io/part-of: mysql\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-mysql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mysql\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: docker.io/bitnami/mysql:9.4.0-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mysql/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mysql/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mysql\n        image: docker.io/bitnami/mysql:9.4.0-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MYSQL_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mysql/secrets/mysql-root-password\n        - name: MYSQL_ENABLE_SSL\n          value: 'no'\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_DATABASE\n          value: my_database\n        envFrom: null\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin ping -uroot -p\\\"${password_aux}\\\" | grep \\\"mysqld is\\\n              \\ alive\\\"\\n\"\n        startupProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MYSQL_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MYSQL_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MYSQL_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmysqladmin ping -uroot -p\\\"${password_aux}\\\" | grep \\\"mysqld is\\\n              \\ alive\\\"\\n\"\n        resources:\n          limits:\n            cpu: 750m\n            ephemeral-storage: 2Gi\n            memory: 768Mi\n          requests:\n            cpu: 500m\n            ephemeral-storage: 50Mi\n            memory: 512Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mysql\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mysql/logs\n          subPath: app-logs-dir\n        - name: config\n          mountPath: /opt/bitnami/mysql/conf/my.cnf\n          subPath: my.cnf\n        - name: mysql-credentials\n          mountPath: /opt/bitnami/mysql/secrets/\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-mysql\n      - name: mysql-credentials\n        secret:\n          secretName: release-name-mysql\n          items:\n          - key: mysql-root-password\n            path: mysql-root-password\n          - key: mysql-password\n            path: mysql-password\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mysql\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-mysql\" not found"
  },
  {
    "id": "231",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/002_poddisruptionbudget_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "232",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/005_service_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\n  annotations: null\nspec:\n  type: LoadBalancer\n  sessionAffinity: None\n  externalTrafficPolicy: Cluster\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n  - name: https\n    port: 443\n    targetPort: https\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: nginx\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:nginx])"
  },
  {
    "id": "233",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"registry-1.docker.io/bitnami/nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "234",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "235",
    "manifest_path": "data/manifests/artifacthub/bitnami/nginx/006_deployment_release-name-nginx.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: nginx\n    app.kubernetes.io/version: 1.29.1\n    helm.sh/chart: nginx-22.0.7\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: nginx\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: nginx\n        app.kubernetes.io/version: 1.29.1\n        helm.sh/chart: nginx-22.0.7\n      annotations: null\n    spec:\n      shareProcessNamespace: false\n      serviceAccountName: release-name-nginx\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: nginx\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      hostNetwork: false\n      hostIPC: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/nginx/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/nginx/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: nginx\n        image: registry-1.docker.io/bitnami/nginx:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: NGINX_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: NGINX_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: http\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n          httpGet:\n            path: /\n            port: http\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/nginx/tmp\n          subPath: app-tmp-dir\n        - name: certificate\n          mountPath: /certs\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: certificate\n        secret:\n          secretName: release-name-nginx-tls\n          items:\n          - key: tls.crt\n            path: tls.crt\n          - key: tls.key\n            path: tls.key\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-nginx\" not found"
  },
  {
    "id": "236",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/002_poddisruptionbudget_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "237",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/005_service_release-name-postgresql-hl.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-hl\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "238",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/006_service_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-postgresql\n    port: 5432\n    targetPort: tcp-postgresql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql])"
  },
  {
    "id": "239",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"postgresql\" is using an invalid container image, \"registry-1.docker.io/bitnami/postgresql:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "240",
    "manifest_path": "data/manifests/artifacthub/bitnami/postgresql/007_statefulset_release-name-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql\n    app.kubernetes.io/version: 18.0.0\n    helm.sh/chart: postgresql-18.0.8\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  serviceName: release-name-postgresql-hl\n  updateStrategy:\n    rollingUpdate: {}\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql\n      app.kubernetes.io/component: primary\n  template:\n    metadata:\n      name: release-name-postgresql\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql\n        app.kubernetes.io/version: 18.0.0\n        helm.sh/chart: postgresql-18.0.8\n        app.kubernetes.io/component: primary\n    spec:\n      serviceAccountName: release-name-postgresql\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: registry-1.docker.io/bitnami/postgresql:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRESQL_ENABLE_LDAP\n          value: 'no'\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'false'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit\n        ports:\n        - name: tcp-postgresql\n          containerPort: 5432\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /bin/sh\n            - -c\n            - -e\n            - 'exec pg_isready -U \"postgres\" -h 127.0.0.1 -p 5432\n\n              [ -f /opt/bitnami/postgresql/tmp/.initialized ] || [ -f /bitnami/postgresql/.initialized\n              ]\n\n              '\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: postgresql-password\n          mountPath: /opt/bitnami/postgresql/secrets/\n        - name: dshm\n          mountPath: /dev/shm\n        - name: data\n          mountPath: /bitnami/postgresql\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-password\n        secret:\n          secretName: release-name-postgresql\n      - name: dshm\n        emptyDir:\n          medium: Memory\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-postgresql\" not found"
  },
  {
    "id": "241",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/002_poddisruptionbudget_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: rabbitmq\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "242",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/008_service_release-name-rabbitmq-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-rabbitmq-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  clusterIP: None\n  ports:\n  - name: epmd\n    port: 4369\n    targetPort: epmd\n  - name: amqp\n    port: 5672\n    targetPort: amqp\n  - name: dist\n    port: 25672\n    targetPort: dist\n  - name: http-stats\n    port: 15672\n    targetPort: stats\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: rabbitmq\n  publishNotReadyAddresses: true\n  trafficDistribution: PreferClose\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:rabbitmq])"
  },
  {
    "id": "243",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/009_service_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: amqp\n    port: 5672\n    targetPort: amqp\n    nodePort: null\n  - name: epmd\n    port: 4369\n    targetPort: epmd\n    nodePort: null\n  - name: dist\n    port: 25672\n    targetPort: dist\n    nodePort: null\n  - name: http-stats\n    port: 15672\n    targetPort: stats\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: rabbitmq\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:rabbitmq])"
  },
  {
    "id": "244",
    "manifest_path": "data/manifests/artifacthub/bitnami/rabbitmq/010_statefulset_release-name-rabbitmq.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-rabbitmq\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: rabbitmq\n    app.kubernetes.io/version: 4.1.3\n    helm.sh/chart: rabbitmq-16.0.14\nspec:\n  serviceName: release-name-rabbitmq-headless\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: rabbitmq\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: rabbitmq\n        app.kubernetes.io/version: 4.1.3\n        helm.sh/chart: rabbitmq-16.0.14\n      annotations:\n        checksum/config: 81f24711d28981f706e1ae5b2e3ae075d7014b462120496ce6f50d5053194f5e\n        checksum/secret: 0db9412a28617166460cac5333a5cf8ebbc34cfe87087e0f09b593395c5fa678\n    spec:\n      serviceAccountName: release-name-rabbitmq\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: rabbitmq\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      automountServiceAccountToken: true\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      terminationGracePeriodSeconds: 120\n      enableServiceLinks: true\n      initContainers:\n      - name: prepare-plugins-dir\n        image: docker.io/bitnami/rabbitmq:4.1.3-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - '#!/bin/bash\n\n\n          . /opt/bitnami/scripts/liblog.sh\n\n\n          info \"Copying plugins dir to empty dir\"\n\n          # In order to not break the possibility of installing custom plugins, we\n          need\n\n          # to make the plugins directory writable, so we need to copy it to an empty\n          dir volume\n\n          cp -r --preserve=mode /opt/bitnami/rabbitmq/plugins/ /emptydir/app-plugins-dir\n\n          '\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: rabbitmq\n        image: docker.io/bitnami/rabbitmq:4.1.3-debian-12-r1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/bash\n              - -ec\n              - \"if [[ -f /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh ]]; then\\n\\\n                \\    /opt/bitnami/scripts/rabbitmq/nodeshutdown.sh -t \\\"120\\\" -d \\\"\\\n                false\\\"\\nelse\\n    rabbitmqctl stop_app\\nfi\\n\"\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MY_POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: RABBITMQ_FORCE_BOOT\n          value: 'no'\n        - name: RABBITMQ_NODE_NAME\n          value: rabbit@$(MY_POD_NAME).release-name-rabbitmq-headless.$(MY_POD_NAMESPACE).svc.cluster.local\n        - name: RABBITMQ_UPDATE_PASSWORD\n          value: 'no'\n        - name: RABBITMQ_MNESIA_DIR\n          value: /opt/bitnami/rabbitmq/.rabbitmq/mnesia/$(RABBITMQ_NODE_NAME)\n        - name: RABBITMQ_LDAP_ENABLE\n          value: 'no'\n        - name: RABBITMQ_LOGS\n          value: '-'\n        - name: RABBITMQ_ULIMIT_NOFILES\n          value: '65535'\n        - name: RABBITMQ_USE_LONGNAME\n          value: 'true'\n        - name: RABBITMQ_ERL_COOKIE_FILE\n          value: /opt/bitnami/rabbitmq/secrets/rabbitmq-erlang-cookie\n        - name: RABBITMQ_LOAD_DEFINITIONS\n          value: 'no'\n        - name: RABBITMQ_DEFINITIONS_FILE\n          value: /app/load_definition.json\n        - name: RABBITMQ_SECURE_PASSWORD\n          value: 'yes'\n        - name: RABBITMQ_USERNAME\n          value: user\n        - name: RABBITMQ_PASSWORD_FILE\n          value: /opt/bitnami/rabbitmq/secrets/rabbitmq-password\n        - name: RABBITMQ_PLUGINS\n          value: rabbitmq_management, rabbitmq_peer_discovery_k8s, rabbitmq_auth_backend_ldap\n        envFrom: null\n        ports:\n        - name: amqp\n          containerPort: 5672\n        - name: dist\n          containerPort: 25672\n        - name: stats\n          containerPort: 15672\n        - name: epmd\n          containerPort: 4369\n        - name: metrics\n          containerPort: 9419\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 20\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - curl -f --user user:$(< $RABBITMQ_PASSWORD_FILE) 127.0.0.1:15672/api/health/checks/virtual-hosts\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 20\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - curl -f --user user:$(< $RABBITMQ_PASSWORD_FILE) 127.0.0.1:15672/api/health/checks/local-alarms\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: configuration\n          mountPath: /bitnami/rabbitmq/conf\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/etc/rabbitmq\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/var/lib/rabbitmq\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/.rabbitmq/\n          subPath: app-erlang-cookie\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/var/log/rabbitmq\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/rabbitmq/plugins\n          subPath: app-plugins-dir\n        - name: data\n          mountPath: /opt/bitnami/rabbitmq/.rabbitmq/mnesia\n        - name: rabbitmq-secrets\n          mountPath: /opt/bitnami/rabbitmq/secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: configuration\n        projected:\n          sources:\n          - secret:\n              name: release-name-rabbitmq-config\n      - name: rabbitmq-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-rabbitmq\n          - secret:\n              name: release-name-rabbitmq\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: rabbitmq\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-rabbitmq\" not found"
  },
  {
    "id": "245",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/002_poddisruptionbudget_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "246",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/003_poddisruptionbudget_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "247",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/010_service_release-name-redis-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "248",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/011_service_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  type: ClusterIP\n  internalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/component: master\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:master app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "249",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/012_service_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  type: ClusterIP\n  internalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: redis\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/component: replica\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:replica app.kubernetes.io/instance:release-name app.kubernetes.io/name:redis])"
  },
  {
    "id": "250",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"registry-1.docker.io/bitnami/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "251",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/013_statefulset_release-name-redis-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-master\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: master\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: master\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: master\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-master\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: master\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-master.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: master\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-redis-master\" not found"
  },
  {
    "id": "252",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"registry-1.docker.io/bitnami/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "253",
    "manifest_path": "data/manifests/artifacthub/bitnami/redis/014_statefulset_release-name-redis-replicas.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-redis-replicas\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: redis\n    app.kubernetes.io/version: 8.2.2\n    helm.sh/chart: redis-23.1.1\n    app.kubernetes.io/component: replica\nspec:\n  replicas: 3\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: redis\n      app.kubernetes.io/component: replica\n  serviceName: release-name-redis-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/version: 8.2.2\n        helm.sh/chart: redis-23.1.1\n        app.kubernetes.io/component: replica\n      annotations:\n        checksum/configmap: 95944ee35a19af37cefd156f19d93e157e578c1dc63ac01ce735ae93090b7580\n        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9\n        checksum/scripts: 796326939380261bbef30ebf93cef0ad202f8440491c7bfce6ca532637bd0271\n        checksum/secret: e604af60410e66fdbde895aaa50ef67a9838938e9197616eab6627edea05bc2a\n    spec:\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-redis-replica\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: redis\n                  app.kubernetes.io/component: replica\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      enableServiceLinks: true\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: redis\n        image: registry-1.docker.io/bitnami/redis:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - /opt/bitnami/scripts/start-scripts/start-replica.sh\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REDIS_REPLICATION_MODE\n          value: replica\n        - name: REDIS_MASTER_HOST\n          value: release-name-redis-master-0.release-name-redis-headless.default.svc.cluster.local\n        - name: REDIS_MASTER_PORT_NUMBER\n          value: '6379'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'no'\n        - name: REDIS_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_MASTER_PASSWORD_FILE\n          value: /opt/bitnami/redis/secrets/redis-password\n        - name: REDIS_TLS_ENABLED\n          value: 'no'\n        - name: REDIS_PORT\n          value: '6379'\n        ports:\n        - name: redis\n          containerPort: 6379\n        startupProbe:\n          failureThreshold: 22\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          tcpSocket:\n            port: redis\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_liveness_local_and_master.sh 5\n        readinessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - /health/ping_readiness_local_and_master.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: start-scripts\n          mountPath: /opt/bitnami/scripts/start-scripts\n        - name: health\n          mountPath: /health\n        - name: redis-password\n          mountPath: /opt/bitnami/redis/secrets/\n        - name: redis-data\n          mountPath: /data\n        - name: config\n          mountPath: /opt/bitnami/redis/mounted-etc\n        - name: empty-dir\n          mountPath: /opt/bitnami/redis/etc\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: start-scripts\n        configMap:\n          name: release-name-redis-scripts\n          defaultMode: 493\n      - name: health\n        configMap:\n          name: release-name-redis-health\n          defaultMode: 493\n      - name: redis-password\n        secret:\n          secretName: release-name-redis\n          items:\n          - key: redis-password\n            path: redis-password\n      - name: config\n        configMap:\n          name: release-name-redis-configuration\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: redis-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: redis\n        app.kubernetes.io/component: replica\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-redis-replica\" not found"
  },
  {
    "id": "254",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/003_poddisruptionbudget_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query-frontend\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "255",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/004_poddisruptionbudget_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "256",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/007_service_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9090\n    targetPort: http\n    protocol: TCP\n    name: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query-frontend\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query-frontend app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "257",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/008_service_release-name-thanos-query-grpc.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query-grpc\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  type: ClusterIP\n  ports:\n  - port: 10901\n    targetPort: grpc\n    protocol: TCP\n    name: grpc\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "258",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/009_service_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9090\n    targetPort: http\n    protocol: TCP\n    name: http\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/component: query\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:query app.kubernetes.io/instance:release-name app.kubernetes.io/name:thanos])"
  },
  {
    "id": "259",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/010_deployment_release-name-thanos-query-frontend.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-thanos-query-frontend\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query-frontend\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query-frontend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: thanos\n        app.kubernetes.io/version: 0.39.2\n        helm.sh/chart: thanos-17.3.1\n        app.kubernetes.io/component: query-frontend\n    spec:\n      serviceAccountName: release-name-thanos-query-frontend\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: thanos\n                  app.kubernetes.io/component: query-frontend\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: query-frontend\n        image: docker.io/bitnami/thanos:0.39.2-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - query-frontend\n        - --log.level=info\n        - --log.format=logfmt\n        - --http-address=0.0.0.0:9090\n        - --query-frontend.downstream-url=http://release-name-thanos-query:9090\n        ports:\n        - name: http\n          containerPort: 9090\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/healthy\n            port: http\n            scheme: HTTP\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/ready\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts: null\n      volumes: null\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-thanos-query-frontend\" not found"
  },
  {
    "id": "260",
    "manifest_path": "data/manifests/artifacthub/bitnami/thanos/011_deployment_release-name-thanos-query.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-thanos-query\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: thanos\n    app.kubernetes.io/version: 0.39.2\n    helm.sh/chart: thanos-17.3.1\n    app.kubernetes.io/component: query\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: thanos\n      app.kubernetes.io/component: query\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: thanos\n        app.kubernetes.io/version: 0.39.2\n        helm.sh/chart: thanos-17.3.1\n        app.kubernetes.io/component: query\n    spec:\n      serviceAccountName: release-name-thanos-query\n      automountServiceAccountToken: true\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: thanos\n                  app.kubernetes.io/component: query\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      containers:\n      - name: query\n        image: docker.io/bitnami/thanos:0.39.2-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        args:\n        - query\n        - --log.level=info\n        - --log.format=logfmt\n        - --grpc-address=0.0.0.0:10901\n        - --http-address=0.0.0.0:10902\n        - --query.replica-label=replica\n        - --alert.query-url=http://release-name-thanos-query.default.svc.cluster.local:9090\n        ports:\n        - name: http\n          containerPort: 10902\n          protocol: TCP\n        - name: grpc\n          containerPort: 10901\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/healthy\n            port: http\n            scheme: HTTP\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 30\n          httpGet:\n            path: /-/ready\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts: null\n      volumes: null\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-thanos-query\" not found"
  },
  {
    "id": "261",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/003_poddisruptionbudget_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/component: primary\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "262",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/004_poddisruptionbudget_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "263",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/011_service_release-name-mariadb-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\nspec:\n  type: ClusterIP\n  publishNotReadyAddresses: true\n  clusterIP: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/part-of: mariadb\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb app.kubernetes.io/part-of:mariadb])"
  },
  {
    "id": "264",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/012_service_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\n  annotations: null\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: mysql\n    port: 3306\n    protocol: TCP\n    targetPort: mysql\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/component: primary\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:primary app.kubernetes.io/instance:release-name app.kubernetes.io/name:mariadb])"
  },
  {
    "id": "265",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/013_service_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  type: LoadBalancer\n  externalTrafficPolicy: Cluster\n  sessionAffinity: None\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: wordpress\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:wordpress])"
  },
  {
    "id": "266",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"prepare-base-dir\" is using an invalid container image, \"registry-1.docker.io/bitnami/wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "267",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"wordpress\" is using an invalid container image, \"registry-1.docker.io/bitnami/wordpress:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "268",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/014_deployment_release-name-wordpress.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-wordpress\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: wordpress\n    app.kubernetes.io/version: 6.8.3\n    helm.sh/chart: wordpress-27.0.7\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: wordpress\n  strategy:\n    type: RollingUpdate\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: wordpress\n        app.kubernetes.io/version: 6.8.3\n        helm.sh/chart: wordpress-27.0.7\n    spec:\n      automountServiceAccountToken: false\n      hostAliases:\n      - hostnames:\n        - status.localhost\n        ip: 127.0.0.1\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: wordpress\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-wordpress\n      initContainers:\n      - name: prepare-base-dir\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/liblog.sh\\n. /opt/bitnami/scripts/libfs.sh\\n\\\n          \\ninfo \\\"Copying base dir to empty dir\\\"\\n# In order to not break the application\\\n          \\ functionality (such as upgrades or plugins) we need\\n# to make the base\\\n          \\ directory writable, so we need to copy it to an empty dir volume\\ncp -r\\\n          \\ --preserve=mode /opt/bitnami/wordpress /emptydir/app-base-dir\\n\\ninfo\\\n          \\ \\\"Copying symlinks to stdout/stderr\\\"\\n# We copy the logs folder because\\\n          \\ it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/apache/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/apache/logs /emptydir/apache-logs-dir\\nfi\\n\\\n          \\ninfo \\\"Copying default PHP config\\\"\\ncp -r --preserve=mode /opt/bitnami/php/etc\\\n          \\ /emptydir/php-conf-dir\\n\\ninfo \\\"Copying php var directory\\\"\\nif ! is_dir_empty\\\n          \\ /opt/bitnami/php/var; then\\n  cp -r /opt/bitnami/php/var /emptydir/php-var-dir\\n\\\n          fi\\n\\ninfo \\\"Copy operation completed\\\"\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: wordpress\n        image: registry-1.docker.io/bitnami/wordpress:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: WORDPRESS_SKIP_BOOTSTRAP\n          value: 'no'\n        - name: MARIADB_HOST\n          value: release-name-mariadb\n        - name: MARIADB_PORT_NUMBER\n          value: '3306'\n        - name: WORDPRESS_DATABASE_NAME\n          value: bitnami_wordpress\n        - name: WORDPRESS_DATABASE_USER\n          value: bn_wordpress\n        - name: WORDPRESS_DATABASE_PASSWORD_FILE\n          value: /secrets/mariadb-password\n        - name: WORDPRESS_USERNAME\n          value: user\n        - name: WORDPRESS_PASSWORD_FILE\n          value: /secrets/wordpress-password\n        - name: WORDPRESS_EMAIL\n          value: user@example.com\n        - name: WORDPRESS_FIRST_NAME\n          value: FirstName\n        - name: WORDPRESS_LAST_NAME\n          value: LastName\n        - name: WORDPRESS_HTACCESS_OVERRIDE_NONE\n          value: 'no'\n        - name: WORDPRESS_ENABLE_HTACCESS_PERSISTENCE\n          value: 'no'\n        - name: WORDPRESS_BLOG_NAME\n          value: User's Blog!\n        - name: WORDPRESS_TABLE_PREFIX\n          value: wp_\n        - name: WORDPRESS_SCHEME\n          value: http\n        - name: WORDPRESS_EXTRA_WP_CONFIG_CONTENT\n          value: ''\n        - name: WORDPRESS_PLUGINS\n          value: none\n        - name: WORDPRESS_OVERRIDE_DATABASE_SETTINGS\n          value: 'no'\n        - name: APACHE_HTTP_PORT_NUMBER\n          value: '8080'\n        - name: APACHE_HTTPS_PORT_NUMBER\n          value: '8443'\n        envFrom: null\n        ports:\n        - name: http\n          containerPort: 8080\n        - name: https\n          containerPort: 8443\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            httpHeaders: []\n            path: /wp-login.php\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/conf\n          subPath: apache-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/logs\n          subPath: apache-logs-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/apache/var/run\n          subPath: apache-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/etc\n          subPath: php-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/tmp\n          subPath: php-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/php/var\n          subPath: php-var-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/wordpress\n          subPath: app-base-dir\n        - mountPath: /bitnami/wordpress\n          name: wordpress-data\n          subPath: wordpress\n        - name: wordpress-secrets\n          mountPath: /secrets\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: wordpress-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-wordpress\n          - secret:\n              name: release-name-mariadb\n      - name: wordpress-data\n        persistentVolumeClaim:\n          claimName: release-name-wordpress\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-wordpress\" not found"
  },
  {
    "id": "269",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"mariadb\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "270",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"preserve-logs-symlinks\" is using an invalid container image, \"registry-1.docker.io/bitnami/mariadb:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "271",
    "manifest_path": "data/manifests/artifacthub/bitnami/wordpress/015_statefulset_release-name-mariadb.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-mariadb\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: mariadb\n    app.kubernetes.io/version: 12.0.2\n    helm.sh/chart: mariadb-22.0.2\n    app.kubernetes.io/part-of: mariadb\n    app.kubernetes.io/component: primary\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: mariadb\n      app.kubernetes.io/part-of: mariadb\n      app.kubernetes.io/component: primary\n  serviceName: release-name-mariadb-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        checksum/configuration: 99c5698089cb2d501d6285e8a852fa828778b04554e408ae7b3b77a77a839d84\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/version: 12.0.2\n        helm.sh/chart: mariadb-22.0.2\n        app.kubernetes.io/part-of: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      automountServiceAccountToken: false\n      serviceAccountName: release-name-mariadb\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: mariadb\n                  app.kubernetes.io/component: primary\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      initContainers:\n      - name: preserve-logs-symlinks\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        command:\n        - /bin/bash\n        args:\n        - -ec\n        - \"#!/bin/bash\\n\\n. /opt/bitnami/scripts/libfs.sh\\n# We copy the logs folder\\\n          \\ because it has symlinks to stdout and stderr\\nif ! is_dir_empty /opt/bitnami/mariadb/logs;\\\n          \\ then\\n  cp -r /opt/bitnami/mariadb/logs /emptydir/app-logs-dir\\nfi\\n\"\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /emptydir\n      containers:\n      - name: mariadb\n        image: registry-1.docker.io/bitnami/mariadb:latest\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: MARIADB_ROOT_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-root-password\n        - name: MARIADB_USER\n          value: bn_wordpress\n        - name: MARIADB_PASSWORD_FILE\n          value: /opt/bitnami/mariadb/secrets/mariadb-password\n        - name: MARIADB_DATABASE\n          value: bitnami_wordpress\n        - name: MARIADB_ENABLE_SSL\n          value: 'no'\n        ports:\n        - name: mysql\n          containerPort: 3306\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin status -uroot -p\\\"${password_aux}\\\"\\n\"\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n          exec:\n            command:\n            - /bin/bash\n            - -ec\n            - \"password_aux=\\\"${MARIADB_ROOT_PASSWORD:-}\\\"\\nif [[ -f \\\"${MARIADB_ROOT_PASSWORD_FILE:-}\\\"\\\n              \\ ]]; then\\n    password_aux=$(cat \\\"$MARIADB_ROOT_PASSWORD_FILE\\\")\\n\\\n              fi\\nmariadb-admin ping -uroot -p\\\"${password_aux}\\\"\\n\"\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: data\n          mountPath: /bitnami/mariadb\n        - name: config\n          mountPath: /opt/bitnami/mariadb/conf/my.cnf\n          subPath: my.cnf\n        - name: mariadb-credentials\n          mountPath: /opt/bitnami/mariadb/secrets/\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/mariadb/logs\n          subPath: app-logs-dir\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: config\n        configMap:\n          name: release-name-mariadb\n      - name: mariadb-credentials\n        secret:\n          secretName: release-name-mariadb\n          items:\n          - key: mariadb-root-password\n            path: mariadb-root-password\n          - key: mariadb-password\n            path: mariadb-password\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: mariadb\n        app.kubernetes.io/component: primary\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-mariadb\" not found"
  },
  {
    "id": "272",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/035_service_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - protocol: TCP\n    port: 9402\n    name: http-metrics\n  selector:\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cainjector app.kubernetes.io/instance:release-name app.kubernetes.io/name:cainjector])"
  },
  {
    "id": "273",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/036_service_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - protocol: TCP\n    port: 9402\n    name: tcp-prometheus-servicemonitor\n    targetPort: http-metrics\n  selector:\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:cert-manager])"
  },
  {
    "id": "274",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/037_service_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  type: ClusterIP\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n  - name: metrics\n    port: 9402\n    protocol: TCP\n    targetPort: http-metrics\n  selector:\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:webhook app.kubernetes.io/instance:release-name app.kubernetes.io/name:webhook])"
  },
  {
    "id": "275",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-cert-manager-cainjector\" not found"
  },
  {
    "id": "276",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-manager-cainjector\" has cpu request 0"
  },
  {
    "id": "277",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/038_deployment_release-name-cert-manager-cainjector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-cainjector\n  namespace: default\n  labels:\n    app: cainjector\n    app.kubernetes.io/name: cainjector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: cainjector\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cainjector\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: cainjector\n  template:\n    metadata:\n      labels:\n        app: cainjector\n        app.kubernetes.io/name: cainjector\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: cainjector\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-cainjector\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-cainjector\n        image: quay.io/jetstack/cert-manager-cainjector:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --leader-election-namespace=kube-system\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-manager-cainjector\" has memory limit 0"
  },
  {
    "id": "278",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-cert-manager\" not found"
  },
  {
    "id": "279",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-manager-controller\" has cpu request 0"
  },
  {
    "id": "280",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/039_deployment_release-name-cert-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager\n  namespace: default\n  labels:\n    app: cert-manager\n    app.kubernetes.io/name: cert-manager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: cert-manager\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app: cert-manager\n        app.kubernetes.io/name: cert-manager\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-controller\n        image: quay.io/jetstack/cert-manager-controller:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        - --acme-http01-solver-image=quay.io/jetstack/cert-manager-acmesolver:v1.19.0\n        - --max-concurrent-challenges=60\n        ports:\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        - containerPort: 9403\n          name: http-healthz\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        livenessProbe:\n          httpGet:\n            port: http-healthz\n            path: /livez\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 15\n          successThreshold: 1\n          failureThreshold: 8\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-manager-controller\" has memory limit 0"
  },
  {
    "id": "281",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-cert-manager-webhook\" not found"
  },
  {
    "id": "282",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-manager-webhook\" has cpu request 0"
  },
  {
    "id": "283",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/040_deployment_release-name-cert-manager-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-cert-manager-webhook\n  namespace: default\n  labels:\n    app: webhook\n    app.kubernetes.io/name: webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: webhook\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: webhook\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: webhook\n  template:\n    metadata:\n      labels:\n        app: webhook\n        app.kubernetes.io/name: webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: webhook\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9402'\n    spec:\n      serviceAccountName: release-name-cert-manager-webhook\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-webhook\n        image: quay.io/jetstack/cert-manager-webhook:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --v=2\n        - --secure-port=10250\n        - --dynamic-serving-ca-secret-namespace=$(POD_NAMESPACE)\n        - --dynamic-serving-ca-secret-name=release-name-cert-manager-webhook-ca\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE)\n        - --dynamic-serving-dns-names=release-name-cert-manager-webhook.$(POD_NAMESPACE).svc\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        - name: healthcheck\n          protocol: TCP\n          containerPort: 6080\n        - containerPort: 9402\n          name: http-metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /livez\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: healthcheck\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-manager-webhook\" has memory limit 0"
  },
  {
    "id": "284",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "285",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-cert-manager-startupapicheck\" not found"
  },
  {
    "id": "286",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-manager-startupapicheck\" has cpu request 0"
  },
  {
    "id": "287",
    "manifest_path": "data/manifests/artifacthub/cert-manager/cert-manager/046_job_release-name-cert-manager-startupapicheck.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-cert-manager-startupapicheck\n  namespace: default\n  labels:\n    app: startupapicheck\n    app.kubernetes.io/name: startupapicheck\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: startupapicheck\n    app.kubernetes.io/version: v1.19.0\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cert-manager-v1.19.0\n  annotations:\n    helm.sh/hook: post-install\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n    helm.sh/hook-weight: '1'\nspec:\n  backoffLimit: 4\n  template:\n    metadata:\n      labels:\n        app: startupapicheck\n        app.kubernetes.io/name: startupapicheck\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: startupapicheck\n        app.kubernetes.io/version: v1.19.0\n        app.kubernetes.io/managed-by: Helm\n        helm.sh/chart: cert-manager-v1.19.0\n    spec:\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-cert-manager-startupapicheck\n      enableServiceLinks: false\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cert-manager-startupapicheck\n        image: quay.io/jetstack/cert-manager-startupapicheck:v1.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - check\n        - api\n        - --wait=1m\n        - -v\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-manager-startupapicheck\" has memory limit 0"
  },
  {
    "id": "288",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/019_service_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  annotations:\n    prometheus.io/scrape: 'true'\n    prometheus.io/port: '9964'\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/name: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    io.cilium/app: proxy\nspec:\n  clusterIP: None\n  type: ClusterIP\n  selector:\n    k8s-app: cilium-envoy\n  ports:\n  - name: envoy-metrics\n    port: 9964\n    protocol: TCP\n    targetPort: envoy-metrics\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[k8s-app:cilium-envoy])"
  },
  {
    "id": "289",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/020_service_hubble-peer.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: hubble-peer\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: hubble-peer\nspec:\n  selector:\n    k8s-app: cilium\n  ports:\n  - name: peer-service\n    port: 443\n    protocol: TCP\n    targetPort: 4244\n  internalTrafficPolicy: Local\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[k8s-app:cilium])"
  },
  {
    "id": "290",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"cilium-agent\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "291",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no_host_network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "293",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"apply-sysctl-overwrites\" does not have a read-only root file system"
  },
  {
    "id": "294",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "295",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "296",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"config\" does not have a read-only root file system"
  },
  {
    "id": "297",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"install-cni-binaries\" does not have a read-only root file system"
  },
  {
    "id": "298",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"mount-bpf-fs\" does not have a read-only root file system"
  },
  {
    "id": "299",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"mount-cgroup\" does not have a read-only root file system"
  },
  {
    "id": "300",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"cilium\" not found"
  },
  {
    "id": "301",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"apply-sysctl-overwrites\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "302",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"cilium-agent\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "303",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"clean-cilium-state\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "304",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"mount-bpf-fs\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "305",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"mount-cgroup\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "306",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "no_privileged",
    "violation_text": "container \"mount-bpf-fs\" is privileged"
  },
  {
    "id": "308",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"apply-sysctl-overwrites\" is not set to runAsNonRoot"
  },
  {
    "id": "309",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "310",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "311",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"config\" is not set to runAsNonRoot"
  },
  {
    "id": "312",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"install-cni-binaries\" is not set to runAsNonRoot"
  },
  {
    "id": "313",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"mount-bpf-fs\" is not set to runAsNonRoot"
  },
  {
    "id": "314",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"mount-cgroup\" is not set to runAsNonRoot"
  },
  {
    "id": "318",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"apply-sysctl-overwrites\" has cpu request 0"
  },
  {
    "id": "319",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "320",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "321",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"config\" has cpu request 0"
  },
  {
    "id": "322",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mount-bpf-fs\" has cpu request 0"
  },
  {
    "id": "323",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mount-cgroup\" has cpu request 0"
  },
  {
    "id": "324",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"apply-sysctl-overwrites\" has memory limit 0"
  },
  {
    "id": "325",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "326",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "327",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"config\" has memory limit 0"
  },
  {
    "id": "328",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"install-cni-binaries\" has memory limit 0"
  },
  {
    "id": "329",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mount-bpf-fs\" has memory limit 0"
  },
  {
    "id": "330",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/021_daemonset_cilium.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium\n  namespace: default\n  labels:\n    k8s-app: cilium\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-agent\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: cilium-agent\n      labels:\n        k8s-app: cilium\n        app.kubernetes.io/name: cilium-agent\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n        seccompProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-agent\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-agent\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          failureThreshold: 300\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n            - name: require-k8s-connectivity\n              value: 'false'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9879\n            scheme: HTTP\n            httpHeaders:\n            - name: brief\n              value: 'true'\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: KUBE_CLIENT_BACKOFF_BASE\n          value: '1'\n        - name: KUBE_CLIENT_BACKOFF_DURATION\n          value: '120'\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - bash\n              - -c\n              - \"set -o errexit\\nset -o pipefail\\nset -o nounset\\n\\n# When running\\\n                \\ in AWS ENI mode, it's likely that 'aws-node' has\\n# had a chance\\\n                \\ to install SNAT iptables rules. These can result\\n# in dropped traffic,\\\n                \\ so we should attempt to remove them.\\n# We do it using a 'postStart'\\\n                \\ hook since this may need to run\\n# for nodes which might have already\\\n                \\ been init'ed but may still\\n# have dangling rules. This is safe\\\n                \\ because there are no\\n# dependencies on anything that is part of\\\n                \\ the startup script\\n# itself, and can be safely run multiple times\\\n                \\ per node (e.g. in\\n# case of a restart).\\nif [[ \\\"$(iptables-save\\\n                \\ | grep -E -c 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN')\\\" != \\\"0\\\" ]];\\n\\\n                then\\n    echo 'Deleting iptables rules created by the AWS CNI VPC\\\n                \\ plugin'\\n    iptables-save | grep -E -v 'AWS-SNAT-CHAIN|AWS-CONNMARK-CHAIN'\\\n                \\ | iptables-restore\\nfi\\necho 'Done!'\\n\"\n          preStop:\n            exec:\n              command:\n              - /cni-uninstall.sh\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - CHOWN\n            - KILL\n            - NET_ADMIN\n            - NET_RAW\n            - IPC_LOCK\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            - DAC_OVERRIDE\n            - FOWNER\n            - SETGID\n            - SETUID\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - mountPath: /host/proc/sys/net\n          name: host-proc-sys-net\n        - mountPath: /host/proc/sys/kernel\n          name: host-proc-sys-kernel\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n        - name: cilium-netns\n          mountPath: /var/run/cilium/netns\n          mountPropagation: HostToContainer\n        - name: etc-cni-netd\n          mountPath: /host/etc/cni/net.d\n        - name: clustermesh-secrets\n          mountPath: /var/lib/cilium/clustermesh\n          readOnly: true\n        - name: lib-modules\n          mountPath: /lib/modules\n          readOnly: true\n        - name: xtables-lock\n          mountPath: /run/xtables.lock\n        - name: hubble-tls\n          mountPath: /var/lib/cilium/tls/hubble\n          readOnly: true\n        - name: tmp\n          mountPath: /tmp\n      initContainers:\n      - name: config\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-dbg\n        - build-config\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            drop:\n            - ALL\n      - name: mount-cgroup\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: CGROUP_ROOT\n          value: /run/cilium/cgroupv2\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-mount /hostbin/cilium-mount;\n\n          nsenter --cgroup=/hostproc/1/ns/cgroup --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-mount\"\n          $CGROUP_ROOT;\n\n          rm /hostbin/cilium-mount\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: apply-sysctl-overwrites\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: BIN_PATH\n          value: /opt/cni/bin\n        command:\n        - sh\n        - -ec\n        - 'cp /usr/bin/cilium-sysctlfix /hostbin/cilium-sysctlfix;\n\n          nsenter --mount=/hostproc/1/ns/mnt \"${BIN_PATH}/cilium-sysctlfix\";\n\n          rm /hostbin/cilium-sysctlfix\n\n          '\n        volumeMounts:\n        - name: hostproc\n          mountPath: /hostproc\n        - name: cni-path\n          mountPath: /hostbin\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - SYS_ADMIN\n            - SYS_CHROOT\n            - SYS_PTRACE\n            drop:\n            - ALL\n      - name: mount-bpf-fs\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        args:\n        - mount | grep \"/sys/fs/bpf type bpf\" || mount -t bpf bpf /sys/fs/bpf\n        command:\n        - /bin/bash\n        - -c\n        - --\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: Bidirectional\n      - name: clean-cilium-state\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-state\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: clean-cilium-bpf-state\n              optional: true\n        - name: WRITE_CNI_CONF_WHEN_READY\n          valueFrom:\n            configMapKeyRef:\n              name: cilium-config\n              key: write-cni-conf-when-ready\n              optional: true\n        terminationMessagePolicy: FallbackToLogsOnError\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n            - SYS_ADMIN\n            - SYS_RESOURCE\n            drop:\n            - ALL\n        volumeMounts:\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n        - name: cilium-cgroup\n          mountPath: /run/cilium/cgroupv2\n          mountPropagation: HostToContainer\n        - name: cilium-run\n          mountPath: /var/run/cilium\n      - name: install-cni-binaries\n        image: quay.io/cilium/cilium:v1.19.0-pre.1@sha256:202e41af73c0b8b772d1847c67b4d3f7d13f075fae16fa4ad9f6edd553d4f6f4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /install-plugin.sh\n        resources:\n          requests:\n            cpu: 100m\n            memory: 10Mi\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: cni-path\n          mountPath: /host/opt/cni/bin\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: cilium-run\n        hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n      - name: cilium-netns\n        hostPath:\n          path: /var/run/netns\n          type: DirectoryOrCreate\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n      - name: hostproc\n        hostPath:\n          path: /proc\n          type: Directory\n      - name: cilium-cgroup\n        hostPath:\n          path: /run/cilium/cgroupv2\n          type: DirectoryOrCreate\n      - name: cni-path\n        hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n      - name: etc-cni-netd\n        hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n      - name: lib-modules\n        hostPath:\n          path: /lib/modules\n      - name: xtables-lock\n        hostPath:\n          path: /run/xtables.lock\n          type: FileOrCreate\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: clustermesh-secrets\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: cilium-clustermesh\n              optional: true\n          - secret:\n              name: clustermesh-apiserver-remote-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: common-etcd-client.key\n              - key: tls.crt\n                path: common-etcd-client.crt\n              - key: ca.crt\n                path: common-etcd-client-ca.crt\n          - secret:\n              name: clustermesh-apiserver-local-cert\n              optional: true\n              items:\n              - key: tls.key\n                path: local-etcd-client.key\n              - key: tls.crt\n                path: local-etcd-client.crt\n              - key: ca.crt\n                path: local-etcd-client-ca.crt\n      - name: host-proc-sys-net\n        hostPath:\n          path: /proc/sys/net\n          type: Directory\n      - name: host-proc-sys-kernel\n        hostPath:\n          path: /proc/sys/kernel\n          type: Directory\n      - name: hubble-tls\n        projected:\n          defaultMode: 256\n          sources:\n          - secret:\n              name: hubble-server-certs\n              optional: true\n              items:\n              - key: tls.crt\n                path: server.crt\n              - key: tls.key\n                path: server.key\n              - key: ca.crt\n                path: client-ca.crt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mount-cgroup\" has memory limit 0"
  },
  {
    "id": "331",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "no_host_network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "333",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"cilium-envoy\" does not have a read-only root file system"
  },
  {
    "id": "334",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"cilium-envoy\" not found"
  },
  {
    "id": "335",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"cilium-envoy\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "337",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"cilium-envoy\" is not set to runAsNonRoot"
  },
  {
    "id": "339",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cilium-envoy\" has cpu request 0"
  },
  {
    "id": "340",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/022_daemonset_cilium-envoy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cilium-envoy\n  namespace: default\n  labels:\n    k8s-app: cilium-envoy\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-envoy\n    name: cilium-envoy\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium-envoy\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 2\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations: null\n      labels:\n        k8s-app: cilium-envoy\n        name: cilium-envoy\n        app.kubernetes.io/name: cilium-envoy\n        app.kubernetes.io/part-of: cilium\n    spec:\n      securityContext:\n        appArmorProfile:\n          type: Unconfined\n      containers:\n      - name: cilium-envoy\n        image: quay.io/cilium/cilium-envoy:v1.35.3-1758113963-8cb437bde0dcb78485ecc8edf685267452919ce5@sha256:b018cc3f6ef1fc6e9a4c7bda3ed559253289c947c087b9a30f9ef935400b57f1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/cilium-envoy-starter\n        args:\n        - --\n        - -c /var/run/cilium/envoy/bootstrap-config.json\n        - --base-id 0\n        - --log-level info\n        startupProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          failureThreshold: 105\n          periodSeconds: 2\n          successThreshold: 1\n          initialDelaySeconds: 5\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9878\n            scheme: HTTP\n          periodSeconds: 30\n          successThreshold: 1\n          failureThreshold: 3\n          timeoutSeconds: 5\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        ports:\n        - name: envoy-metrics\n          containerPort: 9964\n          hostPort: 9964\n          protocol: TCP\n        securityContext:\n          seLinuxOptions:\n            level: s0\n            type: spc_t\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_ADMIN\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n        volumeMounts:\n        - name: envoy-sockets\n          mountPath: /var/run/cilium/envoy/sockets\n          readOnly: false\n        - name: envoy-artifacts\n          mountPath: /var/run/cilium/envoy/artifacts\n          readOnly: true\n        - name: envoy-config\n          mountPath: /var/run/cilium/envoy/\n          readOnly: true\n        - name: bpf-maps\n          mountPath: /sys/fs/bpf\n          mountPropagation: HostToContainer\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      serviceAccountName: cilium-envoy\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 1\n      hostNetwork: true\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: cilium.io/no-schedule\n                operator: NotIn\n                values:\n                - 'true'\n        podAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium\n            topologyKey: kubernetes.io/hostname\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                k8s-app: cilium-envoy\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - operator: Exists\n      volumes:\n      - name: envoy-sockets\n        hostPath:\n          path: /var/run/cilium/envoy/sockets\n          type: DirectoryOrCreate\n      - name: envoy-artifacts\n        hostPath:\n          path: /var/run/cilium/envoy/artifacts\n          type: DirectoryOrCreate\n      - name: envoy-config\n        configMap:\n          name: cilium-envoy-config\n          defaultMode: 256\n          items:\n          - key: bootstrap-config.json\n            path: bootstrap-config.json\n      - name: bpf-maps\n        hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cilium-envoy\" has memory limit 0"
  },
  {
    "id": "341",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "no_host_network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "343",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"cilium-operator\" does not have a read-only root file system"
  },
  {
    "id": "344",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"cilium-operator\" not found"
  },
  {
    "id": "346",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"cilium-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "347",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cilium-operator\" has cpu request 0"
  },
  {
    "id": "348",
    "manifest_path": "data/manifests/artifacthub/cilium/cilium/023_deployment_cilium-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cilium-operator\n  namespace: default\n  labels:\n    io.cilium/app: operator\n    name: cilium-operator\n    app.kubernetes.io/part-of: cilium\n    app.kubernetes.io/name: cilium-operator\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      io.cilium/app: operator\n      name: cilium-operator\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 50%\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9963'\n        prometheus.io/scrape: 'true'\n      labels:\n        io.cilium/app: operator\n        name: cilium-operator\n        app.kubernetes.io/part-of: cilium\n        app.kubernetes.io/name: cilium-operator\n    spec:\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: cilium-operator\n        image: quay.io/cilium/operator-generic:v1.19.0-pre.1@sha256:d92409eb3f603ab1f73c3e98eaf71eee6aada27b1bec5c1fd83be9358c0db263\n        imagePullPolicy: IfNotPresent\n        command:\n        - cilium-operator-generic\n        args:\n        - --config-dir=/tmp/cilium/config-map\n        - --debug=$(CILIUM_DEBUG)\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: debug\n              name: cilium-config\n              optional: true\n        ports:\n        - name: prometheus\n          containerPort: 9963\n          hostPort: 9963\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          timeoutSeconds: 3\n        readinessProbe:\n          httpGet:\n            host: 127.0.0.1\n            path: /healthz\n            port: 9234\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 5\n          timeoutSeconds: 3\n          failureThreshold: 5\n        volumeMounts:\n        - name: cilium-config-path\n          mountPath: /tmp/cilium/config-map\n          readOnly: true\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        terminationMessagePolicy: FallbackToLogsOnError\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-cluster-critical\n      serviceAccountName: cilium-operator\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                io.cilium/app: operator\n            topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      - key: node-role.kubernetes.io/master\n        operator: Exists\n      - key: node.kubernetes.io/not-ready\n        operator: Exists\n      - key: node.cloudprovider.kubernetes.io/uninitialized\n        operator: Exists\n      - key: node.cilium.io/agent-not-ready\n        operator: Exists\n      volumes:\n      - name: cilium-config-path\n        configMap:\n          name: cilium-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cilium-operator\" has memory limit 0"
  },
  {
    "id": "349",
    "manifest_path": "data/manifests/artifacthub/cluster-autoscaler/cluster-autoscaler/001_poddisruptionbudget_release-name-aws-cluster-autoscaler.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cluster-autoscaler-9.50.1\n  name: release-name-aws-cluster-autoscaler\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: aws-cluster-autoscaler\n  maxUnavailable: 1\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "350",
    "manifest_path": "data/manifests/artifacthub/cluster-autoscaler/cluster-autoscaler/007_service_release-name-aws-cluster-autoscaler.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: cluster-autoscaler-9.50.1\n  name: release-name-aws-cluster-autoscaler\n  namespace: default\nspec:\n  ports:\n  - port: 8085\n    protocol: TCP\n    targetPort: 8085\n    name: http\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: aws-cluster-autoscaler\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:aws-cluster-autoscaler])"
  },
  {
    "id": "351",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/018_service_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  type: ClusterIP\n  selector:\n    app: release-name-datadog-cluster-agent\n  ports:\n  - port: 5005\n    name: agentport\n    protocol: TCP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog-cluster-agent])"
  },
  {
    "id": "352",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/019_service_release-name-datadog-cluster-agent-admission-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog-cluster-agent-admission-controller\n  namespace: default\n  labels:\n    app: release-name-datadog\n    chart: datadog-3.136.1\n    release: release-name\n    heritage: Helm\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  selector:\n    app: release-name-datadog-cluster-agent\n  ports:\n  - port: 443\n    targetPort: 8000\n    name: datadog-webhook\n    protocol: TCP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog-cluster-agent])"
  },
  {
    "id": "353",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/020_service_release-name-datadog.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-datadog\n  namespace: default\n  labels:\n    app: release-name-datadog\n    chart: datadog-3.136.1\n    release: release-name\n    heritage: Helm\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\nspec:\n  selector:\n    app: release-name-datadog\n  ports:\n  - protocol: UDP\n    port: 8125\n    targetPort: 8125\n    name: dogstatsdport\n  - protocol: TCP\n    port: 8126\n    targetPort: 8126\n    name: traceport\n  internalTrafficPolicy: Local\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:release-name-datadog])"
  },
  {
    "id": "355",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"init-volume\" does not have a read-only root file system"
  },
  {
    "id": "356",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-datadog-cluster-agent\" not found"
  },
  {
    "id": "358",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"cluster-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "359",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"init-volume\" is not set to runAsNonRoot"
  },
  {
    "id": "361",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cluster-agent\" has cpu request 0"
  },
  {
    "id": "362",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"init-volume\" has cpu request 0"
  },
  {
    "id": "363",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cluster-agent\" has memory limit 0"
  },
  {
    "id": "364",
    "manifest_path": "data/manifests/artifacthub/datadog/datadog/021_deployment_release-name-datadog-cluster-agent.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-datadog-cluster-agent\n  namespace: default\n  labels:\n    helm.sh/chart: datadog-3.136.1\n    app.kubernetes.io/name: release-name-datadog\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '7'\n    app.kubernetes.io/component: cluster-agent\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: release-name-datadog-cluster-agent\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: release-name-datadog\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: cluster-agent\n        admission.datadoghq.com/enabled: 'false'\n        app: release-name-datadog-cluster-agent\n      name: release-name-datadog-cluster-agent\n      annotations:\n        checksum/clusteragent_token: fbc6f2d3ad945adf5812e9df33556b49f4e894870efae7f9b3c06e4d76bb6539\n        checksum/clusteragent-configmap: ba001253667f3112944161e6f10483d958fbd5ceed616a7be2b475ec467be270\n        checksum/api_key: 4317ca275bb5816462b100364420a02a6389db436bc82720be80904e1ec2fe73\n        checksum/application_key: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n        checksum/install_info: 3c6c1e175f7b6f52fe95ffd0dcb9831ca000b3aa095a64b1e8b4c0442b47e9c1\n    spec:\n      serviceAccountName: release-name-datadog-cluster-agent\n      automountServiceAccountToken: true\n      initContainers:\n      - name: init-volume\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        args:\n        - /etc/datadog-agent\n        - /opt\n        volumeMounts:\n        - name: config\n          mountPath: /opt/datadog-agent\n      containers:\n      - name: cluster-agent\n        image: gcr.io/datadoghq/cluster-agent:7.70.2\n        imagePullPolicy: IfNotPresent\n        resources: {}\n        ports:\n        - containerPort: 5005\n          name: agentport\n          protocol: TCP\n        - containerPort: 5000\n          name: agentmetrics\n          protocol: TCP\n        - containerPort: 8000\n          name: datadog-webhook\n          protocol: TCP\n        env:\n        - name: DD_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: DD_CLUSTER_AGENT_SERVICE_ACCOUNT_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: DD_HEALTH_PORT\n          value: '5556'\n        - name: DD_API_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog\n              key: api-key\n              optional: true\n        - name: KUBERNETES\n          value: 'yes'\n        - name: DD_CSI_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_LANGUAGE_DETECTION_REPORTING_ENABLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_VALIDATION_ENABLED\n          value: 'true'\n        - name: DD_ADMISSION_CONTROLLER_MUTATION_ENABLED\n          value: 'true'\n        - name: DD_TRACE_AGENT_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_HOST_SOCKET_PATH\n          value: /var/run/datadog/\n        - name: DD_DOGSTATSD_SOCKET\n          value: /var/run/datadog/dsd.socket\n        - name: DD_APM_RECEIVER_SOCKET\n          value: /var/run/datadog/apm.socket\n        - name: DD_ADMISSION_CONTROLLER_WEBHOOK_NAME\n          value: datadog-webhook\n        - name: DD_ADMISSION_CONTROLLER_MUTATE_UNLABELLED\n          value: 'false'\n        - name: DD_ADMISSION_CONTROLLER_SERVICE_NAME\n          value: release-name-datadog-cluster-agent-admission-controller\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_MODE\n          value: socket\n        - name: DD_ADMISSION_CONTROLLER_INJECT_CONFIG_LOCAL_SERVICE_NAME\n          value: release-name-datadog\n        - name: DD_ADMISSION_CONTROLLER_FAILURE_POLICY\n          value: Ignore\n        - name: DD_ADMISSION_CONTROLLER_PORT\n          value: '8000'\n        - name: DD_ADMISSION_CONTROLLER_CONTAINER_REGISTRY\n          value: gcr.io/datadoghq\n        - name: DD_REMOTE_CONFIGURATION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_CHECKS_ENABLED\n          value: 'true'\n        - name: DD_EXTRA_CONFIG_PROVIDERS\n          value: kube_endpoints kube_services\n        - name: DD_EXTRA_LISTENERS\n          value: kube_endpoints kube_services\n        - name: DD_LOG_LEVEL\n          value: INFO\n        - name: DD_LEADER_ELECTION\n          value: 'true'\n        - name: DD_LEADER_ELECTION_DEFAULT_RESOURCE\n          value: configmap\n        - name: DD_LEADER_LEASE_NAME\n          value: release-name-datadog-leader-election\n        - name: DD_CLUSTER_AGENT_TOKEN_NAME\n          value: release-name-datadogtoken\n        - name: DD_COLLECT_KUBERNETES_EVENTS\n          value: 'true'\n        - name: DD_KUBERNETES_USE_ENDPOINT_SLICES\n          value: 'false'\n        - name: DD_KUBERNETES_EVENTS_SOURCE_DETECTION_ENABLED\n          value: 'false'\n        - name: DD_CLUSTER_AGENT_KUBERNETES_SERVICE_NAME\n          value: release-name-datadog-cluster-agent\n        - name: DD_CLUSTER_AGENT_AUTH_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-datadog-cluster-agent\n              key: token\n        - name: DD_CLUSTER_AGENT_COLLECT_KUBERNETES_TAGS\n          value: 'false'\n        - name: DD_KUBE_RESOURCES_NAMESPACE\n          value: default\n        - name: CHART_RELEASE_NAME\n          value: release-name\n        - name: AGENT_DAEMONSET\n          value: release-name-datadog\n        - name: CLUSTER_AGENT_DEPLOYMENT\n          value: release-name-datadog-cluster-agent\n        - name: DD_ORCHESTRATOR_EXPLORER_ENABLED\n          value: 'true'\n        - name: DD_ORCHESTRATOR_EXPLORER_CONTAINER_SCRUBBING_ENABLED\n          value: 'true'\n        - name: DD_CLUSTER_AGENT_LANGUAGE_DETECTION_PATCHER_ENABLED\n          value: 'false'\n        - name: DD_INSTRUMENTATION_INSTALL_TIME\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_time\n        - name: DD_INSTRUMENTATION_INSTALL_ID\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_id\n        - name: DD_INSTRUMENTATION_INSTALL_TYPE\n          valueFrom:\n            configMapKeyRef:\n              name: release-name-datadog-kpi-telemetry-configmap\n              key: install_type\n        livenessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /live\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /ready\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /startup\n            port: 5556\n            scheme: HTTP\n          initialDelaySeconds: 15\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: datadogrun\n          mountPath: /opt/datadog-agent/run\n          readOnly: false\n        - name: varlog\n          mountPath: /var/log/datadog\n          readOnly: false\n        - name: tmpdir\n          mountPath: /tmp\n          readOnly: false\n        - name: installinfo\n          subPath: install_info\n          mountPath: /etc/datadog-agent/install_info\n          readOnly: true\n        - name: confd\n          mountPath: /conf.d\n          readOnly: true\n        - name: config\n          mountPath: /etc/datadog-agent\n      volumes:\n      - name: datadogrun\n        emptyDir: {}\n      - name: varlog\n        emptyDir: {}\n      - name: tmpdir\n        emptyDir: {}\n      - name: installinfo\n        configMap:\n          name: release-name-datadog-installinfo\n      - name: confd\n        configMap:\n          name: release-name-datadog-cluster-agent-confd\n          items:\n          - key: kubernetes_state_core.yaml.default\n            path: kubernetes_state_core.yaml.default\n          - key: kubernetes_apiserver.yaml\n            path: kubernetes_apiserver.yaml\n      - name: config\n        emptyDir: {}\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 50\n            podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app: release-name-datadog-cluster-agent\n              topologyKey: kubernetes.io/hostname\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"init-volume\" has memory limit 0"
  },
  {
    "id": "365",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/001_poddisruptionbudget_elasticsearch-master-pdb.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: elasticsearch-master-pdb\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "366",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/004_service_elasticsearch-master.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations: {}\nspec:\n  type: ClusterIP\n  selector:\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  publishNotReadyAddresses: false\n  ports:\n  - name: http\n    protocol: TCP\n    port: 9200\n  - name: transport\n    protocol: TCP\n    port: 9300\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:elasticsearch-master chart:elasticsearch release:release-name])"
  },
  {
    "id": "367",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/005_service_elasticsearch-master-headless.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch-master-headless\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    service.alpha.kubernetes.io/tolerate-unready-endpoints: 'true'\nspec:\n  clusterIP: None\n  publishNotReadyAddresses: true\n  selector:\n    app: elasticsearch-master\n  ports:\n  - name: http\n    port: 9200\n  - name: transport\n    port: 9300\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:elasticsearch-master])"
  },
  {
    "id": "368",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"configure-sysctl\" does not have a read-only root file system"
  },
  {
    "id": "369",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"elasticsearch\" does not have a read-only root file system"
  },
  {
    "id": "370",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"configure-sysctl\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "371",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "no_privileged",
    "violation_text": "container \"configure-sysctl\" is privileged"
  },
  {
    "id": "372",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"configure-sysctl\" is not set to runAsNonRoot"
  },
  {
    "id": "373",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"configure-sysctl\" has cpu request 0"
  },
  {
    "id": "374",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/006_statefulset_elasticsearch-master.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: elasticsearch-master\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: elasticsearch\n    app: elasticsearch-master\n  annotations:\n    esMajorVersion: '8'\nspec:\n  serviceName: elasticsearch-master-headless\n  selector:\n    matchLabels:\n      app: elasticsearch-master\n  replicas: 3\n  podManagementPolicy: Parallel\n  updateStrategy:\n    type: RollingUpdate\n  volumeClaimTemplates:\n  - metadata:\n      name: elasticsearch-master\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 30Gi\n  template:\n    metadata:\n      name: elasticsearch-master\n      labels:\n        release: release-name\n        chart: elasticsearch\n        app: elasticsearch-master\n      annotations: null\n    spec:\n      securityContext:\n        fsGroup: 1000\n        runAsUser: 1000\n      automountServiceAccountToken: true\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: app\n                operator: In\n                values:\n                - elasticsearch-master\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 120\n      volumes:\n      - name: elasticsearch-certs\n        secret:\n          secretName: elasticsearch-master-certs\n      enableServiceLinks: true\n      initContainers:\n      - name: configure-sysctl\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - sysctl\n        - -w\n        - vm.max_map_count=262144\n        resources: {}\n      containers:\n      - name: elasticsearch\n        securityContext:\n          capabilities:\n            drop:\n            - ALL\n          runAsNonRoot: true\n          runAsUser: 1000\n        image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n        imagePullPolicy: IfNotPresent\n        readinessProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - \"set -e\\n\\n# Exit if ELASTIC_PASSWORD in unset\\nif [ -z \\\"${ELASTIC_PASSWORD}\\\"\\\n              \\ ]; then\\n  echo \\\"ELASTIC_PASSWORD variable is missing, exiting\\\"\\n\\\n              \\  exit 1\\nfi\\n\\n# If the node is starting up wait for the cluster to\\\n              \\ be ready (request params: \\\"wait_for_status=green&timeout=1s\\\" )\\n\\\n              # Once it has started only check that the node itself is responding\\n\\\n              START_FILE=/tmp/.es_start_file\\n\\n# Disable nss cache to avoid filling\\\n              \\ dentry cache when calling curl\\n# This is required with Elasticsearch\\\n              \\ Docker using nss < 3.52\\nexport NSS_SDB_USE_CACHE=no\\n\\nhttp () {\\n\\\n              \\  local path=\\\"${1}\\\"\\n  local args=\\\"${2}\\\"\\n  set -- -XGET -s\\n\\n\\\n              \\  if [ \\\"$args\\\" != \\\"\\\" ]; then\\n    set -- \\\"$@\\\" $args\\n  fi\\n\\n\\\n              \\  set -- \\\"$@\\\" -u \\\"elastic:${ELASTIC_PASSWORD}\\\"\\n\\n  curl --output\\\n              \\ /dev/null -k \\\"$@\\\" \\\"https://127.0.0.1:9200${path}\\\"\\n}\\n\\nif [ -f\\\n              \\ \\\"${START_FILE}\\\" ]; then\\n  echo 'Elasticsearch is already running,\\\n              \\ lets check the node is healthy'\\n  HTTP_CODE=$(http \\\"/\\\" \\\"-w %{http_code}\\\"\\\n              )\\n  RC=$?\\n  if [[ ${RC} -ne 0 ]]; then\\n    echo \\\"curl --output /dev/null\\\n              \\ -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with RC ${RC}\\\"\\n    exit ${RC}\\n  fi\\n  # ready if HTTP code\\\n              \\ 200, 503 is tolerable if ES version is 6.x\\n  if [[ ${HTTP_CODE} ==\\\n              \\ \\\"200\\\" ]]; then\\n    exit 0\\n  elif [[ ${HTTP_CODE} == \\\"503\\\" &&\\\n              \\ \\\"8\\\" == \\\"6\\\" ]]; then\\n    exit 0\\n  else\\n    echo \\\"curl --output\\\n              \\ /dev/null -k -XGET -s -w '%{http_code}' \\\\${BASIC_AUTH} https://127.0.0.1:9200/\\\n              \\ failed with HTTP code ${HTTP_CODE}\\\"\\n    exit 1\\n  fi\\n\\nelse\\n \\\n              \\ echo 'Waiting for elasticsearch cluster to become ready (request params:\\\n              \\ \\\"wait_for_status=green&timeout=1s\\\" )'\\n  if http \\\"/_cluster/health?wait_for_status=green&timeout=1s\\\"\\\n              \\ \\\"--fail\\\" ; then\\n    touch ${START_FILE}\\n    exit 0\\n  else\\n \\\n              \\   echo 'Cluster is not yet ready (request params: \\\"wait_for_status=green&timeout=1s\\\"\\\n              \\ )'\\n    exit 1\\n  fi\\nfi\\n\"\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 3\n          timeoutSeconds: 5\n        ports:\n        - name: http\n          containerPort: 9200\n        - name: transport\n          containerPort: 9300\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 2Gi\n          requests:\n            cpu: 1000m\n            memory: 2Gi\n        env:\n        - name: node.name\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: cluster.initial_master_nodes\n          value: elasticsearch-master-0,elasticsearch-master-1,elasticsearch-master-2,\n        - name: node.roles\n          value: master,data,data_content,data_hot,data_warm,data_cold,ingest,ml,remote_cluster_client,transform,\n        - name: discovery.seed_hosts\n          value: elasticsearch-master-headless\n        - name: cluster.name\n          value: elasticsearch\n        - name: network.host\n          value: 0.0.0.0\n        - name: ELASTIC_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: elasticsearch-master-credentials\n              key: password\n        - name: xpack.security.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.enabled\n          value: 'true'\n        - name: xpack.security.http.ssl.enabled\n          value: 'true'\n        - name: xpack.security.transport.ssl.verification_mode\n          value: certificate\n        - name: xpack.security.transport.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.transport.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.transport.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        - name: xpack.security.http.ssl.key\n          value: /usr/share/elasticsearch/config/certs/tls.key\n        - name: xpack.security.http.ssl.certificate\n          value: /usr/share/elasticsearch/config/certs/tls.crt\n        - name: xpack.security.http.ssl.certificate_authorities\n          value: /usr/share/elasticsearch/config/certs/ca.crt\n        volumeMounts:\n        - name: elasticsearch-master\n          mountPath: /usr/share/elasticsearch/data\n        - name: elasticsearch-certs\n          mountPath: /usr/share/elasticsearch/config/certs\n          readOnly: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"configure-sysctl\" has memory limit 0"
  },
  {
    "id": "375",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"release-name-mvgmy-test\" does not have a read-only root file system"
  },
  {
    "id": "376",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-mvgmy-test\" has cpu request 0"
  },
  {
    "id": "377",
    "manifest_path": "data/manifests/artifacthub/elastic/elasticsearch/007_pod_release-name-ikewa-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ikewa-test\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    fsGroup: 1000\n    runAsUser: 1000\n  containers:\n  - name: release-name-mvgmy-test\n    env:\n    - name: ELASTIC_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: elasticsearch-master-credentials\n          key: password\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.5.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - sh\n    - -c\n    - '#!/usr/bin/env bash -e\n\n      curl -XGET --fail --cacert /usr/share/elasticsearch/config/certs/tls.crt -u\n      \"elastic:${ELASTIC_PASSWORD}\" https://''elasticsearch-master:9200/_cluster/health?wait_for_status=green&timeout=1s''\n\n      '\n    volumeMounts:\n    - name: elasticsearch-certs\n      mountPath: /usr/share/elasticsearch/config/certs\n      readOnly: true\n  restartPolicy: Never\n  volumes:\n  - name: elasticsearch-certs\n    secret:\n      secretName: elasticsearch-master-certs\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-mvgmy-test\" has memory limit 0"
  },
  {
    "id": "378",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/004_service_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  selector:\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n  ports:\n  - name: http\n    port: 7979\n    targetPort: http\n    protocol: TCP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:external-dns])"
  },
  {
    "id": "379",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-external-dns\" not found"
  },
  {
    "id": "380",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"external-dns\" has cpu request 0"
  },
  {
    "id": "381",
    "manifest_path": "data/manifests/artifacthub/external-dns/external-dns/005_deployment_release-name-external-dns.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-dns\n  namespace: default\n  labels:\n    helm.sh/chart: external-dns-1.19.0\n    app.kubernetes.io/name: external-dns\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.19.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-dns\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: external-dns\n        app.kubernetes.io/instance: release-name\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-external-dns\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      containers:\n      - name: external-dns\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n        image: registry.k8s.io/external-dns/external-dns:v0.19.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --log-level=info\n        - --log-format=text\n        - --interval=1m\n        - --source=service\n        - --source=ingress\n        - --policy=upsert-only\n        - --registry=txt\n        - --provider=aws\n        ports:\n        - name: http\n          protocol: TCP\n          containerPort: 7979\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /healthz\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"external-dns\" has memory limit 0"
  },
  {
    "id": "382",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/037_service_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\n    external-secrets.io/component: webhook\nspec:\n  type: ClusterIP\n  ports:\n  - port: 443\n    targetPort: webhook\n    protocol: TCP\n    name: webhook\n  selector:\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:external-secrets-webhook])"
  },
  {
    "id": "383",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"external-secrets-cert-controller\" not found"
  },
  {
    "id": "385",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-controller\" has cpu request 0"
  },
  {
    "id": "386",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/038_deployment_release-name-external-secrets-cert-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-cert-controller\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-cert-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-cert-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-cert-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: external-secrets-cert-controller\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: cert-controller\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - certcontroller\n        - --crd-requeue-interval=5m\n        - --service-name=release-name-external-secrets-webhook\n        - --service-namespace=default\n        - --secret-name=release-name-external-secrets-webhook\n        - --secret-namespace=default\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        - --enable-partial-cache=true\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cert-controller\" has memory limit 0"
  },
  {
    "id": "387",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-external-secrets\" not found"
  },
  {
    "id": "388",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"external-secrets\" has cpu request 0"
  },
  {
    "id": "389",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/039_deployment_release-name-external-secrets.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-external-secrets\n      automountServiceAccountToken: true\n      hostNetwork: false\n      containers:\n      - name: external-secrets\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --concurrent=1\n        - --metrics-addr=:8080\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n      dnsPolicy: ClusterFirst\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"external-secrets\" has memory limit 0"
  },
  {
    "id": "390",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"external-secrets-webhook\" not found"
  },
  {
    "id": "392",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"webhook\" has cpu request 0"
  },
  {
    "id": "393",
    "manifest_path": "data/manifests/artifacthub/external-secrets-operator/external-secrets/040_deployment_release-name-external-secrets-webhook.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-external-secrets-webhook\n  namespace: default\n  labels:\n    helm.sh/chart: external-secrets-0.20.2\n    app.kubernetes.io/name: external-secrets-webhook\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.20.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: external-secrets-webhook\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: external-secrets-0.20.2\n        app.kubernetes.io/name: external-secrets-webhook\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v0.20.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      hostNetwork: false\n      serviceAccountName: external-secrets-webhook\n      automountServiceAccountToken: true\n      containers:\n      - name: webhook\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: oci.external-secrets.io/external-secrets/external-secrets:v0.20.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - webhook\n        - --port=10250\n        - --dns-name=release-name-external-secrets-webhook.default.svc\n        - --cert-dir=/tmp/certs\n        - --check-interval=5m\n        - --metrics-addr=:8080\n        - --healthz-addr=:8081\n        - --loglevel=info\n        - --zap-time-encoding=epoch\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: metrics\n        - containerPort: 10250\n          protocol: TCP\n          name: webhook\n        readinessProbe:\n          httpGet:\n            port: 8081\n            path: /readyz\n          initialDelaySeconds: 20\n          periodSeconds: 5\n        volumeMounts:\n        - name: certs\n          mountPath: /tmp/certs\n          readOnly: true\n      volumes:\n      - name: certs\n        secret:\n          secretName: release-name-external-secrets-webhook\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"webhook\" has memory limit 0"
  },
  {
    "id": "394",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/005_service_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - port: 2020\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:fluent-bit])"
  },
  {
    "id": "395",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"fluent-bit\" does not have a read-only root file system"
  },
  {
    "id": "396",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-fluent-bit\" not found"
  },
  {
    "id": "397",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"fluent-bit\" is not set to runAsNonRoot"
  },
  {
    "id": "398",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"fluent-bit\" has cpu request 0"
  },
  {
    "id": "399",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/006_daemonset_release-name-fluent-bit.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-fluent-bit\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/name: fluent-bit\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: fluent-bit\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: fluent-bit\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 8171e72b067d5266d7bcddb8052f04bdf9247e9889fb075d90ccd5939a6ae0da\n    spec:\n      serviceAccountName: release-name-fluent-bit\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: fluent-bit\n        image: cr.fluentbit.io/fluent/fluent-bit:4.0.7\n        imagePullPolicy: IfNotPresent\n        command:\n        - /fluent-bit/bin/fluent-bit\n        args:\n        - --workdir=/fluent-bit/etc\n        - --config=/fluent-bit/etc/conf/fluent-bit.conf\n        ports:\n        - name: http\n          containerPort: 2020\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /api/v1/health\n            port: http\n        volumeMounts:\n        - name: config\n          mountPath: /fluent-bit/etc/conf\n        - mountPath: /var/log\n          name: varlog\n        - mountPath: /var/lib/docker/containers\n          name: varlibdockercontainers\n          readOnly: true\n        - mountPath: /etc/machine-id\n          name: etcmachineid\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-fluent-bit\n      - hostPath:\n          path: /var/log\n        name: varlog\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: varlibdockercontainers\n      - hostPath:\n          path: /etc/machine-id\n          type: File\n        name: etcmachineid\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"fluent-bit\" has memory limit 0"
  },
  {
    "id": "400",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "401",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wget\" does not have a read-only root file system"
  },
  {
    "id": "402",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"wget\" is not set to runAsNonRoot"
  },
  {
    "id": "403",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "404",
    "manifest_path": "data/manifests/artifacthub/fluent/fluent-bit/007_pod_release-name-fluent-bit-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-fluent-bit-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: fluent-bit-0.53.0\n    app.kubernetes.io/version: 4.0.7\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    imagePullPolicy: Always\n    command:\n    - sh\n    args:\n    - -c\n    - sleep 5s && wget -O- release-name-fluent-bit:2020\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "405",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/004_poddisruptionbudget_release-name-postgresql-ha-pgpool.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: pgpool\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "406",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/005_poddisruptionbudget_release-name-postgresql-ha-postgresql.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "407",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/006_poddisruptionbudget_release-name-postgresql-ha-postgresql-witness.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-postgresql-ha-postgresql-witness\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\n    role: witness\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n      role: witness\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "408",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/007_poddisruptionbudget_release-name-valkey-cluster.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  maxUnavailable: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: valkey-cluster\n    matchExpressions:\n    - key: job-name\n      operator: NotIn\n      values:\n      - release-name-valkey-cluster-cluster-update\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "409",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/019_service_release-name-postgresql-ha-pgpool.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: postgresql\n    port: 5432\n    targetPort: postgresql\n    protocol: TCP\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/component: pgpool\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:pgpool app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql-ha])"
  },
  {
    "id": "410",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/020_service_release-name-postgresql-ha-postgresql-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-postgresql-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: false\n  ports:\n  - name: postgresql\n    port: 5432\n    targetPort: postgresql\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/component: postgresql\n    role: data\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:postgresql app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql-ha role:data])"
  },
  {
    "id": "411",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/021_service_release-name-postgresql-ha-postgresql.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\nspec:\n  type: ClusterIP\n  ports:\n  - name: postgresql\n    port: 5432\n    targetPort: postgresql\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/component: postgresql\n    role: data\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:postgresql app.kubernetes.io/instance:release-name app.kubernetes.io/name:postgresql-ha role:data])"
  },
  {
    "id": "412",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/022_service_release-name-valkey-cluster-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-valkey-cluster-headless\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: tcp-redis\n  - name: tcp-redis-bus\n    port: 16379\n    targetPort: tcp-redis-bus\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: valkey-cluster\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:valkey-cluster])"
  },
  {
    "id": "413",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/023_service_release-name-valkey-cluster.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  type: ClusterIP\n  sessionAffinity: None\n  ports:\n  - name: tcp-redis\n    port: 6379\n    targetPort: tcp-redis\n    protocol: TCP\n    nodePort: null\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: valkey-cluster\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:valkey-cluster])"
  },
  {
    "id": "414",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/024_service_release-name-gitea-http.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-gitea-http\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations: {}\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: http\n    port: 3000\n    targetPort: null\n  selector:\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:gitea])"
  },
  {
    "id": "415",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/025_service_release-name-gitea-ssh.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-gitea-ssh\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations: {}\nspec:\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: ssh\n    port: 22\n    targetPort: 2222\n    protocol: TCP\n  selector:\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:gitea])"
  },
  {
    "id": "416",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/026_deployment_release-name-postgresql-ha-pgpool.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-postgresql-ha-pgpool\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 4.6.3\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: pgpool\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: pgpool\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql-ha\n        app.kubernetes.io/version: 4.6.3\n        helm.sh/chart: postgresql-ha-16.3.2\n        app.kubernetes.io/component: pgpool\n      annotations: null\n    spec:\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql-ha\n                  app.kubernetes.io/component: pgpool\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-postgresql-ha\n      containers:\n      - name: pgpool\n        image: docker.io/bitnamilegacy/pgpool:4.6.3-debian-12-r0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: REPMGR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: PGPOOL_BACKEND_NODES\n          value: 0:release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,1:release-name-postgresql-ha-postgresql-1.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,2:release-name-postgresql-ha-postgresql-2.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local:5432,\n        - name: PGPOOL_SR_CHECK_USER\n          value: sr_check_user\n        - name: PGPOOL_SR_CHECK_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/sr-check-password\n        - name: PGPOOL_SR_CHECK_DATABASE\n          value: postgres\n        - name: PGPOOL_ENABLE_LDAP\n          value: 'no'\n        - name: PGPOOL_POSTGRES_USERNAME\n          value: gitea\n        - name: PGPOOL_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/pgpool-password\n        - name: PGPOOL_ADMIN_USERNAME\n          value: admin\n        - name: PGPOOL_ADMIN_PASSWORD_FILE\n          value: /opt/bitnami/pgpool/secrets/admin-password\n        - name: PGPOOL_AUTHENTICATION_METHOD\n          value: scram-sha-256\n        - name: PGPOOL_ENABLE_LOAD_BALANCING\n          value: 'yes'\n        - name: PGPOOL_ENABLE_CONNECTION_CACHE\n          value: 'yes'\n        - name: PGPOOL_DISABLE_LOAD_BALANCE_ON_WRITE\n          value: transaction\n        - name: PGPOOL_ENABLE_LOG_CONNECTIONS\n          value: 'no'\n        - name: PGPOOL_ENABLE_LOG_HOSTNAME\n          value: 'yes'\n        - name: PGPOOL_ENABLE_LOG_PCP_PROCESSES\n          value: 'yes'\n        - name: PGPOOL_ENABLE_LOG_PER_NODE_STATEMENT\n          value: 'no'\n        - name: PGPOOL_RESERVED_CONNECTIONS\n          value: '1'\n        - name: PGPOOL_CHILD_LIFE_TIME\n          value: ''\n        - name: PGPOOL_ENABLE_TLS\n          value: 'no'\n        - name: PGPOOL_HEALTH_CHECK_PSQL_TIMEOUT\n          value: '6'\n        envFrom: null\n        ports:\n        - name: postgresql\n          containerPort: 5432\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /opt/bitnami/scripts/pgpool/healthcheck.sh\n        readinessProbe:\n          failureThreshold: 5\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - bash\n            - -ec\n            - PGPASSWORD=$(< $PGPOOL_POSTGRES_PASSWORD_FILE) psql -U \"gitea\" -d \"gitea\"\n              -h /opt/bitnami/pgpool/tmp -tA -c \"SELECT 1\" >/dev/null\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/etc\n          subPath: app-etc-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/pgpool/logs\n          subPath: app-logs-dir\n        - name: postgresql-creds\n          subPath: pgpool-password\n          mountPath: /opt/bitnami/pgpool/secrets/pgpool-password\n        - name: pgpool-creds\n          subPath: admin-password\n          mountPath: /opt/bitnami/pgpool/secrets/admin-password\n        - name: pgpool-creds\n          subPath: sr-check-password\n          mountPath: /opt/bitnami/pgpool/secrets/sr-check-password\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: postgresql-creds\n        secret:\n          secretName: release-name-postgresql-ha-postgresql\n          items:\n          - key: password\n            path: pgpool-password\n      - name: pgpool-creds\n        secret:\n          secretName: release-name-postgresql-ha-pgpool\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-postgresql-ha\" not found"
  },
  {
    "id": "417",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"configure-gitea\" does not have a read-only root file system"
  },
  {
    "id": "418",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"gitea\" does not have a read-only root file system"
  },
  {
    "id": "419",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"init-app-ini\" does not have a read-only root file system"
  },
  {
    "id": "420",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"init-directories\" does not have a read-only root file system"
  },
  {
    "id": "421",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"gitea\" is not set to runAsNonRoot"
  },
  {
    "id": "422",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"init-app-ini\" is not set to runAsNonRoot"
  },
  {
    "id": "423",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"init-directories\" is not set to runAsNonRoot"
  },
  {
    "id": "424",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"gitea\" has cpu request 0"
  },
  {
    "id": "425",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"configure-gitea\" has memory limit 0"
  },
  {
    "id": "426",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"gitea\" has memory limit 0"
  },
  {
    "id": "427",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"init-app-ini\" has memory limit 0"
  },
  {
    "id": "428",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/027_deployment_release-name-gitea.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitea\n  namespace: default\n  annotations: null\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 100%\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: gitea\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: bb454c57a0d08076ac35d4f54b5de0cf4b6d442941fd825366a47173d035eb5b\n      labels:\n        helm.sh/chart: gitea-12.4.0\n        app: gitea\n        app.kubernetes.io/name: gitea\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.24.6\n        version: 1.24.6\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      securityContext:\n        fsGroup: 1000\n      initContainers:\n      - name: init-directories\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/init_directory_structure.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: init-app-ini\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/sbinx/config_environment.sh\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMP_EXISTING_ENVS_FILE\n          value: /tmp/existing-envs\n        - name: ENV_TO_INI_MOUNT_POINT\n          value: /env-to-ini-mounts\n        volumeMounts:\n        - name: config\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        - name: inline-config-sources\n          mountPath: /env-to-ini-mounts/inlines/\n        securityContext: {}\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      - name: configure-gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        command:\n        - /usr/sbinx/configure_gitea.sh\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          runAsUser: 1000\n        env:\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        - name: GITEA_ADMIN_USERNAME\n          value: gitea_admin\n        - name: GITEA_ADMIN_PASSWORD\n          value: r8sA8CPHD9!bt6d\n        - name: GITEA_ADMIN_PASSWORD_MODE\n          value: keepUpdated\n        volumeMounts:\n        - name: init\n          mountPath: /usr/sbinx\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n        resources:\n          limits: {}\n          requests:\n            cpu: 100m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: gitea\n        image: docker.gitea.com/gitea:1.24.6-rootless\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SSH_LISTEN_PORT\n          value: '2222'\n        - name: SSH_PORT\n          value: '22'\n        - name: GITEA_APP_INI\n          value: /data/gitea/conf/app.ini\n        - name: GITEA_CUSTOM\n          value: /data/gitea\n        - name: GITEA_WORK_DIR\n          value: /data\n        - name: GITEA_TEMP\n          value: /tmp/gitea\n        - name: TMPDIR\n          value: /tmp/gitea\n        - name: HOME\n          value: /data/gitea/git\n        ports:\n        - name: ssh\n          containerPort: 2222\n        - name: http\n          containerPort: 3000\n        livenessProbe:\n          failureThreshold: 10\n          initialDelaySeconds: 200\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          tcpSocket:\n            port: http\n          timeoutSeconds: 1\n        resources: {}\n        securityContext: {}\n        volumeMounts:\n        - name: temp\n          mountPath: /tmp\n        - name: data\n          mountPath: /data\n      volumes:\n      - name: init\n        secret:\n          secretName: release-name-gitea-init\n          defaultMode: 110\n      - name: config\n        secret:\n          secretName: release-name-gitea\n          defaultMode: 110\n      - name: inline-config-sources\n        secret:\n          secretName: release-name-gitea-inline-config\n      - name: temp\n        emptyDir: {}\n      - name: data\n        persistentVolumeClaim:\n          claimName: gitea-shared-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"init-directories\" has memory limit 0"
  },
  {
    "id": "429",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/028_statefulset_release-name-postgresql-ha-postgresql.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-postgresql-ha-postgresql\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: postgresql-ha\n    app.kubernetes.io/version: 17.6.0\n    helm.sh/chart: postgresql-ha-16.3.2\n    app.kubernetes.io/component: postgresql\n    role: data\nspec:\n  replicas: 3\n  podManagementPolicy: Parallel\n  serviceName: release-name-postgresql-ha-postgresql-headless\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: postgresql-ha\n      app.kubernetes.io/component: postgresql\n      role: data\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: postgresql-ha\n        app.kubernetes.io/version: 17.6.0\n        helm.sh/chart: postgresql-ha-16.3.2\n        app.kubernetes.io/component: postgresql\n        role: data\n      annotations: null\n    spec:\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: postgresql-ha\n                  app.kubernetes.io/component: postgresql\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-postgresql-ha\n      hostNetwork: false\n      hostIPC: false\n      containers:\n      - name: postgresql\n        image: docker.io/bitnamilegacy/postgresql-repmgr:17.6.0-debian-12-r2\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /pre-stop.sh\n              - '25'\n        env:\n        - name: BITNAMI_DEBUG\n          value: 'false'\n        - name: POSTGRESQL_VOLUME_DIR\n          value: /bitnami/postgresql\n        - name: PGDATA\n          value: /bitnami/postgresql/data\n        - name: POSTGRES_POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/postgres-password\n        - name: POSTGRES_USER\n          value: gitea\n        - name: POSTGRES_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/password\n        - name: POSTGRES_DB\n          value: gitea\n        - name: POSTGRESQL_LOG_HOSTNAME\n          value: 'true'\n        - name: POSTGRESQL_LOG_CONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_LOG_DISCONNECTIONS\n          value: 'false'\n        - name: POSTGRESQL_PGAUDIT_LOG_CATALOG\n          value: 'off'\n        - name: POSTGRESQL_CLIENT_MIN_MESSAGES\n          value: error\n        - name: POSTGRESQL_SHARED_PRELOAD_LIBRARIES\n          value: pgaudit, repmgr\n        - name: POSTGRESQL_ENABLE_TLS\n          value: 'no'\n        - name: POSTGRESQL_PORT_NUMBER\n          value: '5432'\n        - name: REPMGR_PORT_NUMBER\n          value: '5432'\n        - name: REPMGR_PRIMARY_PORT\n          value: '5432'\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: REPMGR_UPGRADE_EXTENSION\n          value: 'no'\n        - name: REPMGR_PGHBA_TRUST_ALL\n          value: 'no'\n        - name: REPMGR_MOUNTED_CONF_DIR\n          value: /bitnami/repmgr/conf\n        - name: REPMGR_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: REPMGR_PARTNER_NODES\n          value: release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,release-name-postgresql-ha-postgresql-1.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,release-name-postgresql-ha-postgresql-2.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local,\n        - name: REPMGR_PRIMARY_HOST\n          value: release-name-postgresql-ha-postgresql-0.release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local\n        - name: REPMGR_NODE_NAME\n          value: $(MY_POD_NAME)\n        - name: REPMGR_NODE_NETWORK_NAME\n          value: $(MY_POD_NAME).release-name-postgresql-ha-postgresql-headless.$(REPMGR_NAMESPACE).svc.cluster.local\n        - name: REPMGR_NODE_TYPE\n          value: data\n        - name: REPMGR_LOG_LEVEL\n          value: NOTICE\n        - name: REPMGR_CONNECT_TIMEOUT\n          value: '5'\n        - name: REPMGR_RECONNECT_ATTEMPTS\n          value: '2'\n        - name: REPMGR_RECONNECT_INTERVAL\n          value: '3'\n        - name: REPMGR_USERNAME\n          value: repmgr\n        - name: REPMGR_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/repmgr-password\n        - name: REPMGR_USE_PASSFILE\n          value: 'true'\n        - name: REPMGR_PASSFILE_PATH\n          value: /opt/bitnami/repmgr/conf/.pgpass\n        - name: REPMGR_DATABASE\n          value: repmgr\n        - name: REPMGR_FENCE_OLD_PRIMARY\n          value: 'no'\n        - name: REPMGR_CHILD_NODES_CHECK_INTERVAL\n          value: '5'\n        - name: REPMGR_CHILD_NODES_CONNECTED_MIN_COUNT\n          value: '1'\n        - name: REPMGR_CHILD_NODES_DISCONNECT_TIMEOUT\n          value: '30'\n        - name: POSTGRESQL_SR_CHECK\n          value: 'yes'\n        - name: POSTGRESQL_SR_CHECK_USERNAME\n          value: sr_check_user\n        - name: POSTGRESQL_SR_CHECK_DATABASE\n          value: postgres\n        - name: POSTGRESQL_SR_CHECK_PASSWORD_FILE\n          value: /opt/bitnami/postgresql/secrets/sr-check-password\n        envFrom: null\n        ports:\n        - name: postgresql\n          containerPort: 5432\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 15\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /liveness-probe.sh\n        readinessProbe:\n          failureThreshold: 6\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n          exec:\n            command:\n            - /readiness-probe.sh\n        resources:\n          limits:\n            cpu: 375m\n            ephemeral-storage: 2Gi\n            memory: 384Mi\n          requests:\n            cpu: 250m\n            ephemeral-storage: 50Mi\n            memory: 256Mi\n        volumeMounts:\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/conf\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/postgresql/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/conf\n          subPath: repmgr-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/tmp\n          subPath: repmgr-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/repmgr/logs\n          subPath: repmgr-logs-dir\n        - name: postgresql-creds\n          mountPath: /opt/bitnami/postgresql/secrets/postgres-password\n          subPath: postgres-password\n        - name: postgresql-creds\n          subPath: password\n          mountPath: /opt/bitnami/postgresql/secrets/password\n        - name: postgresql-creds\n          subPath: repmgr-password\n          mountPath: /opt/bitnami/postgresql/secrets/repmgr-password\n        - name: pgpool-creds\n          subPath: sr-check-password\n          mountPath: /opt/bitnami/postgresql/secrets/sr-check-password\n        - name: data\n          mountPath: /bitnami/postgresql\n        - name: scripts\n          mountPath: /pre-stop.sh\n          subPath: pre-stop.sh\n        - name: scripts\n          mountPath: /liveness-probe.sh\n          subPath: liveness-probe.sh\n        - name: scripts\n          mountPath: /readiness-probe.sh\n          subPath: readiness-probe.sh\n      volumes:\n      - name: empty-dir\n        emptyDir: {}\n      - name: scripts\n        configMap:\n          name: release-name-postgresql-ha-postgresql-scripts\n          defaultMode: 493\n      - name: postgresql-creds\n        secret:\n          secretName: release-name-postgresql-ha-postgresql\n      - name: pgpool-creds\n        secret:\n          secretName: release-name-postgresql-ha-pgpool\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-postgresql-ha\" not found"
  },
  {
    "id": "430",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/029_statefulset_release-name-valkey-cluster.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-valkey-cluster\n  namespace: default\n  labels:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/name: valkey-cluster\n    app.kubernetes.io/version: 8.1.3\n    helm.sh/chart: valkey-cluster-3.0.24\nspec:\n  updateStrategy:\n    rollingUpdate:\n      partition: 0\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: valkey-cluster\n  replicas: 3\n  serviceName: release-name-valkey-cluster-headless\n  podManagementPolicy: Parallel\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/name: valkey-cluster\n        app.kubernetes.io/version: 8.1.3\n        helm.sh/chart: valkey-cluster-3.0.24\n      annotations:\n        checksum/scripts: b416f9c23ba0f844168db1fda55802c08f6e6158998b5dc543fb3e2827aee988\n        checksum/secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/config: d260a75cea2840d303318eeb5409b6f6f190e3e221ea66f3e98857402f575fa0\n    spec:\n      hostNetwork: false\n      enableServiceLinks: false\n      securityContext:\n        fsGroup: 1001\n        fsGroupChangePolicy: Always\n        supplementalGroups: []\n        sysctls: []\n      serviceAccountName: release-name-valkey-cluster\n      automountServiceAccountToken: false\n      affinity:\n        podAffinity: null\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchLabels:\n                  app.kubernetes.io/instance: release-name\n                  app.kubernetes.io/name: valkey-cluster\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n        nodeAffinity: null\n      containers:\n      - name: release-name-valkey-cluster\n        image: docker.io/bitnamilegacy/valkey-cluster:8.1.3-debian-12-r3\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1001\n          runAsNonRoot: true\n          runAsUser: 1001\n          seLinuxOptions: {}\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/bash\n        - -c\n        args:\n        - \"# Backwards compatibility change\\nif ! [[ -f /opt/bitnami/valkey/etc/valkey.conf\\\n          \\ ]]; then\\n    echo COPYING FILE\\n    cp  /opt/bitnami/valkey/etc/valkey-default.conf\\\n          \\ /opt/bitnami/valkey/etc/valkey.conf\\nfi\\npod_index=($(echo \\\"$POD_NAME\\\"\\\n          \\ | tr \\\"-\\\" \\\"\\\\n\\\"))\\npod_index=\\\"${pod_index[-1]}\\\"\\nif [[ \\\"$pod_index\\\"\\\n          \\ == \\\"0\\\" ]]; then\\n  export VALKEY_CLUSTER_CREATOR=\\\"yes\\\"\\n  export VALKEY_CLUSTER_REPLICAS=\\\"\\\n          0\\\"\\nfi\\n/opt/bitnami/scripts/valkey-cluster/entrypoint.sh /opt/bitnami/scripts/valkey-cluster/run.sh\\n\"\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VALKEY_NODES\n          value: 'release-name-valkey-cluster-0.release-name-valkey-cluster-headless\n            release-name-valkey-cluster-1.release-name-valkey-cluster-headless release-name-valkey-cluster-2.release-name-valkey-cluster-headless '\n        - name: ALLOW_EMPTY_PASSWORD\n          value: 'yes'\n        - name: VALKEY_AOF_ENABLED\n          value: 'yes'\n        - name: VALKEY_TLS_ENABLED\n          value: 'no'\n        - name: VALKEY_PORT_NUMBER\n          value: '6379'\n        ports:\n        - name: tcp-redis\n          containerPort: 6379\n        - name: tcp-redis-bus\n          containerPort: 16379\n        livenessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 6\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - sh\n            - -c\n            - /scripts/ping_liveness_local.sh 5\n        readinessProbe:\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          timeoutSeconds: 2\n          successThreshold: 1\n          failureThreshold: 5\n          exec:\n            command:\n            - sh\n            - -c\n            - /scripts/ping_readiness_local.sh 1\n        resources:\n          limits:\n            cpu: 150m\n            ephemeral-storage: 2Gi\n            memory: 192Mi\n          requests:\n            cpu: 100m\n            ephemeral-storage: 50Mi\n            memory: 128Mi\n        volumeMounts:\n        - name: scripts\n          mountPath: /scripts\n        - name: valkey-data\n          mountPath: /bitnami/valkey/data\n          subPath: null\n        - name: default-config\n          mountPath: /opt/bitnami/valkey/etc/valkey-default.conf\n          subPath: valkey-default.conf\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/etc/\n          subPath: app-conf-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/tmp\n          subPath: app-tmp-dir\n        - name: empty-dir\n          mountPath: /opt/bitnami/valkey/logs\n          subPath: app-logs-dir\n        - name: empty-dir\n          mountPath: /tmp\n          subPath: tmp-dir\n      volumes:\n      - name: scripts\n        configMap:\n          name: release-name-valkey-cluster-scripts\n          defaultMode: 493\n      - name: default-config\n        configMap:\n          name: release-name-valkey-cluster-default\n      - name: empty-dir\n        emptyDir: {}\n  volumeClaimTemplates:\n  - metadata:\n      name: valkey-data\n      labels:\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: valkey-cluster\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 8Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-valkey-cluster\" not found"
  },
  {
    "id": "431",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "432",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wget\" does not have a read-only root file system"
  },
  {
    "id": "433",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"wget\" is not set to runAsNonRoot"
  },
  {
    "id": "434",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "435",
    "manifest_path": "data/manifests/artifacthub/gitea/gitea/030_pod_release-name-gitea-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-gitea-test-connection\n  namespace: default\n  labels:\n    helm.sh/chart: gitea-12.4.0\n    app: gitea\n    app.kubernetes.io/name: gitea\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.24.6\n    version: 1.24.6\n    app.kubernetes.io/managed-by: Helm\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  containers:\n  - name: wget\n    image: busybox:latest\n    command:\n    - wget\n    args:\n    - release-name-gitea-http:3000\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "436",
    "manifest_path": "data/manifests/artifacthub/gitlab/gitlab-runner/002_deployment_release-name-gitlab-runner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources: {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"release-name-gitlab-runner\" does not have a read-only root file system"
  },
  {
    "id": "437",
    "manifest_path": "data/manifests/artifacthub/gitlab/gitlab-runner/002_deployment_release-name-gitlab-runner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources: {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-gitlab-runner\" has cpu request 0"
  },
  {
    "id": "438",
    "manifest_path": "data/manifests/artifacthub/gitlab/gitlab-runner/002_deployment_release-name-gitlab-runner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-gitlab-runner\n  namespace: default\n  labels:\n    app: release-name-gitlab-runner\n    chart: gitlab-runner-0.81.0\n    release: release-name\n    heritage: Helm\nspec:\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: release-name-gitlab-runner\n  template:\n    metadata:\n      labels:\n        app: release-name-gitlab-runner\n        chart: gitlab-runner-0.81.0\n        release: release-name\n        heritage: Helm\n      annotations:\n        checksum/configmap: b9a97367f708f909d06f31603187138f92ff2d5db59610719b91b9901db1e075\n    spec:\n      securityContext:\n        fsGroup: 65533\n        runAsUser: 100\n      terminationGracePeriodSeconds: 3600\n      serviceAccountName: ''\n      containers:\n      - name: release-name-gitlab-runner\n        image: registry.gitlab.com/gitlab-org/gitlab-runner:alpine-v18.4.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: false\n          runAsNonRoot: true\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /entrypoint\n              - unregister\n              - --all-runners\n        command:\n        - /usr/bin/dumb-init\n        - --\n        - /bin/bash\n        - /configmaps/entrypoint\n        env:\n        - name: CI_SERVER_URL\n          value: null\n        - name: RUNNER_EXECUTOR\n          value: kubernetes\n        - name: REGISTER_LOCKED\n          value: 'true'\n        - name: RUNNER_TAG_LIST\n          value: ''\n        - name: SESSION_SERVER_ADDRESS\n          value: null\n        livenessProbe:\n          exec:\n            command:\n            - /bin/bash\n            - /configmaps/check-live\n            - '3'\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          exec:\n            command:\n            - /usr/bin/pgrep\n            - gitlab.*runner\n          initialDelaySeconds: 60\n          timeoutSeconds: 4\n          periodSeconds: 60\n          successThreshold: 1\n          failureThreshold: 3\n        ports:\n        - name: metrics\n          containerPort: 9252\n        volumeMounts:\n        - name: etc-gitlab-runner\n          mountPath: /home/gitlab-runner/.gitlab-runner\n        - name: configmaps\n          mountPath: /configmaps\n        resources: {}\n      volumes:\n      - name: runner-secrets\n        emptyDir:\n          medium: Memory\n      - name: etc-gitlab-runner\n        emptyDir:\n          medium: Memory\n      - name: configmaps\n        configMap:\n          name: release-name-gitlab-runner\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-gitlab-runner\" has memory limit 0"
  },
  {
    "id": "439",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/008_service_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  type: ClusterIP\n  ports:\n  - name: service\n    port: 80\n    protocol: TCP\n    targetPort: grafana\n  selector:\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:grafana])"
  },
  {
    "id": "440",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "441",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-grafana\" not found"
  },
  {
    "id": "442",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "443",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/009_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 76dc78574fdf5e7dc5f7e5c5ba10f46944b189b4f96f822030e16cd4ac0d7f90\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "444",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"release-name-test\" does not have a read-only root file system"
  },
  {
    "id": "445",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-grafana-test\" not found"
  },
  {
    "id": "446",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"release-name-test\" is not set to runAsNonRoot"
  },
  {
    "id": "447",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-test\" has cpu request 0"
  },
  {
    "id": "448",
    "manifest_path": "data/manifests/artifacthub/grafana/grafana/012_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-test\" has memory limit 0"
  },
  {
    "id": "449",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/011_service_release-name-loki-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki-headless\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n    variant: headless\nspec:\n  clusterIP: None\n  ports:\n  - port: 3100\n    protocol: TCP\n    name: http-metrics\n    targetPort: http-metrics\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "450",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/012_service_release-name-loki-memberlist.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki-memberlist\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\nspec:\n  type: ClusterIP\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: http\n    port: 7946\n    targetPort: memberlist-port\n    protocol: TCP\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "451",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/013_service_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  type: ClusterIP\n  ports:\n  - port: 3100\n    protocol: TCP\n    name: http-metrics\n    targetPort: http-metrics\n  selector:\n    app: loki\n    release: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:loki release:release-name])"
  },
  {
    "id": "452",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-promtail\" not found"
  },
  {
    "id": "453",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"promtail\" is not set to runAsNonRoot"
  },
  {
    "id": "454",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"promtail\" has cpu request 0"
  },
  {
    "id": "455",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/014_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.15.5\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.9.3\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: ce51ec04c807e159ed54a4abde551f3d3f9a92d09941d6dac409b3ea36396245\n    spec:\n      serviceAccountName: release-name-promtail\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:2.9.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"promtail\" has memory limit 0"
  },
  {
    "id": "456",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-loki\" not found"
  },
  {
    "id": "457",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"loki\" has cpu request 0"
  },
  {
    "id": "458",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/015_statefulset_release-name-loki.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-loki\n  namespace: default\n  labels:\n    app: loki\n    chart: loki-2.16.0\n    release: release-name\n    heritage: Helm\n  annotations: {}\nspec:\n  podManagementPolicy: OrderedReady\n  replicas: 1\n  selector:\n    matchLabels:\n      app: loki\n      release: release-name\n  serviceName: release-name-loki-headless\n  updateStrategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: loki\n        name: release-name-loki\n        release: release-name\n      annotations:\n        checksum/config: f9b589982c89ac0bea7797751079655a51e67d7d41c063ed83f3ab5b4b110f60\n        prometheus.io/port: http-metrics\n        prometheus.io/scrape: 'true'\n    spec:\n      serviceAccountName: release-name-loki\n      securityContext:\n        fsGroup: 10001\n        runAsGroup: 10001\n        runAsNonRoot: true\n        runAsUser: 10001\n      initContainers: []\n      containers:\n      - name: loki\n        image: grafana/loki:2.6.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/loki/loki.yaml\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        - name: config\n          mountPath: /etc/loki\n        - name: storage\n          mountPath: /data\n          subPath: null\n        ports:\n        - name: http-metrics\n          containerPort: 3100\n          protocol: TCP\n        - name: grpc\n          containerPort: 9095\n          protocol: TCP\n        - name: memberlist-port\n          containerPort: 7946\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 45\n        resources: {}\n        securityContext:\n          readOnlyRootFilesystem: true\n        env: null\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n      terminationGracePeriodSeconds: 4800\n      volumes:\n      - name: tmp\n        emptyDir: {}\n      - name: config\n        secret:\n          secretName: release-name-loki\n      - name: storage\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"loki\" has memory limit 0"
  },
  {
    "id": "459",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"test\" does not have a read-only root file system"
  },
  {
    "id": "460",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"test\" is not set to runAsNonRoot"
  },
  {
    "id": "461",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"test\" has cpu request 0"
  },
  {
    "id": "462",
    "manifest_path": "data/manifests/artifacthub/grafana/loki-stack/016_pod_release-name-loki-stack-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    helm.sh/hook: test-success\n  labels:\n    app: loki-stack\n    chart: loki-stack-2.10.2\n    release: release-name\n    heritage: Helm\n  name: release-name-loki-stack-test\nspec:\n  containers:\n  - name: test\n    image: bats/bats:1.8.2\n    imagePullPolicy: ''\n    args:\n    - /var/lib/loki/test.sh\n    env:\n    - name: LOKI_SERVICE\n      value: release-name-loki\n    - name: LOKI_PORT\n      value: '3100'\n    volumeMounts:\n    - name: tests\n      mountPath: /var/lib/loki\n  restartPolicy: Never\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-loki-stack-test\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"test\" has memory limit 0"
  },
  {
    "id": "463",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-promtail\" not found"
  },
  {
    "id": "464",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"promtail\" is not set to runAsNonRoot"
  },
  {
    "id": "465",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"promtail\" has cpu request 0"
  },
  {
    "id": "466",
    "manifest_path": "data/manifests/artifacthub/grafana/promtail/005_daemonset_release-name-promtail.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-promtail\n  namespace: default\n  labels:\n    helm.sh/chart: promtail-6.17.0\n    app.kubernetes.io/name: promtail\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 3.5.1\n    app.kubernetes.io/managed-by: Helm\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: promtail\n      app.kubernetes.io/instance: release-name\n  updateStrategy: {}\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: promtail\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 2ef0f14afc8ed4b72495a244ef20e42a4bc14afd488033fe94e04c341b97529a\n    spec:\n      serviceAccountName: release-name-promtail\n      automountServiceAccountToken: true\n      enableServiceLinks: true\n      securityContext:\n        runAsGroup: 0\n        runAsUser: 0\n      containers:\n      - name: promtail\n        image: docker.io/grafana/promtail:3.5.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - -config.file=/etc/promtail/promtail.yaml\n        volumeMounts:\n        - name: config\n          mountPath: /etc/promtail\n        - mountPath: /run/promtail\n          name: run\n        - mountPath: /var/lib/docker/containers\n          name: containers\n          readOnly: true\n        - mountPath: /var/log/pods\n          name: pods\n          readOnly: true\n        env:\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        ports:\n        - name: http-metrics\n          containerPort: 3101\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /ready\n            port: http-metrics\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n      tolerations:\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/master\n        operator: Exists\n      - effect: NoSchedule\n        key: node-role.kubernetes.io/control-plane\n        operator: Exists\n      volumes:\n      - name: config\n        secret:\n          secretName: release-name-promtail\n      - hostPath:\n          path: /run/promtail\n        name: run\n      - hostPath:\n          path: /var/lib/docker/containers\n        name: containers\n      - hostPath:\n          path: /var/log/pods\n        name: pods\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"promtail\" has memory limit 0"
  },
  {
    "id": "467",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/017_service_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-web\n    port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: core\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:core release:release-name])"
  },
  {
    "id": "468",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/018_service_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 5432\n  selector:\n    release: release-name\n    app: harbor\n    component: database\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:database release:release-name])"
  },
  {
    "id": "469",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/019_service_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-jobservice\n    port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: jobservice\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:jobservice release:release-name])"
  },
  {
    "id": "470",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/020_service_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: portal\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:portal release:release-name])"
  },
  {
    "id": "471",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/021_service_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - port: 6379\n  selector:\n    release: release-name\n    app: harbor\n    component: redis\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:redis release:release-name])"
  },
  {
    "id": "472",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/022_service_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-registry\n    port: 5000\n  - name: http-controller\n    port: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: registry\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:registry release:release-name])"
  },
  {
    "id": "473",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/023_service_release-name-harbor-trivy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-harbor-trivy\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\nspec:\n  ports:\n  - name: http-trivy\n    protocol: TCP\n    port: 8080\n  selector:\n    release: release-name\n    app: harbor\n    component: trivy\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:harbor component:trivy release:release-name])"
  },
  {
    "id": "474",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"core\" does not have a read-only root file system"
  },
  {
    "id": "475",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"core\" has cpu request 0"
  },
  {
    "id": "476",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/024_deployment_release-name-harbor-core.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-core\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: core\n    app.kubernetes.io/component: core\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: core\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: core\n        app.kubernetes.io/component: core\n      annotations:\n        checksum/configmap: 9f10ac573f31bb0810150a5f7c1f04fe8883b5850e0c0864ac1413792af939c2\n        checksum/secret: a96e028877820958786a1bec0806cc8371456bb27c2a090276a01f71e3ea2dd6\n        checksum/secret-jobservice: 65d7ad3b4b4e98e4243bfc35342599fa0006c75bab7916ff31b211456a37c582\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: core\n        image: goharbor/harbor-core:v2.14.0\n        imagePullPolicy: IfNotPresent\n        startupProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 360\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v2.0/ping\n            scheme: HTTP\n            port: 8080\n          failureThreshold: 2\n          periodSeconds: 10\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-core\n        - secretRef:\n            name: release-name-harbor-core\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: config\n          mountPath: /etc/core/app.conf\n          subPath: app.conf\n        - name: secret-key\n          mountPath: /etc/core/key\n          subPath: key\n        - name: token-service-private-key\n          mountPath: /etc/core/private_key.pem\n          subPath: tls.key\n        - name: ca-download\n          mountPath: /etc/core/ca\n        - name: psc\n          mountPath: /etc/core/token\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-harbor-core\n          items:\n          - key: app.conf\n            path: app.conf\n      - name: secret-key\n        secret:\n          secretName: release-name-harbor-core\n          items:\n          - key: secretKey\n            path: key\n      - name: token-service-private-key\n        secret:\n          secretName: release-name-harbor-core\n      - name: ca-download\n        secret:\n          secretName: release-name-harbor-ingress\n      - name: psc\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"core\" has memory limit 0"
  },
  {
    "id": "477",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"jobservice\" does not have a read-only root file system"
  },
  {
    "id": "478",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"jobservice\" has cpu request 0"
  },
  {
    "id": "479",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/025_deployment_release-name-harbor-jobservice.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-jobservice\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: jobservice\n    app.kubernetes.io/component: jobservice\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: jobservice\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: jobservice\n        app.kubernetes.io/component: jobservice\n      annotations:\n        checksum/configmap: 244ec50e63efe66fc3ec84e285fa0e8c83e019830b4462a17e81bc3b0745982a\n        checksum/configmap-env: 1c2af1daccf2ab6f5e6433b5d6de46e5b3610a14ef6337b0c14980f428212ea0\n        checksum/secret: 837d754c99f5985f960d8aed969e7884560427df8584bf6d1706a2915a66b198\n        checksum/secret-core: c412b2f1985d742be1ff1d9f161c9ea9a1bf4d2b525a362636620f5ce60ad18a\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: jobservice\n        image: goharbor/harbor-jobservice:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/v1/stats\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-jobservice-env\n        - secretRef:\n            name: release-name-harbor-jobservice\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: jobservice-config\n          mountPath: /etc/jobservice/config.yml\n          subPath: config.yml\n        - name: job-logs\n          mountPath: /var/log/jobs\n          subPath: null\n      volumes:\n      - name: jobservice-config\n        configMap:\n          name: release-name-harbor-jobservice\n      - name: job-logs\n        persistentVolumeClaim:\n          claimName: release-name-harbor-jobservice\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"jobservice\" has memory limit 0"
  },
  {
    "id": "480",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/026_deployment_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"portal\" does not have a read-only root file system"
  },
  {
    "id": "481",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/026_deployment_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"portal\" has cpu request 0"
  },
  {
    "id": "482",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/026_deployment_release-name-harbor-portal.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-portal\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: portal\n    app.kubernetes.io/component: portal\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: portal\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: portal\n        app.kubernetes.io/component: portal\n      annotations:\n        checksum/configmap: c9e04324738b148b4530aa2bd57b606d50600544b949ceef09270ef9c207bf85\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: portal\n        image: goharbor/harbor-portal:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: portal-config\n          mountPath: /etc/nginx/nginx.conf\n          subPath: nginx.conf\n      volumes:\n      - name: portal-config\n        configMap:\n          name: release-name-harbor-portal\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"portal\" has memory limit 0"
  },
  {
    "id": "483",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"registry\" does not have a read-only root file system"
  },
  {
    "id": "484",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"registryctl\" does not have a read-only root file system"
  },
  {
    "id": "485",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"registry\" has cpu request 0"
  },
  {
    "id": "486",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"registryctl\" has cpu request 0"
  },
  {
    "id": "487",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"registry\" has memory limit 0"
  },
  {
    "id": "488",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/027_deployment_release-name-harbor-registry.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-harbor-registry\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: registry\n    app.kubernetes.io/component: registry\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: registry\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: registry\n        app.kubernetes.io/component: registry\n      annotations:\n        checksum/configmap: b2a013a32036502b320d086fb71abf4702ba64f19e1c14222e31a2824982ad92\n        checksum/secret: 3f19d74357204ca317a51986a152e29cd0315df70f7eba2dbd585fe1938fa89d\n        checksum/secret-jobservice: 1502b63330d5a64588c432e97b8166219b6868bb9fc8ca2885e9f36e44f48efd\n        checksum/secret-core: c2f598f5d58a28ad67529eec72b7814fbbfd9b8c9012c9bd48cc51eda35cb56b\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n        fsGroupChangePolicy: OnRootMismatch\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: registry\n        image: goharbor/registry-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /\n            scheme: HTTP\n            port: 5000\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-registry\n        env: null\n        ports:\n        - containerPort: 5000\n        - containerPort: 5001\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-htpasswd\n          mountPath: /etc/registry/passwd\n          subPath: passwd\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n      - name: registryctl\n        image: goharbor/harbor-registryctl:v2.14.0\n        imagePullPolicy: IfNotPresent\n        livenessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            scheme: HTTP\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        envFrom:\n        - configMapRef:\n            name: release-name-harbor-registryctl\n        - secretRef:\n            name: release-name-harbor-registry\n        - secretRef:\n            name: release-name-harbor-registryctl\n        env:\n        - name: CORE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-core\n              key: secret\n        - name: JOBSERVICE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-jobservice\n              key: JOBSERVICE_SECRET\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - name: registry-data\n          mountPath: /storage\n          subPath: null\n        - name: registry-config\n          mountPath: /etc/registry/config.yml\n          subPath: config.yml\n        - name: registry-config\n          mountPath: /etc/registryctl/config.yml\n          subPath: ctl-config.yml\n      volumes:\n      - name: registry-htpasswd\n        secret:\n          secretName: release-name-harbor-registry-htpasswd\n          items:\n          - key: REGISTRY_HTPASSWD\n            path: passwd\n      - name: registry-config\n        configMap:\n          name: release-name-harbor-registry\n      - name: registry-data\n        persistentVolumeClaim:\n          claimName: release-name-harbor-registry\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"registryctl\" has memory limit 0"
  },
  {
    "id": "489",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"data-permissions-ensurer\" does not have a read-only root file system"
  },
  {
    "id": "490",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"database\" does not have a read-only root file system"
  },
  {
    "id": "491",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"data-permissions-ensurer\" has cpu request 0"
  },
  {
    "id": "492",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"database\" has cpu request 0"
  },
  {
    "id": "493",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"data-permissions-ensurer\" has memory limit 0"
  },
  {
    "id": "494",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/028_statefulset_release-name-harbor-database.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-database\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: database\n    app.kubernetes.io/component: database\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-database\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: database\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: database\n        app.kubernetes.io/component: database\n      annotations:\n        checksum/secret: dee1c88277937d69af2062f5960366add60a3730428bb1dc41b98d04e8bc57eb\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      initContainers:\n      - name: data-permissions-ensurer\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - chmod -R 700 /var/lib/postgresql/data/pgdata || true\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n      containers:\n      - name: database\n        image: goharbor/harbor-db:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 300\n          periodSeconds: 10\n          timeoutSeconds: 1\n        readinessProbe:\n          exec:\n            command:\n            - /docker-healthcheck.sh\n          initialDelaySeconds: 1\n          periodSeconds: 10\n          timeoutSeconds: 1\n        envFrom:\n        - secretRef:\n            name: release-name-harbor-database\n        env:\n        - name: PGDATA\n          value: /var/lib/postgresql/data/pgdata\n        volumeMounts:\n        - name: database-data\n          mountPath: /var/lib/postgresql/data\n          subPath: null\n        - name: shm-volume\n          mountPath: /dev/shm\n      volumes:\n      - name: shm-volume\n        emptyDir:\n          medium: Memory\n          sizeLimit: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: database-data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"database\" has memory limit 0"
  },
  {
    "id": "496",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/029_statefulset_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "498",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/029_statefulset_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "499",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/029_statefulset_release-name-harbor-redis.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-redis\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: redis\n    app.kubernetes.io/component: redis\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-redis\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: redis\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: redis\n        app.kubernetes.io/component: redis\n    spec:\n      securityContext:\n        runAsUser: 999\n        fsGroup: 999\n      automountServiceAccountToken: false\n      terminationGracePeriodSeconds: 120\n      containers:\n      - name: redis\n        image: goharbor/redis-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        livenessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 300\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 6379\n          initialDelaySeconds: 1\n          periodSeconds: 10\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/redis\n          subPath: null\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "500",
    "manifest_path": "data/manifests/artifacthub/harbor/harbor/030_statefulset_release-name-harbor-trivy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-harbor-trivy\n  namespace: default\n  labels:\n    heritage: Helm\n    release: release-name\n    chart: harbor\n    app: harbor\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: harbor\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: harbor\n    app.kubernetes.io/version: 2.14.0\n    component: trivy\n    app.kubernetes.io/component: trivy\nspec:\n  replicas: 1\n  serviceName: release-name-harbor-trivy\n  selector:\n    matchLabels:\n      release: release-name\n      app: harbor\n      component: trivy\n  template:\n    metadata:\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/name: harbor\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: harbor\n        app.kubernetes.io/version: 2.14.0\n        component: trivy\n        app.kubernetes.io/component: trivy\n      annotations:\n        checksum/secret: 6fbc9db4d37c6bc53260a46776f92e1e0390c1d50cd884e19a8d0a3c73deed8f\n    spec:\n      securityContext:\n        runAsUser: 10000\n        fsGroup: 10000\n      automountServiceAccountToken: false\n      containers:\n      - name: trivy\n        image: goharbor/trivy-adapter-photon:v2.14.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: HTTP_PROXY\n          value: ''\n        - name: HTTPS_PROXY\n          value: ''\n        - name: NO_PROXY\n          value: release-name-harbor-core,release-name-harbor-jobservice,release-name-harbor-database,release-name-harbor-registry,release-name-harbor-portal,release-name-harbor-trivy,release-name-harbor-exporter,127.0.0.1,localhost,.local,.internal\n        - name: SCANNER_LOG_LEVEL\n          value: info\n        - name: SCANNER_TRIVY_CACHE_DIR\n          value: /home/scanner/.cache/trivy\n        - name: SCANNER_TRIVY_REPORTS_DIR\n          value: /home/scanner/.cache/reports\n        - name: SCANNER_TRIVY_DEBUG_MODE\n          value: 'false'\n        - name: SCANNER_TRIVY_VULN_TYPE\n          value: os,library\n        - name: SCANNER_TRIVY_TIMEOUT\n          value: 5m0s\n        - name: SCANNER_TRIVY_GITHUB_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: gitHubToken\n        - name: SCANNER_TRIVY_SEVERITY\n          value: UNKNOWN,LOW,MEDIUM,HIGH,CRITICAL\n        - name: SCANNER_TRIVY_IGNORE_UNFIXED\n          value: 'false'\n        - name: SCANNER_TRIVY_SKIP_UPDATE\n          value: 'false'\n        - name: SCANNER_TRIVY_SKIP_JAVA_DB_UPDATE\n          value: 'false'\n        - name: SCANNER_TRIVY_DB_REPOSITORY\n          value: mirror.gcr.io/aquasec/trivy-db,ghcr.io/aquasecurity/trivy-db\n        - name: SCANNER_TRIVY_JAVA_DB_REPOSITORY\n          value: mirror.gcr.io/aquasec/trivy-java-db,ghcr.io/aquasecurity/trivy-java-db\n        - name: SCANNER_TRIVY_OFFLINE_SCAN\n          value: 'false'\n        - name: SCANNER_TRIVY_SECURITY_CHECKS\n          value: vuln\n        - name: SCANNER_TRIVY_INSECURE\n          value: 'false'\n        - name: SCANNER_API_SERVER_ADDR\n          value: :8080\n        - name: SCANNER_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        - name: SCANNER_STORE_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        - name: SCANNER_JOB_QUEUE_REDIS_URL\n          valueFrom:\n            secretKeyRef:\n              name: release-name-harbor-trivy\n              key: redisURL\n        ports:\n        - name: api-server\n          containerPort: 8080\n        volumeMounts:\n        - name: data\n          mountPath: /home/scanner/.cache\n          subPath: null\n          readOnly: false\n        livenessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /probe/healthy\n            port: api-server\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 10\n        readinessProbe:\n          httpGet:\n            scheme: HTTP\n            path: /probe/ready\n            port: api-server\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          failureThreshold: 3\n        resources:\n          limits:\n            cpu: 1\n            memory: 1Gi\n          requests:\n            cpu: 200m\n            memory: 512Mi\n  volumeClaimTemplates:\n  - apiVersion: v1\n    kind: PersistentVolumeClaim\n    metadata:\n      name: data\n      labels:\n        heritage: Helm\n        release: release-name\n        chart: harbor\n        app: harbor\n      annotations: null\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"trivy\" does not have a read-only root file system"
  },
  {
    "id": "501",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/007_service_release-name-vault-agent-injector-svc.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault-agent-injector-svc\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ports:\n  - name: https\n    port: 443\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    component: webhook\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:vault-agent-injector component:webhook])"
  },
  {
    "id": "502",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/008_service_release-name-vault-internal.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault-internal\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.31.0\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    vault-internal: 'true'\n  annotations: null\nspec:\n  clusterIP: None\n  publishNotReadyAddresses: true\n  ports:\n  - name: http\n    port: 8200\n    targetPort: 8200\n  - name: https-internal\n    port: 8201\n    targetPort: 8201\n  selector:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    component: server\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:vault component:server])"
  },
  {
    "id": "503",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/009_service_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    helm.sh/chart: vault-0.31.0\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  publishNotReadyAddresses: true\n  ports:\n  - name: http\n    port: 8200\n    targetPort: 8200\n  - name: https-internal\n    port: 8201\n    targetPort: 8201\n  selector:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    component: server\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:vault component:server])"
  },
  {
    "id": "505",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"sidecar-injector\" does not have a read-only root file system"
  },
  {
    "id": "506",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-vault-agent-injector\" not found"
  },
  {
    "id": "509",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"sidecar-injector\" has cpu request 0"
  },
  {
    "id": "510",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/010_deployment_release-name-vault-agent-injector.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-vault-agent-injector\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault-agent-injector\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    component: webhook\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault-agent-injector\n      app.kubernetes.io/instance: release-name\n      component: webhook\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: vault-agent-injector\n        app.kubernetes.io/instance: release-name\n        component: webhook\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault-agent-injector\n                app.kubernetes.io/instance: release-name\n                component: webhook\n            topologyKey: kubernetes.io/hostname\n      serviceAccountName: release-name-vault-agent-injector\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      containers:\n      - name: sidecar-injector\n        image: hashicorp/vault-k8s:1.7.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n        env:\n        - name: AGENT_INJECT_LISTEN\n          value: :8080\n        - name: AGENT_INJECT_LOG_LEVEL\n          value: info\n        - name: AGENT_INJECT_VAULT_ADDR\n          value: http://release-name-vault.default.svc:8200\n        - name: AGENT_INJECT_VAULT_AUTH_PATH\n          value: auth/kubernetes\n        - name: AGENT_INJECT_VAULT_IMAGE\n          value: hashicorp/vault:1.20.4\n        - name: AGENT_INJECT_TLS_AUTO\n          value: release-name-vault-agent-injector-cfg\n        - name: AGENT_INJECT_TLS_AUTO_HOSTS\n          value: release-name-vault-agent-injector-svc,release-name-vault-agent-injector-svc.default,release-name-vault-agent-injector-svc.default.svc\n        - name: AGENT_INJECT_LOG_FORMAT\n          value: standard\n        - name: AGENT_INJECT_REVOKE_ON_SHUTDOWN\n          value: 'false'\n        - name: AGENT_INJECT_CPU_REQUEST\n          value: 250m\n        - name: AGENT_INJECT_CPU_LIMIT\n          value: 500m\n        - name: AGENT_INJECT_MEM_REQUEST\n          value: 64Mi\n        - name: AGENT_INJECT_MEM_LIMIT\n          value: 128Mi\n        - name: AGENT_INJECT_DEFAULT_TEMPLATE\n          value: map\n        - name: AGENT_INJECT_TEMPLATE_CONFIG_EXIT_ON_RETRY_FAILURE\n          value: 'true'\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - agent-inject\n        - 2>&1\n        livenessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 2\n          successThreshold: 1\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            path: /health/ready\n            port: 8080\n            scheme: HTTPS\n          failureThreshold: 12\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"sidecar-injector\" has memory limit 0"
  },
  {
    "id": "511",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/011_statefulset_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"vault\" does not have a read-only root file system"
  },
  {
    "id": "512",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/011_statefulset_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-vault\" not found"
  },
  {
    "id": "513",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/011_statefulset_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"vault\" has cpu request 0"
  },
  {
    "id": "514",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/011_statefulset_release-name-vault.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-vault\n  namespace: default\n  labels:\n    app.kubernetes.io/name: vault\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\nspec:\n  serviceName: release-name-vault-internal\n  podManagementPolicy: Parallel\n  replicas: 1\n  updateStrategy:\n    type: OnDelete\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: vault\n      app.kubernetes.io/instance: release-name\n      component: server\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: vault-0.31.0\n        app.kubernetes.io/name: vault\n        app.kubernetes.io/instance: release-name\n        component: server\n      annotations: null\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchLabels:\n                app.kubernetes.io/name: vault\n                app.kubernetes.io/instance: release-name\n                component: server\n            topologyKey: kubernetes.io/hostname\n      terminationGracePeriodSeconds: 10\n      serviceAccountName: release-name-vault\n      securityContext:\n        runAsNonRoot: true\n        runAsGroup: 1000\n        runAsUser: 100\n        fsGroup: 1000\n      hostNetwork: false\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-vault-config\n      - name: home\n        emptyDir: {}\n      containers:\n      - name: vault\n        image: hashicorp/vault:1.20.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -ec\n        args:\n        - \"cp /vault/config/extraconfig-from-values.hcl /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOST_IP}\\\" ] && sed -Ei \\\"s|HOST_IP|${HOST_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${POD_IP}\\\" ] && sed -Ei \\\"s|POD_IP|${POD_IP?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${HOSTNAME}\\\" ] && sed -Ei \\\"s|HOSTNAME|${HOSTNAME?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${API_ADDR}\\\" ] && sed -Ei \\\"s|API_ADDR|${API_ADDR?}|g\\\" /tmp/storageconfig.hcl;\\n\\\n          [ -n \\\"${TRANSIT_ADDR}\\\" ] && sed -Ei \\\"s|TRANSIT_ADDR|${TRANSIT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n[ -n \\\"${RAFT_ADDR}\\\" ] && sed -Ei \\\"s|RAFT_ADDR|${RAFT_ADDR?}|g\\\"\\\n          \\ /tmp/storageconfig.hcl;\\n/usr/local/bin/docker-entrypoint.sh vault server\\\n          \\ -config=/tmp/storageconfig.hcl \\n\"\n        securityContext:\n          allowPrivilegeEscalation: false\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: VAULT_K8S_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: VAULT_ADDR\n          value: http://127.0.0.1:8200\n        - name: VAULT_API_ADDR\n          value: http://$(POD_IP):8200\n        - name: SKIP_CHOWN\n          value: 'true'\n        - name: SKIP_SETCAP\n          value: 'true'\n        - name: HOSTNAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: VAULT_CLUSTER_ADDR\n          value: https://$(HOSTNAME).release-name-vault-internal:8201\n        - name: HOME\n          value: /home/vault\n        volumeMounts:\n        - name: data\n          mountPath: /vault/data\n        - name: config\n          mountPath: /vault/config\n        - name: home\n          mountPath: /home/vault\n        ports:\n        - containerPort: 8200\n          name: http\n        - containerPort: 8201\n          name: https-internal\n        - containerPort: 8202\n          name: http-rep\n        readinessProbe:\n          exec:\n            command:\n            - /bin/sh\n            - -ec\n            - vault status -tls-skip-verify\n          failureThreshold: 2\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          successThreshold: 1\n          timeoutSeconds: 3\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - sleep 5 && kill -SIGTERM $(pidof vault)\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"vault\" has memory limit 0"
  },
  {
    "id": "515",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n  restartPolicy: Never\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"release-name-server-test\" does not have a read-only root file system"
  },
  {
    "id": "516",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n  restartPolicy: Never\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"release-name-server-test\" is not set to runAsNonRoot"
  },
  {
    "id": "517",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-server-test\" has cpu request 0"
  },
  {
    "id": "518",
    "manifest_path": "data/manifests/artifacthub/hashicorp/vault/013_pod_release-name-vault-server-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-vault-server-test\n  namespace: default\n  annotations:\n    helm.sh/hook: test\nspec:\n  containers:\n  - name: release-name-server-test\n    image: hashicorp/vault:1.20.4\n    imagePullPolicy: IfNotPresent\n    env:\n    - name: VAULT_ADDR\n      value: http://release-name-vault.default.svc:8200\n    command:\n    - /bin/sh\n    - -c\n    - \"echo \\\"Checking for sealed info in 'vault status' output\\\"\\nATTEMPTS=10\\nn=0\\n\\\n      until [ \\\"$n\\\" -ge $ATTEMPTS ]\\ndo\\n  echo \\\"Attempt\\\" $n...\\n  vault status\\\n      \\ -format yaml | grep -E '^sealed: (true|false)' && break\\n  n=$((n+1))\\n  sleep\\\n      \\ 5\\ndone\\nif [ $n -ge $ATTEMPTS ]; then\\n  echo \\\"timed out looking for sealed\\\n      \\ info in 'vault status' output\\\"\\n  exit 1\\nfi\\n\\nexit 0\\n\"\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-server-test\" has memory limit 0"
  },
  {
    "id": "519",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/007_service_release-name-ingress-nginx-controller-admission.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller-admission\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - name: https-webhook\n    port: 443\n    targetPort: webhook\n    appProtocol: https\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:ingress-nginx])"
  },
  {
    "id": "520",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/008_service_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations: null\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  type: LoadBalancer\n  ipFamilyPolicy: SingleStack\n  ipFamilies:\n  - IPv4\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: http\n    appProtocol: http\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:ingress-nginx])"
  },
  {
    "id": "522",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"controller\" does not have a read-only root file system"
  },
  {
    "id": "523",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-ingress-nginx\" not found"
  },
  {
    "id": "525",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/009_deployment_release-name-ingress-nginx-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\n  name: release-name-ingress-nginx-controller\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: ingress-nginx\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  replicas: 1\n  revisionHistoryLimit: 10\n  minReadySeconds: 0\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: controller\n    spec:\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: controller\n        image: registry.k8s.io/ingress-nginx/controller:v1.13.3@sha256:1b044f6dcac3afbb59e05d98463f1dec6f3d3fb99940bc12ca5d80270358e3bd\n        imagePullPolicy: IfNotPresent\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /wait-shutdown\n        args:\n        - /nginx-ingress-controller\n        - --publish-service=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --election-id=release-name-ingress-nginx-leader\n        - --controller-class=k8s.io/ingress-nginx\n        - --ingress-class=nginx\n        - --configmap=$(POD_NAMESPACE)/release-name-ingress-nginx-controller\n        - --validating-webhook=:8443\n        - --validating-webhook-certificate=/usr/local/certificates/cert\n        - --validating-webhook-key=/usr/local/certificates/key\n        securityContext:\n          runAsNonRoot: true\n          runAsUser: 101\n          runAsGroup: 82\n          allowPrivilegeEscalation: false\n          seccompProfile:\n            type: RuntimeDefault\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n          readOnlyRootFilesystem: false\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: LD_PRELOAD\n          value: /usr/local/lib/libmimalloc.so\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthz\n            port: 10254\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: webhook\n          containerPort: 8443\n          protocol: TCP\n        volumeMounts:\n        - name: webhook-cert\n          mountPath: /usr/local/certificates/\n          readOnly: true\n        resources:\n          requests:\n            cpu: 100m\n            memory: 90Mi\n      nodeSelector:\n        kubernetes.io/os: linux\n      serviceAccountName: release-name-ingress-nginx\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: webhook-cert\n        secret:\n          secretName: release-name-ingress-nginx-admission\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "526",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/017_job_release-name-ingress-nginx-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-ingress-nginx-admission\" not found"
  },
  {
    "id": "527",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/017_job_release-name-ingress-nginx-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"create\" has cpu request 0"
  },
  {
    "id": "528",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/017_job_release-name-ingress-nginx-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-create\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-ingress-nginx-controller-admission,release-name-ingress-nginx-controller-admission.$(POD_NAMESPACE).svc\n        - --namespace=$(POD_NAMESPACE)\n        - --secret-name=release-name-ingress-nginx-admission\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"create\" has memory limit 0"
  },
  {
    "id": "529",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/018_job_release-name-ingress-nginx-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-ingress-nginx-admission\" not found"
  },
  {
    "id": "530",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/018_job_release-name-ingress-nginx-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"patch\" has cpu request 0"
  },
  {
    "id": "531",
    "manifest_path": "data/manifests/artifacthub/ingress-nginx/ingress-nginx/018_job_release-name-ingress-nginx-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-ingress-nginx-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    helm.sh/chart: ingress-nginx-4.13.3\n    app.kubernetes.io/name: ingress-nginx\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.13.3\n    app.kubernetes.io/part-of: ingress-nginx\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: admission-webhook\nspec:\n  ttlSecondsAfterFinished: 0\n  template:\n    metadata:\n      name: release-name-ingress-nginx-admission-patch\n      labels:\n        helm.sh/chart: ingress-nginx-4.13.3\n        app.kubernetes.io/name: ingress-nginx\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.13.3\n        app.kubernetes.io/part-of: ingress-nginx\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: admission-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3@sha256:3d671cf20a35cd94efc5dcd484970779eb21e7938c98fbc3673693b8a117cf39\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-ingress-nginx-admission\n        - --namespace=$(POD_NAMESPACE)\n        - --patch-mutating=false\n        - --secret-name=release-name-ingress-nginx-admission\n        - --patch-failure-policy=Fail\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n          seccompProfile:\n            type: RuntimeDefault\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-ingress-nginx-admission\n      automountServiceAccountToken: true\n      nodeSelector:\n        kubernetes.io/os: linux\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"patch\" has memory limit 0"
  },
  {
    "id": "532",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/001_poddisruptionbudget_istiod.yaml",
    "manifest_yaml": "apiVersion: policy/v1\nkind: PodDisruptionBudget\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    release: release-name\n    istio: pilot\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  minAvailable: 1\n  selector:\n    matchLabels:\n      app: istiod\n      istio: pilot\n",
    "policy_id": "pdb_unhealthy_eviction_policy",
    "violation_text": "unhealthyPodEvictionPolicy is not explicitly set"
  },
  {
    "id": "533",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/014_service_istiod.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    app: istiod\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  ports:\n  - port: 15010\n    name: grpc-xds\n    protocol: TCP\n  - port: 15012\n    name: https-dns\n    protocol: TCP\n  - port: 443\n    name: https-webhook\n    targetPort: 15017\n    protocol: TCP\n  - port: 15014\n    name: http-monitoring\n    protocol: TCP\n  selector:\n    app: istiod\n    istio: pilot\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:istiod istio:pilot])"
  },
  {
    "id": "534",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/015_deployment_istiod.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: 'false'\n        operator.istio.io/component: Pilot\n        istio: pilot\n        istio.io/dataplane-mode: none\n        app.kubernetes.io/name: istiod\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/part-of: istio\n        app.kubernetes.io/version: 1.27.1\n        helm.sh/chart: istiod-1.27.1\n      annotations:\n        prometheus.io/port: '15014'\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n    spec:\n      tolerations:\n      - key: cni.istio.io/not-ready\n        operator: Exists\n      serviceAccountName: istiod\n      containers:\n      - name: discovery\n        image: docker.io/istio/pilot:1.27.1\n        args:\n        - discovery\n        - --monitoringAddr=:15014\n        - --log_output_level=default:info\n        - --domain\n        - cluster.local\n        - --keepaliveMaxServerConnectionAge\n        - 30m\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: http-debug\n        - containerPort: 15010\n          protocol: TCP\n          name: grpc-xds\n        - containerPort: 15012\n          protocol: TCP\n          name: tls-xds\n        - containerPort: 15017\n          protocol: TCP\n          name: https-webhooks\n        - containerPort: 15014\n          protocol: TCP\n          name: http-monitoring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 3\n          timeoutSeconds: 5\n        env:\n        - name: REVISION\n          value: default\n        - name: PILOT_CERT_PROVIDER\n          value: istiod\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.serviceAccountName\n        - name: KUBECONFIG\n          value: /var/run/secrets/remote/config\n        - name: CA_TRUSTED_NODE_ACCOUNTS\n          value: default/ztunnel\n        - name: PILOT_TRACE_SAMPLING\n          value: '1'\n        - name: PILOT_ENABLE_ANALYSIS\n          value: 'false'\n        - name: CLUSTER_ID\n          value: Kubernetes\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: PLATFORM\n          value: ''\n        resources:\n          requests:\n            cpu: 500m\n            memory: 2048Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: istio-token\n          mountPath: /var/run/secrets/tokens\n          readOnly: true\n        - name: local-certs\n          mountPath: /var/run/secrets/istio-dns\n        - name: cacerts\n          mountPath: /etc/cacerts\n          readOnly: true\n        - name: istio-kubeconfig\n          mountPath: /var/run/secrets/remote\n          readOnly: true\n        - name: istio-csr-dns-cert\n          mountPath: /var/run/secrets/istiod/tls\n          readOnly: true\n        - name: istio-csr-ca-configmap\n          mountPath: /var/run/secrets/istiod/ca\n          readOnly: true\n      volumes:\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              audience: istio-ca\n              expirationSeconds: 43200\n              path: istio-token\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n      - name: istio-csr-dns-cert\n        secret:\n          secretName: istiod-tls\n          optional: true\n      - name: istio-csr-ca-configmap\n        configMap:\n          name: istio-ca-root-cert\n          defaultMode: 420\n          optional: true\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"istiod\" not found"
  },
  {
    "id": "535",
    "manifest_path": "data/manifests/artifacthub/istio-official/istiod/015_deployment_istiod.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istiod\n  namespace: default\n  labels:\n    app: istiod\n    istio.io/rev: default\n    install.operator.istio.io/owning-resource: unknown\n    operator.istio.io/component: Pilot\n    istio: pilot\n    release: release-name\n    app.kubernetes.io/name: istiod\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: istio\n    app.kubernetes.io/version: 1.27.1\n    helm.sh/chart: istiod-1.27.1\nspec:\n  strategy:\n    rollingUpdate:\n      maxSurge: 100%\n      maxUnavailable: 25%\n  selector:\n    matchLabels:\n      istio: pilot\n  template:\n    metadata:\n      labels:\n        app: istiod\n        istio.io/rev: default\n        install.operator.istio.io/owning-resource: unknown\n        sidecar.istio.io/inject: 'false'\n        operator.istio.io/component: Pilot\n        istio: pilot\n        istio.io/dataplane-mode: none\n        app.kubernetes.io/name: istiod\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/part-of: istio\n        app.kubernetes.io/version: 1.27.1\n        helm.sh/chart: istiod-1.27.1\n      annotations:\n        prometheus.io/port: '15014'\n        prometheus.io/scrape: 'true'\n        sidecar.istio.io/inject: 'false'\n    spec:\n      tolerations:\n      - key: cni.istio.io/not-ready\n        operator: Exists\n      serviceAccountName: istiod\n      containers:\n      - name: discovery\n        image: docker.io/istio/pilot:1.27.1\n        args:\n        - discovery\n        - --monitoringAddr=:15014\n        - --log_output_level=default:info\n        - --domain\n        - cluster.local\n        - --keepaliveMaxServerConnectionAge\n        - 30m\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n          name: http-debug\n        - containerPort: 15010\n          protocol: TCP\n          name: grpc-xds\n        - containerPort: 15012\n          protocol: TCP\n          name: tls-xds\n        - containerPort: 15017\n          protocol: TCP\n          name: https-webhooks\n        - containerPort: 15014\n          protocol: TCP\n          name: http-monitoring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8080\n          initialDelaySeconds: 1\n          periodSeconds: 3\n          timeoutSeconds: 5\n        env:\n        - name: REVISION\n          value: default\n        - name: PILOT_CERT_PROVIDER\n          value: istiod\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.serviceAccountName\n        - name: KUBECONFIG\n          value: /var/run/secrets/remote/config\n        - name: CA_TRUSTED_NODE_ACCOUNTS\n          value: default/ztunnel\n        - name: PILOT_TRACE_SAMPLING\n          value: '1'\n        - name: PILOT_ENABLE_ANALYSIS\n          value: 'false'\n        - name: CLUSTER_ID\n          value: Kubernetes\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: PLATFORM\n          value: ''\n        resources:\n          requests:\n            cpu: 500m\n            memory: 2048Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n        volumeMounts:\n        - name: istio-token\n          mountPath: /var/run/secrets/tokens\n          readOnly: true\n        - name: local-certs\n          mountPath: /var/run/secrets/istio-dns\n        - name: cacerts\n          mountPath: /etc/cacerts\n          readOnly: true\n        - name: istio-kubeconfig\n          mountPath: /var/run/secrets/remote\n          readOnly: true\n        - name: istio-csr-dns-cert\n          mountPath: /var/run/secrets/istiod/tls\n          readOnly: true\n        - name: istio-csr-ca-configmap\n          mountPath: /var/run/secrets/istiod/ca\n          readOnly: true\n      volumes:\n      - emptyDir:\n          medium: Memory\n        name: local-certs\n      - name: istio-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              audience: istio-ca\n              expirationSeconds: 43200\n              path: istio-token\n      - name: cacerts\n        secret:\n          secretName: cacerts\n          optional: true\n      - name: istio-kubeconfig\n        secret:\n          secretName: istio-kubeconfig\n          optional: true\n      - name: istio-csr-dns-cert\n        secret:\n          secretName: istiod-tls\n          optional: true\n      - name: istio-csr-ca-configmap\n        configMap:\n          name: istio-ca-root-cert\n          defaultMode: 420\n          optional: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"discovery\" has memory limit 0"
  },
  {
    "id": "536",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/010_service_release-name-jenkins-agent.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-jenkins-agent\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  ports:\n  - port: 50000\n    targetPort: 50000\n    name: agent-listener\n  selector:\n    app.kubernetes.io/component: jenkins-controller\n    app.kubernetes.io/instance: release-name\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:jenkins-controller app.kubernetes.io/instance:release-name])"
  },
  {
    "id": "537",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/011_service_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  ports:\n  - port: 8080\n    name: http\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/component: jenkins-controller\n    app.kubernetes.io/instance: release-name\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:jenkins-controller app.kubernetes.io/instance:release-name])"
  },
  {
    "id": "539",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-jenkins\" not found"
  },
  {
    "id": "540",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"config-reload\" has cpu request 0"
  },
  {
    "id": "541",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"config-reload-init\" has cpu request 0"
  },
  {
    "id": "542",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"config-reload\" has memory limit 0"
  },
  {
    "id": "543",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/012_statefulset_release-name-jenkins.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-jenkins\n  namespace: default\n  labels:\n    app.kubernetes.io/name: jenkins\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: jenkins-controller\n    helm.sh/chart: jenkins-5.8.98\nspec:\n  serviceName: release-name-jenkins\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: jenkins-controller\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: jenkins-controller\n        helm.sh/chart: jenkins-5.8.98\n      annotations:\n        checksum/config: e05b6d43d9ba96c1b1cd78116a387bdc4a776b05046455033c876ade1617360e\n    spec:\n      enableServiceLinks: false\n      securityContext:\n        runAsUser: 1000\n        fsGroup: 1000\n        runAsNonRoot: true\n      serviceAccountName: release-name-jenkins\n      automountServiceAccountToken: true\n      initContainers:\n      - name: config-reload-init\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: METHOD\n          value: LIST\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      - name: init\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        command:\n        - sh\n        - /var/jenkins_config/apply_config.sh\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n        - mountPath: /usr/share/jenkins/ref/plugins\n          name: plugins\n        - mountPath: /var/jenkins_plugins\n          name: plugin-dir\n        - mountPath: /tmp\n          name: tmp-volume\n      containers:\n      - name: jenkins\n        image: docker.io/jenkins/jenkins:2.516.3-jdk21\n        imagePullPolicy: Always\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 1000\n          runAsUser: 1000\n        args:\n        - --httpPort=8080\n        env:\n        - name: SECRETS\n          value: /run/secrets/additional\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: JAVA_OPTS\n          value: '-Dcasc.reload.token=$(POD_NAME) '\n        - name: JENKINS_OPTS\n          value: '--webroot=/var/jenkins_cache/war '\n        - name: JENKINS_SLAVE_AGENT_PORT\n          value: '50000'\n        - name: CASC_JENKINS_CONFIG\n          value: /var/jenkins_home/casc_configs\n        ports:\n        - containerPort: 8080\n          name: http\n        - containerPort: 50000\n          name: agent-listener\n        startupProbe:\n          failureThreshold: 12\n          httpGet:\n            path: /login\n            port: http\n          periodSeconds: 10\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /login\n            port: http\n          initialDelaySeconds: null\n          periodSeconds: 10\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 4096Mi\n          requests:\n            cpu: 50m\n            memory: 256Mi\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-home\n          readOnly: false\n        - mountPath: /var/jenkins_config\n          name: jenkins-config\n          readOnly: true\n        - mountPath: /usr/share/jenkins/ref/plugins/\n          name: plugin-dir\n          readOnly: false\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-secrets\n          mountPath: /run/secrets/additional\n          readOnly: true\n        - name: jenkins-cache\n          mountPath: /var/jenkins_cache\n        - mountPath: /tmp\n          name: tmp-volume\n      - name: config-reload\n        image: docker.io/kiwigrid/k8s-sidecar:1.30.7\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: LABEL\n          value: release-name-jenkins-jenkins-config\n        - name: FOLDER\n          value: /var/jenkins_home/casc_configs\n        - name: NAMESPACE\n          value: default\n        - name: REQ_URL\n          value: http://localhost:8080/reload-configuration-as-code/?casc-reload-token=$(POD_NAME)\n        - name: REQ_METHOD\n          value: POST\n        - name: REQ_RETRY_CONNECT\n          value: '10'\n        resources: {}\n        volumeMounts:\n        - name: sc-config-volume\n          mountPath: /var/jenkins_home/casc_configs\n        - name: jenkins-home\n          mountPath: /var/jenkins_home\n      volumes:\n      - name: plugins\n        emptyDir: {}\n      - name: jenkins-config\n        configMap:\n          name: release-name-jenkins\n      - name: plugin-dir\n        emptyDir: {}\n      - name: jenkins-secrets\n        projected:\n          sources:\n          - secret:\n              name: release-name-jenkins\n              items:\n              - key: jenkins-admin-user\n                path: chart-admin-username\n              - key: jenkins-admin-password\n                path: chart-admin-password\n      - name: jenkins-cache\n        emptyDir: {}\n      - name: jenkins-home\n        persistentVolumeClaim:\n          claimName: release-name-jenkins\n      - name: sc-config-volume\n        emptyDir: {}\n      - name: tmp-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"config-reload-init\" has memory limit 0"
  },
  {
    "id": "544",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"release-name-ui-test\" does not have a read-only root file system"
  },
  {
    "id": "545",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"test-framework\" does not have a read-only root file system"
  },
  {
    "id": "546",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"release-name-ui-test\" is not set to runAsNonRoot"
  },
  {
    "id": "547",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"test-framework\" is not set to runAsNonRoot"
  },
  {
    "id": "548",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-ui-test\" has cpu request 0"
  },
  {
    "id": "549",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"test-framework\" has cpu request 0"
  },
  {
    "id": "550",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-ui-test\" has memory limit 0"
  },
  {
    "id": "551",
    "manifest_path": "data/manifests/artifacthub/jenkinsci/jenkins/014_pod_release-name-ui-test-qwsqu.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-ui-test-qwsqu\n  namespace: default\n  annotations:\n    helm.sh/hook: test-success\nspec:\n  initContainers:\n  - name: test-framework\n    image: docker.io/bats/bats:1.12.0\n    command:\n    - bash\n    - -c\n    args:\n    - '# copy bats to tools dir\n\n      set -ex\n\n      cp -R /opt/bats /tools/bats/\n\n      '\n    volumeMounts:\n    - mountPath: /tools\n      name: tools\n  containers:\n  - name: release-name-ui-test\n    image: docker.io/jenkins/jenkins:2.516.3-jdk21\n    command:\n    - /tools/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n    - mountPath: /tools\n      name: tools\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-jenkins-tests\n  - name: tools\n    emptyDir: {}\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"test-framework\" has memory limit 0"
  },
  {
    "id": "552",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/014_service_release-name-kong-proxy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kong-proxy\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    enable-metrics: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - name: kong-proxy-tls\n    port: 443\n    targetPort: 8443\n    protocol: TCP\n  selector:\n    app.kubernetes.io/name: kong\n    app.kubernetes.io/component: app\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:app app.kubernetes.io/instance:release-name app.kubernetes.io/name:kong])"
  },
  {
    "id": "553",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/015_service_release-name-kubernetes-dashboard-api.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  annotations: null\n  name: release-name-kubernetes-dashboard-api\nspec:\n  type: ClusterIP\n  ports:\n  - name: api\n    port: 8000\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kubernetes-dashboard-api app.kubernetes.io/part-of:kubernetes-dashboard])"
  },
  {
    "id": "554",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/016_service_release-name-kubernetes-dashboard-auth.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-auth\n    app.kubernetes.io/version: 1.3.0\n    app.kubernetes.io/component: auth\n  annotations: null\n  name: release-name-kubernetes-dashboard-auth\nspec:\n  type: ClusterIP\n  ports:\n  - name: auth\n    port: 8000\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-auth\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kubernetes-dashboard-auth app.kubernetes.io/part-of:kubernetes-dashboard])"
  },
  {
    "id": "555",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/017_service_release-name-kubernetes-dashboard-metrics-scraper.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n    app.kubernetes.io/version: 1.2.2\n    app.kubernetes.io/component: metrics-scraper\n  annotations: null\n  name: release-name-kubernetes-dashboard-metrics-scraper\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8000\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kubernetes-dashboard-metrics-scraper app.kubernetes.io/part-of:kubernetes-dashboard])"
  },
  {
    "id": "556",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/018_service_release-name-kubernetes-dashboard-web.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n    app.kubernetes.io/version: 1.7.0\n    app.kubernetes.io/component: web\n  annotations: null\n  name: release-name-kubernetes-dashboard-web\nspec:\n  type: ClusterIP\n  ports:\n  - name: web\n    port: 8000\n    protocol: TCP\n  selector:\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kubernetes-dashboard-web app.kubernetes.io/part-of:kubernetes-dashboard])"
  },
  {
    "id": "557",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kong\" not found"
  },
  {
    "id": "558",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"clear-stale-pid\" has cpu request 0"
  },
  {
    "id": "559",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"proxy\" has cpu request 0"
  },
  {
    "id": "560",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"clear-stale-pid\" has memory limit 0"
  },
  {
    "id": "561",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/019_deployment_release-name-kong.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kong\n  namespace: default\n  labels:\n    app.kubernetes.io/name: kong\n    helm.sh/chart: kong-2.46.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: '3.8'\n    app.kubernetes.io/component: app\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kong\n      app.kubernetes.io/component: app\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        kuma.io/service-account-token-volume: release-name-kong-token\n        kuma.io/gateway: enabled\n        traffic.sidecar.istio.io/includeInboundPorts: ''\n      labels:\n        app.kubernetes.io/name: kong\n        helm.sh/chart: kong-2.46.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: '3.8'\n        app.kubernetes.io/component: app\n        app: release-name-kong\n        version: '3.8'\n    spec:\n      serviceAccountName: release-name-kong\n      automountServiceAccountToken: false\n      initContainers:\n      - name: clear-stale-pid\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        resources: {}\n        command:\n        - rm\n        - -vrf\n        - $KONG_PREFIX/pids\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n      containers:\n      - name: proxy\n        image: kong:3.8\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KONG_ADMIN_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_GUI_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_ADMIN_GUI_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ADMIN_LISTEN\n          value: 127.0.0.1:8444 http2 ssl, [::1]:8444 http2 ssl\n        - name: KONG_CLUSTER_LISTEN\n          value: 'off'\n        - name: KONG_DATABASE\n          value: 'off'\n        - name: KONG_DECLARATIVE_CONFIG\n          value: /kong_dbless/kong.yml\n        - name: KONG_DNS_ORDER\n          value: LAST,A,CNAME,AAAA,SRV\n        - name: KONG_LUA_PACKAGE_PATH\n          value: /opt/?.lua;/opt/?/init.lua;;\n        - name: KONG_NGINX_WORKER_PROCESSES\n          value: '1'\n        - name: KONG_PLUGINS\n          value: 'off'\n        - name: KONG_PORTAL_API_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PORTAL_API_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PORT_MAPS\n          value: 443:8443\n        - name: KONG_PREFIX\n          value: /kong_prefix/\n        - name: KONG_PROXY_ACCESS_LOG\n          value: /dev/stdout\n        - name: KONG_PROXY_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_PROXY_LISTEN\n          value: 0.0.0.0:8443 http2 ssl, [::]:8443 http2 ssl\n        - name: KONG_PROXY_STREAM_ACCESS_LOG\n          value: /dev/stdout basic\n        - name: KONG_PROXY_STREAM_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_ROUTER_FLAVOR\n          value: traditional\n        - name: KONG_STATUS_ACCESS_LOG\n          value: 'off'\n        - name: KONG_STATUS_ERROR_LOG\n          value: /dev/stderr\n        - name: KONG_STATUS_LISTEN\n          value: 0.0.0.0:8100, [::]:8100\n        - name: KONG_STREAM_LISTEN\n          value: 'off'\n        - name: KONG_NGINX_DAEMON\n          value: 'off'\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - kong\n              - quit\n              - --wait=15\n        ports:\n        - name: proxy-tls\n          containerPort: 8443\n          protocol: TCP\n        - name: status\n          containerPort: 8100\n          protocol: TCP\n        volumeMounts:\n        - name: release-name-kong-prefix-dir\n          mountPath: /kong_prefix/\n        - name: release-name-kong-tmp\n          mountPath: /tmp\n        - name: kong-custom-dbless-config-volume\n          mountPath: /kong_dbless/\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status/ready\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /status\n            port: status\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n      securityContext: {}\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: release-name-kong-prefix-dir\n        emptyDir:\n          sizeLimit: 256Mi\n      - name: release-name-kong-tmp\n        emptyDir:\n          sizeLimit: 1Gi\n      - name: release-name-kong-token\n        projected:\n          sources:\n          - serviceAccountToken:\n              expirationSeconds: 3607\n              path: token\n          - configMap:\n              items:\n              - key: ca.crt\n                path: ca.crt\n              name: kube-root-ca.crt\n          - downwardAPI:\n              items:\n              - fieldRef:\n                  apiVersion: v1\n                  fieldPath: metadata.namespace\n                path: namespace\n      - name: kong-custom-dbless-config-volume\n        configMap:\n          name: kong-dbless-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "562",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/020_deployment_release-name-kubernetes-dashboard-api.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-api\n    app.kubernetes.io/version: 1.13.0\n    app.kubernetes.io/component: api\n  annotations: null\n  name: release-name-kubernetes-dashboard-api\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-api\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-api\n        app.kubernetes.io/version: 1.13.0\n        app.kubernetes.io/component: api\n      annotations:\n        checksum/config: 204c1765e58ad5edfbba4d9e2caec1985db487314063c390ec4bb1957d34bc3d\n    spec:\n      containers:\n      - name: kubernetes-dashboard-api\n        image: docker.io/kubernetesui/dashboard-api:1.13.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --metrics-scraper-service-name=release-name-kubernetes-dashboard-metrics-scraper\n        env:\n        - name: CSRF_KEY\n          valueFrom:\n            secretKeyRef:\n              name: release-name-kubernetes-dashboard-csrf\n              key: private.key\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: api\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-api\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-api\" not found"
  },
  {
    "id": "563",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/022_deployment_release-name-kubernetes-dashboard-metrics-scraper.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n    app.kubernetes.io/version: 1.2.2\n    app.kubernetes.io/component: metrics-scraper\n  annotations: null\n  name: release-name-kubernetes-dashboard-metrics-scraper\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-metrics-scraper\n        app.kubernetes.io/version: 1.2.2\n        app.kubernetes.io/component: metrics-scraper\n      annotations: null\n    spec:\n      containers:\n      - name: kubernetes-dashboard-metrics-scraper\n        image: docker.io/kubernetesui/dashboard-metrics-scraper:1.2.2\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8000\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 30\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-metrics-scraper\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-metrics-scraper\" not found"
  },
  {
    "id": "564",
    "manifest_path": "data/manifests/artifacthub/k8s-dashboard/kubernetes-dashboard/023_deployment_release-name-kubernetes-dashboard-web.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  labels:\n    helm.sh/chart: kubernetes-dashboard-7.13.0\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: kubernetes-dashboard\n    app.kubernetes.io/name: kubernetes-dashboard-web\n    app.kubernetes.io/version: 1.7.0\n    app.kubernetes.io/component: web\n  annotations: null\n  name: release-name-kubernetes-dashboard-web\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: kubernetes-dashboard\n      app.kubernetes.io/name: kubernetes-dashboard-web\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kubernetes-dashboard-7.13.0\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: kubernetes-dashboard\n        app.kubernetes.io/name: kubernetes-dashboard-web\n        app.kubernetes.io/version: 1.7.0\n        app.kubernetes.io/component: web\n      annotations: null\n    spec:\n      containers:\n      - name: kubernetes-dashboard-web\n        image: docker.io/kubernetesui/dashboard-web:1.7.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=default\n        - --settings-config-map-name=release-name-kubernetes-dashboard-web-settings\n        env:\n        - name: GOMAXPROCS\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.cpu\n              divisor: '1'\n        - name: GOMEMLIMIT\n          valueFrom:\n            resourceFieldRef:\n              resource: limits.memory\n              divisor: '1'\n        ports:\n        - containerPort: 8000\n          name: web\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp\n          name: tmp-volume\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2001\n          runAsUser: 1001\n        resources:\n          limits:\n            cpu: 250m\n            memory: 400Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      automountServiceAccountToken: true\n      securityContext:\n        runAsNonRoot: true\n        seccompProfile:\n          type: RuntimeDefault\n      volumes:\n      - emptyDir: {}\n        name: tmp-volume\n      serviceAccountName: release-name-kubernetes-dashboard-web\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kubernetes-dashboard-web\" not found"
  },
  {
    "id": "565",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/055_service_release-name-kyverno-svc.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kyverno-svc\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 443\n    targetPort: https\n    protocol: TCP\n    name: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:admission-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "566",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/056_service_release-name-kyverno-svc-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kyverno-svc-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:admission-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "567",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/057_service_kyverno-background-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-background-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:background-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "568",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/058_service_kyverno-cleanup-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-cleanup-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 443\n    targetPort: https\n    protocol: TCP\n    name: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cleanup-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "569",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/059_service_kyverno-cleanup-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-cleanup-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:cleanup-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "570",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/060_service_kyverno-reports-controller-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyverno-reports-controller-metrics\n  namespace: default\n  labels:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  ports:\n  - port: 8000\n    targetPort: 8000\n    protocol: TCP\n    name: metrics-port\n  selector:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/part-of: release-name-kyverno\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:reports-controller app.kubernetes.io/instance:release-name app.kubernetes.io/part-of:release-name-kyverno])"
  },
  {
    "id": "571",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/061_deployment_kyverno-admission-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-admission-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: admission-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: admission-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: admission-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - admission-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kyverno-pre\n        image: reg.kyverno.io/kyverno/kyvernopre:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --loggingFormat=text\n        - --v=2\n        - --openreportsEnabled=false\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-admission-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:admission-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-admission-controller\n        - name: KYVERNO_SVC\n          value: release-name-kyverno-svc\n      containers:\n      - name: kyverno\n        image: reg.kyverno.io/kyverno/kyverno:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - --caSecretName=release-name-kyverno-svc.default.svc.kyverno-tls-ca\n        - --tlsSecretName=release-name-kyverno-svc.default.svc.kyverno-tls-pair\n        - --backgroundServiceAccountName=system:serviceaccount:default:kyverno-background-controller\n        - --reportsServiceAccountName=system:serviceaccount:default:kyverno-reports-controller\n        - --servicePort=443\n        - --webhookServerPort=9443\n        - --resyncPeriod=15m\n        - --crdWatcher=false\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --admissionReports=true\n        - --maxAdmissionReports=1000\n        - --autoUpdateWebhooks=true\n        - --enableConfigMapCaching=true\n        - --controllerRuntimeMetricsAddress=:8080\n        - --enableDeferredLoading=true\n        - --dumpPayload=false\n        - --forceFailurePolicyIgnore=false\n        - --generateValidatingAdmissionPolicy=true\n        - --generateMutatingAdmissionPolicy=false\n        - --dumpPatches=false\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --protectManagedResources=false\n        - --allowInsecureRegistry=false\n        - --registryCredentialHelpers=default,google,amazon,azure,github\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        resources:\n          limits:\n            memory: 384Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics-port\n          protocol: TCP\n        env:\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-admission-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:admission-controller\n        - name: KYVERNO_SVC\n          value: release-name-kyverno-svc\n        - name: TUF_ROOT\n          value: /.sigstore\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-admission-controller\n        startupProbe:\n          failureThreshold: 20\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 2\n          periodSeconds: 6\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "572",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/062_deployment_kyverno-background-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-background-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: background-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: background-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: background-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - background-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-background-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/background-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --resyncPeriod=15m\n        - --enableConfigMapCaching=true\n        - --enableDeferredLoading=true\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-background-controller\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-background-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"kyverno-background-controller\" not found"
  },
  {
    "id": "573",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/063_deployment_kyverno-cleanup-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-cleanup-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: cleanup-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: cleanup-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: cleanup-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - cleanup-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-cleanup-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/cleanup-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --caSecretName=kyverno-cleanup-controller.default.svc.kyverno-tls-ca\n        - --tlsSecretName=kyverno-cleanup-controller.default.svc.kyverno-tls-pair\n        - --servicePort=443\n        - --cleanupServerPort=9443\n        - --webhookServerPort=9443\n        - --resyncPeriod=15m\n        - --disableMetrics=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --enableDeferredLoading=true\n        - --dumpPayload=false\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --protectManagedResources=false\n        - --ttlReconciliationInterval=1m\n        env:\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-cleanup-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-cleanup-controller\n        - name: KYVERNO_ROLE_NAME\n          value: release-name-kyverno:cleanup-controller\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: KYVERNO_SVC\n          value: kyverno-cleanup-controller\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        startupProbe:\n          failureThreshold: 20\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 2\n          periodSeconds: 6\n        livenessProbe:\n          failureThreshold: 2\n          httpGet:\n            path: /health/liveness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 15\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 6\n          httpGet:\n            path: /health/readiness\n            port: 9443\n            scheme: HTTPS\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"kyverno-cleanup-controller\" not found"
  },
  {
    "id": "574",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/064_deployment_kyverno-reports-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kyverno-reports-controller\n  namespace: default\n  labels:\n    app.kubernetes.io/component: reports-controller\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\nspec:\n  replicas: null\n  revisionHistoryLimit: 10\n  strategy:\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 40%\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: reports-controller\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/part-of: release-name-kyverno\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: reports-controller\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/part-of: release-name-kyverno\n        app.kubernetes.io/version: 3.5.2\n        helm.sh/chart: kyverno-3.5.2\n    spec:\n      dnsPolicy: ClusterFirst\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app.kubernetes.io/component\n                  operator: In\n                  values:\n                  - reports-controller\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      serviceAccountName: kyverno-reports-controller\n      automountServiceAccountToken: true\n      containers:\n      - name: controller\n        image: reg.kyverno.io/kyverno/reports-controller:v1.15.2\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 9443\n          name: https\n          protocol: TCP\n        - containerPort: 8000\n          name: metrics\n          protocol: TCP\n        args:\n        - --disableMetrics=false\n        - --openreportsEnabled=false\n        - --otelConfig=prometheus\n        - --metricsPort=8000\n        - --resyncPeriod=15m\n        - --admissionReports=true\n        - --aggregateReports=true\n        - --policyReports=true\n        - --validatingAdmissionPolicyReports=true\n        - --mutatingAdmissionPolicyReports=false\n        - --backgroundScan=true\n        - --backgroundScanWorkers=2\n        - --backgroundScanInterval=1h\n        - --skipResourceFilters=true\n        - --enableConfigMapCaching=true\n        - --enableDeferredLoading=true\n        - --maxAPICallResponseLength=2000000\n        - --loggingFormat=text\n        - --v=2\n        - --omitEvents=PolicyApplied,PolicySkipped\n        - --enablePolicyException=false\n        - --allowInsecureRegistry=false\n        - --registryCredentialHelpers=default,google,amazon,azure,github\n        - --enableReporting=validate,mutate,mutateExisting,imageVerify,generate\n        env:\n        - name: KYVERNO_SERVICEACCOUNT_NAME\n          value: kyverno-reports-controller\n        - name: KYVERNO_DEPLOYMENT\n          value: kyverno-reports-controller\n        - name: INIT_CONFIG\n          value: release-name-kyverno\n        - name: METRICS_CONFIG\n          value: release-name-kyverno-metrics\n        - name: KYVERNO_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: KYVERNO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: TUF_ROOT\n          value: /.sigstore\n        resources:\n          limits:\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - mountPath: /.sigstore\n          name: sigstore\n      volumes:\n      - name: sigstore\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"kyverno-reports-controller\" not found"
  },
  {
    "id": "575",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/073_job_release-name-kyverno-migrate-resources.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-migrate-resources\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '200'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: release-name-kyverno-migrate-resources\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: reg.kyverno.io/kyverno/kyverno-cli:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - migrate\n        - --resource\n        - cleanuppolicies.kyverno.io\n        - --resource\n        - clustercleanuppolicies.kyverno.io\n        - --resource\n        - clusterpolicies.kyverno.io\n        - --resource\n        - globalcontextentries.kyverno.io\n        - --resource\n        - policies.kyverno.io\n        - --resource\n        - policyexceptions.kyverno.io\n        - --resource\n        - updaterequests.kyverno.io\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "576",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/073_job_release-name-kyverno-migrate-resources.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-migrate-resources\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '200'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: release-name-kyverno-migrate-resources\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: reg.kyverno.io/kyverno/kyverno-cli:v1.15.2\n        imagePullPolicy: IfNotPresent\n        args:\n        - migrate\n        - --resource\n        - cleanuppolicies.kyverno.io\n        - --resource\n        - clustercleanuppolicies.kyverno.io\n        - --resource\n        - clusterpolicies.kyverno.io\n        - --resource\n        - globalcontextentries.kyverno.io\n        - --resource\n        - policies.kyverno.io\n        - --resource\n        - policyexceptions.kyverno.io\n        - --resource\n        - updaterequests.kyverno.io\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kyverno-migrate-resources\" not found"
  },
  {
    "id": "577",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/074_job_release-name-kyverno-rm-mutatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-mutatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - mutatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "578",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/074_job_release-name-kyverno-rm-mutatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-mutatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - mutatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "579",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/075_job_release-name-kyverno-rm-validatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-validatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - validatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "580",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/075_job_release-name-kyverno-rm-validatingwhconfig.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-rm-validatingwhconfig\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '100'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - delete\n        - validatingwebhookconfiguration\n        - -l\n        - webhook.kyverno.io/managed-by=kyverno\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "581",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/076_job_release-name-kyverno-scale-to-zero.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-scale-to-zero\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '90'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - scale\n        - -n\n        - default\n        - deployment\n        - -l\n        - app.kubernetes.io/part-of=release-name-kyverno\n        - --replicas=0\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "582",
    "manifest_path": "data/manifests/artifacthub/kyverno/kyverno/076_job_release-name-kyverno-scale-to-zero.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kyverno-scale-to-zero\n  namespace: default\n  labels:\n    app.kubernetes.io/component: hooks\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/part-of: release-name-kyverno\n    app.kubernetes.io/version: 3.5.2\n    helm.sh/chart: kyverno-3.5.2\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded,hook-failed\n    helm.sh/hook-weight: '90'\nspec:\n  backoffLimit: 2\n  template:\n    spec:\n      serviceAccountName: kyverno-admission-controller\n      automountServiceAccountToken: true\n      restartPolicy: Never\n      containers:\n      - name: kubectl\n        image: registry.k8s.io/kubectl:v1.32.7\n        imagePullPolicy: null\n        command:\n        - kubectl\n        - scale\n        - -n\n        - default\n        - deployment\n        - -l\n        - app.kubernetes.io/part-of=release-name-kyverno\n        - --replicas=0\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 10m\n            memory: 64Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n          seccompProfile:\n            type: RuntimeDefault\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"kyverno-admission-controller\" not found"
  },
  {
    "id": "583",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/035_service_longhorn-backend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-backend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: longhorn-manager\n  ports:\n  - name: manager\n    port: 9500\n    targetPort: manager\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:longhorn-manager])"
  },
  {
    "id": "584",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/036_service_longhorn-frontend.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-frontend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    app: longhorn-ui\n  ports:\n  - name: http\n    port: 80\n    targetPort: http\n    nodePort: null\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:longhorn-ui])"
  },
  {
    "id": "585",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/037_service_longhorn-admission-webhook.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-admission-webhook\n  name: longhorn-admission-webhook\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    longhorn.io/admission-webhook: longhorn-admission-webhook\n  ports:\n  - name: admission-webhook\n    port: 9502\n    targetPort: admission-wh\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[longhorn.io/admission-webhook:longhorn-admission-webhook])"
  },
  {
    "id": "586",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/038_service_longhorn-recovery-backend.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-recovery-backend\n  name: longhorn-recovery-backend\n  namespace: default\nspec:\n  type: ClusterIP\n  selector:\n    longhorn.io/recovery-backend: longhorn-recovery-backend\n  ports:\n  - name: recovery-backend\n    port: 9503\n    targetPort: recov-backend\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[longhorn.io/recovery-backend:longhorn-recovery-backend])"
  },
  {
    "id": "587",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "588",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"pre-pull-share-manager-image\" does not have a read-only root file system"
  },
  {
    "id": "589",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "590",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"longhorn-manager\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "591",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "no_privileged",
    "violation_text": "container \"longhorn-manager\" is privileged"
  },
  {
    "id": "592",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"longhorn-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "593",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"pre-pull-share-manager-image\" is not set to runAsNonRoot"
  },
  {
    "id": "594",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-manager\" has cpu request 0"
  },
  {
    "id": "595",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pre-pull-share-manager-image\" has cpu request 0"
  },
  {
    "id": "596",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-manager\" has memory limit 0"
  },
  {
    "id": "597",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/039_daemonset_longhorn-manager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-manager\n  name: longhorn-manager\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: longhorn-manager\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-manager\n    spec:\n      containers:\n      - name: longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - -d\n        - daemon\n        - --engine-image\n        - longhornio/longhorn-engine:v1.10.0\n        - --instance-manager-image\n        - longhornio/longhorn-instance-manager:v1.10.0\n        - --share-manager-image\n        - longhornio/longhorn-share-manager:v1.10.0\n        - --backing-image-manager-image\n        - longhornio/backing-image-manager:v1.10.0\n        - --support-bundle-manager-image\n        - longhornio/support-bundle-kit:v0.0.69\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --service-account\n        - longhorn-service-account\n        - --upgrade-version-check\n        ports:\n        - containerPort: 9500\n          name: manager\n        - containerPort: 9502\n          name: admission-wh\n        - containerPort: 9503\n          name: recov-backend\n        readinessProbe:\n          httpGet:\n            path: /v1/healthz\n            port: 9502\n            scheme: HTTPS\n        volumeMounts:\n        - name: boot\n          mountPath: /host/boot/\n          readOnly: true\n        - name: dev\n          mountPath: /host/dev/\n        - name: proc\n          mountPath: /host/proc/\n          readOnly: true\n        - name: etc\n          mountPath: /host/etc/\n          readOnly: true\n        - name: longhorn\n          mountPath: /var/lib/longhorn/\n          mountPropagation: Bidirectional\n        - name: longhorn-grpc-tls\n          mountPath: /tls-files/\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n      - name: pre-pull-share-manager-image\n        imagePullPolicy: IfNotPresent\n        image: longhornio/longhorn-share-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - echo share-manager image pulled && sleep infinity\n      volumes:\n      - name: boot\n        hostPath:\n          path: /boot/\n      - name: dev\n        hostPath:\n          path: /dev/\n      - name: proc\n        hostPath:\n          path: /proc/\n      - name: etc\n        hostPath:\n          path: /etc/\n      - name: longhorn\n        hostPath:\n          path: /var/lib/longhorn/\n      - name: longhorn-grpc-tls\n        secret:\n          secretName: longhorn-grpc-tls\n          optional: true\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 100%\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pre-pull-share-manager-image\" has memory limit 0"
  },
  {
    "id": "598",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"longhorn-driver-deployer\" does not have a read-only root file system"
  },
  {
    "id": "599",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wait-longhorn-manager\" does not have a read-only root file system"
  },
  {
    "id": "600",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "601",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"longhorn-driver-deployer\" is not set to runAsNonRoot"
  },
  {
    "id": "602",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"wait-longhorn-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "603",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-driver-deployer\" has cpu request 0"
  },
  {
    "id": "604",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-longhorn-manager\" has cpu request 0"
  },
  {
    "id": "605",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-driver-deployer\" has memory limit 0"
  },
  {
    "id": "606",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/040_deployment_longhorn-driver-deployer.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: longhorn-driver-deployer\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: longhorn-driver-deployer\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-driver-deployer\n    spec:\n      initContainers:\n      - name: wait-longhorn-manager\n        image: longhornio/longhorn-manager:v1.10.0\n        command:\n        - sh\n        - -c\n        - while [ $(curl -m 1 -s -o /dev/null -w \"%{http_code}\" http://longhorn-backend:9500/v1)\n          != \"200\" ]; do echo waiting; sleep 2; done\n      containers:\n      - name: longhorn-driver-deployer\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - -d\n        - deploy-driver\n        - --manager-image\n        - longhornio/longhorn-manager:v1.10.0\n        - --manager-url\n        - http://longhorn-backend:9500/v1\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: SERVICE_ACCOUNT\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.serviceAccountName\n        - name: CSI_ATTACHER_IMAGE\n          value: longhornio/csi-attacher:v4.9.0-20250826\n        - name: CSI_PROVISIONER_IMAGE\n          value: longhornio/csi-provisioner:v5.3.0-20250826\n        - name: CSI_NODE_DRIVER_REGISTRAR_IMAGE\n          value: longhornio/csi-node-driver-registrar:v2.14.0-20250826\n        - name: CSI_RESIZER_IMAGE\n          value: longhornio/csi-resizer:v1.14.0-20250826\n        - name: CSI_SNAPSHOTTER_IMAGE\n          value: longhornio/csi-snapshotter:v8.3.0-20250826\n        - name: CSI_LIVENESS_PROBE_IMAGE\n          value: longhornio/livenessprobe:v2.16.0-20250826\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n      securityContext:\n        runAsUser: 0\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait-longhorn-manager\" has memory limit 0"
  },
  {
    "id": "607",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"longhorn-ui\" does not have a read-only root file system"
  },
  {
    "id": "608",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"longhorn-ui-service-account\" not found"
  },
  {
    "id": "609",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"longhorn-ui\" is not set to runAsNonRoot"
  },
  {
    "id": "610",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-ui\" has cpu request 0"
  },
  {
    "id": "611",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/041_deployment_longhorn-ui.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\n    app: longhorn-ui\n  name: longhorn-ui\n  namespace: default\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: longhorn-ui\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n        app: longhorn-ui\n    spec:\n      serviceAccountName: longhorn-ui-service-account\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - key: app\n                  operator: In\n                  values:\n                  - longhorn-ui\n              topologyKey: kubernetes.io/hostname\n            weight: 1\n      containers:\n      - name: longhorn-ui\n        image: longhornio/longhorn-ui:v1.10.0\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: nginx-cache\n          mountPath: /var/cache/nginx/\n        - name: nginx-config\n          mountPath: /var/config/nginx/\n        - name: var-run\n          mountPath: /var/run/\n        ports:\n        - containerPort: 8000\n          name: http\n        env:\n        - name: LONGHORN_MANAGER_IP\n          value: http://longhorn-backend:9500\n        - name: LONGHORN_UI_PORT\n          value: '8000'\n      volumes:\n      - emptyDir: {}\n        name: nginx-cache\n      - emptyDir: {}\n        name: nginx-config\n      - emptyDir: {}\n        name: var-run\n      priorityClassName: longhorn-critical\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-ui\" has memory limit 0"
  },
  {
    "id": "612",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "613",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"longhorn-post-upgrade\" does not have a read-only root file system"
  },
  {
    "id": "614",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "615",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"longhorn-post-upgrade\" is not set to runAsNonRoot"
  },
  {
    "id": "616",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-post-upgrade\" has cpu request 0"
  },
  {
    "id": "617",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/042_job_longhorn-post-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: post-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation\n  name: longhorn-post-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-post-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-post-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - post-upgrade\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: OnFailure\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-post-upgrade\" has memory limit 0"
  },
  {
    "id": "618",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "619",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"longhorn-pre-upgrade\" does not have a read-only root file system"
  },
  {
    "id": "620",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "621",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"longhorn-pre-upgrade\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "622",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "no_privileged",
    "violation_text": "container \"longhorn-pre-upgrade\" is privileged"
  },
  {
    "id": "623",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"longhorn-pre-upgrade\" is not set to runAsNonRoot"
  },
  {
    "id": "624",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-pre-upgrade\" has cpu request 0"
  },
  {
    "id": "625",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/043_job_longhorn-pre-upgrade.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-upgrade\n    helm.sh/hook-delete-policy: hook-succeeded,before-hook-creation,hook-failed\n  name: longhorn-pre-upgrade\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-pre-upgrade\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-pre-upgrade\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n        command:\n        - longhorn-manager\n        - pre-upgrade\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc/\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc/\n      restartPolicy: OnFailure\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-pre-upgrade\" has memory limit 0"
  },
  {
    "id": "626",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "627",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"longhorn-uninstall\" does not have a read-only root file system"
  },
  {
    "id": "628",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"longhorn-service-account\" not found"
  },
  {
    "id": "629",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"longhorn-uninstall\" is not set to runAsNonRoot"
  },
  {
    "id": "630",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-uninstall\" has cpu request 0"
  },
  {
    "id": "631",
    "manifest_path": "data/manifests/artifacthub/longhorn/longhorn/044_job_longhorn-uninstall.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  annotations:\n    helm.sh/hook: pre-delete\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  name: longhorn-uninstall\n  namespace: default\n  labels:\n    app.kubernetes.io/name: longhorn\n    helm.sh/chart: longhorn-1.10.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.10.0\nspec:\n  activeDeadlineSeconds: 900\n  backoffLimit: 1\n  template:\n    metadata:\n      name: longhorn-uninstall\n      labels:\n        app.kubernetes.io/name: longhorn\n        helm.sh/chart: longhorn-1.10.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.10.0\n    spec:\n      containers:\n      - name: longhorn-uninstall\n        image: longhornio/longhorn-manager:v1.10.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - longhorn-manager\n        - uninstall\n        - --force\n        env:\n        - name: LONGHORN_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n      restartPolicy: Never\n      priorityClassName: longhorn-critical\n      serviceAccountName: longhorn-service-account\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"longhorn-uninstall\" has memory limit 0"
  },
  {
    "id": "632",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/022_service_metallb-webhook-service.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: metallb-webhook-service\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\nspec:\n  ports:\n  - port: 443\n    targetPort: 9443\n  selector:\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: controller\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:controller app.kubernetes.io/instance:release-name app.kubernetes.io/name:metallb])"
  },
  {
    "id": "633",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"frr\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "634",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"frr\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "635",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"speaker\" has ADD capability: \"NET_RAW\", which matched with the forbidden capability for containers"
  },
  {
    "id": "637",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "no_host_network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "639",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"cp-frr-files\" does not have a read-only root file system"
  },
  {
    "id": "640",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"cp-metrics\" does not have a read-only root file system"
  },
  {
    "id": "641",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"cp-reloader\" does not have a read-only root file system"
  },
  {
    "id": "642",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"frr\" does not have a read-only root file system"
  },
  {
    "id": "643",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"frr-metrics\" does not have a read-only root file system"
  },
  {
    "id": "644",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"reloader\" does not have a read-only root file system"
  },
  {
    "id": "645",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-metallb-speaker\" not found"
  },
  {
    "id": "646",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"frr\" has SYS_ADMIN capability hence allows privilege escalation."
  },
  {
    "id": "647",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"cp-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "648",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"cp-reloader\" is not set to runAsNonRoot"
  },
  {
    "id": "649",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"frr\" is not set to runAsNonRoot"
  },
  {
    "id": "650",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"frr-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "651",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"reloader\" is not set to runAsNonRoot"
  },
  {
    "id": "652",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"speaker\" is not set to runAsNonRoot"
  },
  {
    "id": "654",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cp-frr-files\" has cpu request 0"
  },
  {
    "id": "655",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cp-metrics\" has cpu request 0"
  },
  {
    "id": "656",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cp-reloader\" has cpu request 0"
  },
  {
    "id": "657",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"frr\" has cpu request 0"
  },
  {
    "id": "658",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"frr-metrics\" has cpu request 0"
  },
  {
    "id": "659",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"reloader\" has cpu request 0"
  },
  {
    "id": "660",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"speaker\" has cpu request 0"
  },
  {
    "id": "661",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cp-frr-files\" has memory limit 0"
  },
  {
    "id": "662",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cp-metrics\" has memory limit 0"
  },
  {
    "id": "663",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cp-reloader\" has memory limit 0"
  },
  {
    "id": "664",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"frr\" has memory limit 0"
  },
  {
    "id": "665",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"frr-metrics\" has memory limit 0"
  },
  {
    "id": "666",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"reloader\" has memory limit 0"
  },
  {
    "id": "667",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/023_daemonset_release-name-metallb-speaker.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-metallb-speaker\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: speaker\nspec:\n  updateStrategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: speaker\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: speaker\n    spec:\n      serviceAccountName: release-name-metallb-speaker\n      terminationGracePeriodSeconds: 0\n      hostNetwork: true\n      volumes:\n      - name: memberlist\n        secret:\n          secretName: release-name-metallb-memberlist\n          defaultMode: 420\n      - name: metallb-excludel2\n        configMap:\n          defaultMode: 256\n          name: metallb-excludel2\n      - name: frr-sockets\n        emptyDir: {}\n      - name: frr-startup\n        configMap:\n          name: release-name-metallb-frr-startup\n      - name: frr-conf\n        emptyDir: {}\n      - name: reloader\n        emptyDir: {}\n      - name: metrics\n        emptyDir: {}\n      initContainers:\n      - name: cp-frr-files\n        image: quay.io/frrouting/frr:9.1.0\n        securityContext:\n          runAsUser: 100\n          runAsGroup: 101\n        command:\n        - /bin/sh\n        - -c\n        - cp -rLf /tmp/frr/* /etc/frr/\n        volumeMounts:\n        - name: frr-startup\n          mountPath: /tmp/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n      - name: cp-reloader\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-reloader.sh\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: cp-metrics\n        image: quay.io/metallb/speaker:v0.15.2\n        command:\n        - /cp-tool\n        - /frr-metrics\n        - /etc/frr_metrics/frr-metrics\n        volumeMounts:\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      shareProcessNamespace: true\n      containers:\n      - name: speaker\n        image: quay.io/metallb/speaker:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        env:\n        - name: METALLB_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: METALLB_HOST\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: METALLB_ML_BIND_ADDR\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: METALLB_ML_LABELS\n          value: app.kubernetes.io/name=metallb,app.kubernetes.io/component=speaker\n        - name: METALLB_ML_BIND_PORT\n          value: '7946'\n        - name: METALLB_ML_SECRET_KEY_PATH\n          value: /etc/ml_secret_key\n        - name: FRR_CONFIG_FILE\n          value: /etc/frr_reloader/frr.conf\n        - name: FRR_RELOADER_PID_FILE\n          value: /etc/frr_reloader/reloader.pid\n        - name: METALLB_BGP_TYPE\n          value: frr\n        - name: METALLB_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - name: memberlist-tcp\n          containerPort: 7946\n          protocol: TCP\n        - name: memberlist-udp\n          containerPort: 7946\n          protocol: UDP\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_RAW\n        volumeMounts:\n        - name: memberlist\n          mountPath: /etc/ml_secret_key\n        - name: reloader\n          mountPath: /etc/frr_reloader\n        - name: metallb-excludel2\n          mountPath: /etc/metallb\n      - name: frr\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - NET_RAW\n            - SYS_ADMIN\n            - NET_BIND_SERVICE\n        image: quay.io/frrouting/frr:9.1.0\n        env:\n        - name: TINI_SUBREAPER\n          value: 'true'\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        command:\n        - /bin/sh\n        - -c\n        - \"/sbin/tini -- /usr/lib/frr/docker-start &\\nattempts=0\\nuntil [[ -f /etc/frr/frr.log\\\n          \\ || $attempts -eq 60 ]]; do\\n  sleep 1\\n  attempts=$(( $attempts + 1 ))\\n\\\n          done\\ntail -f /etc/frr/frr.log\\n\"\n        livenessProbe:\n          httpGet:\n            path: livez\n            port: 7473\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        startupProbe:\n          httpGet:\n            path: /livez\n            port: 7473\n          failureThreshold: 30\n          periodSeconds: 5\n      - name: reloader\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_reloader/frr-reloader.sh\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: reloader\n          mountPath: /etc/frr_reloader\n      - name: frr-metrics\n        image: quay.io/frrouting/frr:9.1.0\n        command:\n        - /etc/frr_metrics/frr-metrics\n        args:\n        - --metrics-port=7473\n        env:\n        - name: VTYSH_HISTFILE\n          value: /dev/null\n        ports:\n        - containerPort: 7473\n          name: monitoring\n        volumeMounts:\n        - name: frr-sockets\n          mountPath: /var/run/frr\n        - name: frr-conf\n          mountPath: /etc/frr\n        - name: metrics\n          mountPath: /etc/frr_metrics\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n        operator: Exists\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n        operator: Exists\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"speaker\" has memory limit 0"
  },
  {
    "id": "669",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/024_deployment_release-name-metallb-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: release-name-metallb-controller\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-metallb-controller\" not found"
  },
  {
    "id": "670",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/024_deployment_release-name-metallb-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: release-name-metallb-controller\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"controller\" has cpu request 0"
  },
  {
    "id": "671",
    "manifest_path": "data/manifests/artifacthub/metallb/metallb/024_deployment_release-name-metallb-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metallb-controller\n  namespace: default\n  labels:\n    helm.sh/chart: metallb-0.15.2\n    app.kubernetes.io/name: metallb\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.15.2\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: controller\nspec:\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metallb\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: controller\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metallb\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: controller\n    spec:\n      serviceAccountName: release-name-metallb-controller\n      terminationGracePeriodSeconds: 0\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: controller\n        image: quay.io/metallb/controller:v0.15.2\n        args:\n        - --port=7472\n        - --log-level=info\n        - --tls-min-version=VersionTLS12\n        env:\n        - name: METALLB_ML_SECRET_NAME\n          value: release-name-metallb-memberlist\n        - name: METALLB_DEPLOYMENT\n          value: release-name-metallb-controller\n        - name: METALLB_BGP_TYPE\n          value: frr\n        ports:\n        - name: monitoring\n          containerPort: 7472\n        - containerPort: 9443\n          name: webhook-server\n          protocol: TCP\n        volumeMounts:\n        - mountPath: /tmp/k8s-webhook-server/serving-certs\n          name: cert\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /metrics\n            port: monitoring\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n          capabilities:\n            drop:\n            - ALL\n      nodeSelector:\n        kubernetes.io/os: linux\n      volumes:\n      - name: cert\n        secret:\n          defaultMode: 420\n          secretName: metallb-webhook-cert\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"controller\" has memory limit 0"
  },
  {
    "id": "672",
    "manifest_path": "data/manifests/artifacthub/metrics-server/metrics-server/007_service_release-name-metrics-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  type: ClusterIP\n  ports:\n  - name: https\n    port: 443\n    protocol: TCP\n    targetPort: https\n    appProtocol: https\n  selector:\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:metrics-server])"
  },
  {
    "id": "673",
    "manifest_path": "data/manifests/artifacthub/metrics-server/metrics-server/008_deployment_release-name-metrics-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metrics-server\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: release-name-metrics-server\n      priorityClassName: system-cluster-critical\n      containers:\n      - name: metrics-server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --secure-port=10250\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /livez\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-metrics-server\" not found"
  },
  {
    "id": "674",
    "manifest_path": "data/manifests/artifacthub/metrics-server/metrics-server/008_deployment_release-name-metrics-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-metrics-server\n  namespace: default\n  labels:\n    helm.sh/chart: metrics-server-3.13.0\n    app.kubernetes.io/name: metrics-server\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 0.8.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: metrics-server\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: metrics-server\n        app.kubernetes.io/instance: release-name\n    spec:\n      serviceAccountName: release-name-metrics-server\n      priorityClassName: system-cluster-critical\n      containers:\n      - name: metrics-server\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n          runAsUser: 1000\n          seccompProfile:\n            type: RuntimeDefault\n        image: registry.k8s.io/metrics-server/metrics-server:v0.8.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --secure-port=10250\n        - --cert-dir=/tmp\n        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n        - --kubelet-use-node-status-port\n        - --metric-resolution=15s\n        ports:\n        - name: https\n          protocol: TCP\n          containerPort: 10250\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /livez\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /readyz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 20\n          periodSeconds: 10\n        volumeMounts:\n        - name: tmp\n          mountPath: /tmp\n        resources:\n          requests:\n            cpu: 100m\n            memory: 200Mi\n      volumes:\n      - name: tmp\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"metrics-server\" has memory limit 0"
  },
  {
    "id": "675",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/002_service_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8080\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:app app.kubernetes.io/instance:release-name app.kubernetes.io/name:nextcloud])"
  },
  {
    "id": "676",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/003_deployment_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nextcloud\" does not have a read-only root file system"
  },
  {
    "id": "677",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/003_deployment_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nextcloud\" is not set to runAsNonRoot"
  },
  {
    "id": "678",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/003_deployment_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nextcloud\" has cpu request 0"
  },
  {
    "id": "679",
    "manifest_path": "data/manifests/artifacthub/nextcloud/nextcloud/003_deployment_release-name-nextcloud.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nextcloud\n  labels:\n    app.kubernetes.io/name: nextcloud\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/component: app\n    helm.sh/chart: nextcloud-8.4.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 32.0.0\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nextcloud\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/component: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nextcloud\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/component: app\n      annotations:\n        nextcloud-config-hash: 8266a725d5474acb6adbf9f0609a3494dc3340a3ac306db90eac9ddb1b851960\n        php-config-hash: 44136fa355b3678a1146ad16f7e8649e94fb4fc21fe77e8310c060f61caaff8a\n        hooks-hash: 9525c2748a6c7cd0e28ec740623d0b3fa5a75c83b51ccfd136bc89c76737b204\n    spec:\n      containers:\n      - name: nextcloud\n        image: nextcloud:32.0.0-apache\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: SQLITE_DATABASE\n          value: nextcloud\n        - name: NEXTCLOUD_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-username\n        - name: NEXTCLOUD_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-nextcloud\n              key: nextcloud-password\n        - name: NEXTCLOUD_TRUSTED_DOMAINS\n          value: nextcloud.kube.home\n        - name: NEXTCLOUD_DATA_DIR\n          value: /var/www/html/data\n        resources: {}\n        volumeMounts:\n        - name: nextcloud-main\n          mountPath: /var/www/\n          subPath: root\n        - name: nextcloud-main\n          mountPath: /var/www/html\n          subPath: html\n        - name: nextcloud-main\n          mountPath: /var/www/html/data\n          subPath: data\n        - name: nextcloud-main\n          mountPath: /var/www/html/config\n          subPath: config\n        - name: nextcloud-main\n          mountPath: /var/www/html/custom_apps\n          subPath: custom_apps\n        - name: nextcloud-main\n          mountPath: /var/www/tmp\n          subPath: tmp\n        - name: nextcloud-main\n          mountPath: /var/www/html/themes\n          subPath: themes\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /status.php\n            port: 80\n            httpHeaders:\n            - name: Host\n              value: nextcloud.kube.home\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 5\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: nextcloud-main\n        emptyDir: {}\n      securityContext:\n        fsGroup: 33\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nextcloud\" has memory limit 0"
  },
  {
    "id": "680",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nfs-subdir-external-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "681",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-nfs-subdir-external-provisioner\" not found"
  },
  {
    "id": "682",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nfs-subdir-external-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "683",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nfs-subdir-external-provisioner\" has cpu request 0"
  },
  {
    "id": "684",
    "manifest_path": "data/manifests/artifacthub/nfs-subdir-external-provisioner/nfs-subdir-external-provisioner/007_deployment_release-name-nfs-subdir-external-provisioner.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nfs-subdir-external-provisioner\n  labels:\n    chart: nfs-subdir-external-provisioner-4.0.18\n    heritage: Helm\n    app: nfs-subdir-external-provisioner\n    release: release-name\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: nfs-subdir-external-provisioner\n      release: release-name\n  template:\n    metadata:\n      annotations: null\n      labels:\n        app: nfs-subdir-external-provisioner\n        release: release-name\n    spec:\n      serviceAccountName: release-name-nfs-subdir-external-provisioner\n      securityContext: {}\n      containers:\n      - name: nfs-subdir-external-provisioner\n        image: registry.k8s.io/sig-storage/nfs-subdir-external-provisioner:v4.0.2\n        imagePullPolicy: IfNotPresent\n        securityContext: {}\n        volumeMounts:\n        - name: nfs-subdir-external-provisioner-root\n          mountPath: /persistentvolumes\n        env:\n        - name: PROVISIONER_NAME\n          value: cluster.local/release-name-nfs-subdir-external-provisioner\n        - name: NFS_SERVER\n          value: null\n        - name: NFS_PATH\n          value: /nfs-storage\n      volumes:\n      - name: nfs-subdir-external-provisioner-root\n        nfs:\n          server: null\n          path: /nfs-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nfs-subdir-external-provisioner\" has memory limit 0"
  },
  {
    "id": "685",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/008_service_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n    nodePort: null\n  - port: 443\n    targetPort: 443\n    protocol: TCP\n    name: https\n    nodePort: null\n  selector:\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:nginx-ingress])"
  },
  {
    "id": "686",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx-ingress\" does not have a read-only root file system"
  },
  {
    "id": "687",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-nginx-ingress\" not found"
  },
  {
    "id": "688",
    "manifest_path": "data/manifests/artifacthub/nginx/nginx-ingress/009_deployment_release-name-nginx-ingress-controller.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-nginx-ingress-controller\n  namespace: default\n  labels:\n    helm.sh/chart: nginx-ingress-2.3.0\n    app.kubernetes.io/name: nginx-ingress\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 5.2.0\n    app.kubernetes.io/managed-by: Helm\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: nginx-ingress\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: nginx-ingress\n        app.kubernetes.io/instance: release-name\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9113'\n        prometheus.io/scheme: http\n    spec:\n      volumes: []\n      serviceAccountName: release-name-nginx-ingress\n      automountServiceAccountToken: true\n      securityContext:\n        seccompProfile:\n          type: RuntimeDefault\n      terminationGracePeriodSeconds: 30\n      hostNetwork: false\n      dnsPolicy: ClusterFirst\n      containers:\n      - image: nginx/nginx-ingress:5.2.0\n        name: nginx-ingress\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        - name: https\n          containerPort: 443\n          protocol: TCP\n        - name: prometheus\n          containerPort: 9113\n        - name: readiness-port\n          containerPort: 8081\n        readinessProbe:\n          httpGet:\n            path: /nginx-ready\n            port: readiness-port\n          periodSeconds: 1\n          initialDelaySeconds: 0\n        resources:\n          requests:\n            cpu: 100m\n            memory: 128Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: false\n          runAsUser: 101\n          runAsNonRoot: true\n          capabilities:\n            drop:\n            - ALL\n            add:\n            - NET_BIND_SERVICE\n        volumeMounts: []\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        args:\n        - -nginx-plus=false\n        - -nginx-reload-timeout=60000\n        - -enable-app-protect=false\n        - -enable-app-protect-dos=false\n        - -nginx-configmaps=$(POD_NAMESPACE)/release-name-nginx-ingress\n        - -ingress-class=nginx\n        - -health-status=false\n        - -health-status-uri=/nginx-health\n        - -nginx-debug=false\n        - -log-level=info\n        - -log-format=glog\n        - -nginx-status=true\n        - -nginx-status-port=8080\n        - -nginx-status-allow-cidrs=127.0.0.1\n        - -report-ingress-status\n        - -external-service=release-name-nginx-ingress-controller\n        - -enable-leader-election=true\n        - -leader-election-lock-name=release-name-nginx-ingress-leader-election\n        - -enable-prometheus-metrics=true\n        - -prometheus-metrics-listen-port=9113\n        - -prometheus-tls-secret=\n        - -enable-service-insight=false\n        - -service-insight-listen-port=9114\n        - -service-insight-tls-secret=\n        - -enable-custom-resources=true\n        - -enable-snippets=false\n        - -disable-ipv6=false\n        - -enable-tls-passthrough=false\n        - -enable-cert-manager=false\n        - -enable-oidc=false\n        - -enable-external-dns=false\n        - -default-http-listener-port=80\n        - -default-https-listener-port=443\n        - -ready-status=true\n        - -ready-status-port=8081\n        - -enable-latency-metrics=false\n        - -ssl-dynamic-reload=true\n        - -enable-telemetry-reporting=true\n        - -weight-changes-dynamic-reload=false\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx-ingress\" has memory limit 0"
  },
  {
    "id": "689",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/004_service_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: http\n    protocol: TCP\n    appProtocol: http\n    name: http\n  - port: 44180\n    protocol: TCP\n    appProtocol: http\n    targetPort: metrics\n    name: metrics\n  selector:\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:oauth2-proxy])"
  },
  {
    "id": "690",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-oauth2-proxy\" not found"
  },
  {
    "id": "691",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"oauth2-proxy\" has cpu request 0"
  },
  {
    "id": "692",
    "manifest_path": "data/manifests/artifacthub/oauth2-proxy/oauth2-proxy/005_deployment_release-name-oauth2-proxy.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: oauth2-proxy\n    helm.sh/chart: oauth2-proxy-8.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: authentication-proxy\n    app.kubernetes.io/part-of: oauth2-proxy\n    app.kubernetes.io/name: oauth2-proxy\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 7.12.0\n  name: release-name-oauth2-proxy\n  namespace: default\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: oauth2-proxy\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      annotations:\n        checksum/config: c0329892592df8b1519fac51e84aee8cf879bb8e157e5a04f6556b38b5a2435b\n        checksum/secret: 8fa6fdae65861caa2986544b8860a5205be1937328c8ec2bad6bad076b9e2425\n        checksum/google-secret: e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855\n        checksum/redis-secret: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b\n      labels:\n        app: oauth2-proxy\n        helm.sh/chart: oauth2-proxy-8.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: authentication-proxy\n        app.kubernetes.io/part-of: oauth2-proxy\n        app.kubernetes.io/name: oauth2-proxy\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 7.12.0\n    spec:\n      serviceAccountName: release-name-oauth2-proxy\n      enableServiceLinks: true\n      automountServiceAccountToken: true\n      containers:\n      - name: oauth2-proxy\n        image: quay.io/oauth2-proxy/oauth2-proxy:v7.12.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --http-address=0.0.0.0:4180\n        - --https-address=0.0.0.0:4443\n        - --metrics-address=0.0.0.0:44180\n        - --config=/etc/oauth2_proxy/oauth2_proxy.cfg\n        env:\n        - name: OAUTH2_PROXY_CLIENT_ID\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-id\n        - name: OAUTH2_PROXY_CLIENT_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: client-secret\n        - name: OAUTH2_PROXY_COOKIE_SECRET\n          valueFrom:\n            secretKeyRef:\n              name: release-name-oauth2-proxy\n              key: cookie-secret\n        ports:\n        - containerPort: 4180\n          name: http\n          protocol: TCP\n        - containerPort: 44180\n          protocol: TCP\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: http\n            scheme: HTTP\n          initialDelaySeconds: 0\n          timeoutSeconds: 5\n          successThreshold: 1\n          periodSeconds: 10\n        resources: {}\n        volumeMounts:\n        - mountPath: /etc/oauth2_proxy/oauth2_proxy.cfg\n          name: configmain\n          subPath: oauth2_proxy.cfg\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n          runAsGroup: 2000\n          runAsNonRoot: true\n          runAsUser: 2000\n          seccompProfile:\n            type: RuntimeDefault\n      volumes:\n      - configMap:\n          defaultMode: 420\n          name: release-name-oauth2-proxy\n        name: configmain\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"oauth2-proxy\" has memory limit 0"
  },
  {
    "id": "693",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/050_service_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  type: ClusterIP\n  ports:\n  - name: http-web\n    port: 80\n    protocol: TCP\n    targetPort: grafana\n  selector:\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:grafana])"
  },
  {
    "id": "694",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/051_service_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\n  annotations: null\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    protocol: TCP\n    port: 8080\n    targetPort: http\n  selector:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kube-state-metrics])"
  },
  {
    "id": "695",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/052_service_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\n    jobLabel: node-exporter\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9100\n    targetPort: 9100\n    protocol: TCP\n    name: http-metrics\n  selector:\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus-node-exporter])"
  },
  {
    "id": "696",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/053_service_release-name-kube-promethe-alertmanager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-alertmanager\n  namespace: default\n  labels:\n    app: kube-prometheus-stack-alertmanager\n    self-monitor: 'true'\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\nspec:\n  ports:\n  - name: http-web\n    port: 9093\n    targetPort: 9093\n    protocol: TCP\n  - name: reloader-web\n    appProtocol: http\n    port: 8080\n    targetPort: reloader-web\n  selector:\n    app.kubernetes.io/name: alertmanager\n    alertmanager: release-name-kube-promethe-alertmanager\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[alertmanager:release-name-kube-promethe-alertmanager app.kubernetes.io/name:alertmanager])"
  },
  {
    "id": "697",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/054_service_release-name-kube-promethe-coredns.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-coredns\n  labels:\n    app: kube-prometheus-stack-coredns\n    jobLabel: coredns\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 9153\n    protocol: TCP\n    targetPort: 9153\n  selector:\n    k8s-app: kube-dns\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-dns])"
  },
  {
    "id": "698",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/055_service_release-name-kube-promethe-kube-controller-manager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-controller-manager\n  labels:\n    app: kube-prometheus-stack-kube-controller-manager\n    jobLabel: kube-controller-manager\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10257\n    protocol: TCP\n    targetPort: 10257\n  selector:\n    component: kube-controller-manager\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:kube-controller-manager])"
  },
  {
    "id": "699",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/056_service_release-name-kube-promethe-kube-etcd.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-etcd\n  labels:\n    app: kube-prometheus-stack-kube-etcd\n    jobLabel: kube-etcd\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 2381\n    protocol: TCP\n    targetPort: 2381\n  selector:\n    component: etcd\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:etcd])"
  },
  {
    "id": "700",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/057_service_release-name-kube-promethe-kube-proxy.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-proxy\n  labels:\n    app: kube-prometheus-stack-kube-proxy\n    jobLabel: kube-proxy\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10249\n    protocol: TCP\n    targetPort: 10249\n  selector:\n    k8s-app: kube-proxy\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[k8s-app:kube-proxy])"
  },
  {
    "id": "701",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/058_service_release-name-kube-promethe-kube-scheduler.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-kube-scheduler\n  labels:\n    app: kube-prometheus-stack-kube-scheduler\n    jobLabel: kube-scheduler\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n  namespace: kube-system\nspec:\n  clusterIP: None\n  ports:\n  - name: http-metrics\n    port: 10259\n    protocol: TCP\n    targetPort: 10259\n  selector:\n    component: kube-scheduler\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:kube-scheduler])"
  },
  {
    "id": "702",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/059_service_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  ports:\n  - name: https\n    port: 443\n    targetPort: https\n  selector:\n    app: kube-prometheus-stack-operator\n    release: release-name\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:kube-prometheus-stack-operator release:release-name])"
  },
  {
    "id": "703",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/060_service_release-name-kube-promethe-prometheus.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-promethe-prometheus\n  namespace: default\n  labels:\n    app: kube-prometheus-stack-prometheus\n    self-monitor: 'true'\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\nspec:\n  ports:\n  - name: http-web\n    port: 9090\n    targetPort: 9090\n  - name: reloader-web\n    appProtocol: http\n    port: 8080\n    targetPort: reloader-web\n  publishNotReadyAddresses: false\n  selector:\n    app.kubernetes.io/name: prometheus\n    operator.prometheus.io/name: release-name-kube-promethe-prometheus\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:prometheus operator.prometheus.io/name:release-name-kube-promethe-prometheus])"
  },
  {
    "id": "704",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "no_host_network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "705",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "no_host_pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "706",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-prometheus-node-exporter\" not found"
  },
  {
    "id": "710",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "711",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/061_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n        release: release-name\n        jobLabel: node-exporter\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)\n        - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs|erofs)$\n        securityContext:\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: http-metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: http-metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "712",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"grafana\" does not have a read-only root file system"
  },
  {
    "id": "713",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"grafana-sc-dashboard\" does not have a read-only root file system"
  },
  {
    "id": "714",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"grafana-sc-datasources\" does not have a read-only root file system"
  },
  {
    "id": "715",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-grafana\" not found"
  },
  {
    "id": "716",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"grafana\" has cpu request 0"
  },
  {
    "id": "717",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"grafana-sc-dashboard\" has cpu request 0"
  },
  {
    "id": "718",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"grafana-sc-datasources\" has cpu request 0"
  },
  {
    "id": "719",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"grafana\" has memory limit 0"
  },
  {
    "id": "720",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"grafana-sc-dashboard\" has memory limit 0"
  },
  {
    "id": "721",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/062_deployment_release-name-grafana.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-grafana\n  namespace: default\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: grafana\n      app.kubernetes.io/instance: release-name\n  strategy:\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: grafana-10.0.0\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 12.1.1\n      annotations:\n        checksum/config: 0e9cbd0ea8e24e32f7dfca5bab17a2ba05652642f0a09a4882833ae88e4cc4a3\n        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24\n        checksum/secret: 032056e9c62bbe9d1daa41ee49cd3d9524c076f51ca4c65adadf4ef08ef28712\n        kubectl.kubernetes.io/default-container: grafana\n    spec:\n      serviceAccountName: release-name-grafana\n      automountServiceAccountToken: true\n      shareProcessNamespace: false\n      securityContext:\n        fsGroup: 472\n        runAsGroup: 472\n        runAsNonRoot: true\n        runAsUser: 472\n      enableServiceLinks: true\n      containers:\n      - name: grafana-sc-dashboard\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_dashboard\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /tmp/dashboards\n        - name: RESOURCE\n          value: both\n        - name: NAMESPACE\n          value: ALL\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/dashboards/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n      - name: grafana-sc-datasources\n        image: quay.io/kiwigrid/k8s-sidecar:1.30.10\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: METHOD\n          value: WATCH\n        - name: LABEL\n          value: grafana_datasource\n        - name: LABEL_VALUE\n          value: '1'\n        - name: FOLDER\n          value: /etc/grafana/provisioning/datasources\n        - name: RESOURCE\n          value: both\n        - name: REQ_USERNAME\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: REQ_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: REQ_URL\n          value: http://localhost:3000/api/admin/provisioning/datasources/reload\n        - name: REQ_METHOD\n          value: POST\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n      - name: grafana\n        image: docker.io/grafana/grafana:12.1.1\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          seccompProfile:\n            type: RuntimeDefault\n        volumeMounts:\n        - name: config\n          mountPath: /etc/grafana/grafana.ini\n          subPath: grafana.ini\n        - name: storage\n          mountPath: /var/lib/grafana\n        - name: sc-dashboard-volume\n          mountPath: /tmp/dashboards\n        - name: sc-dashboard-provider\n          mountPath: /etc/grafana/provisioning/dashboards/sc-dashboardproviders.yaml\n          subPath: provider.yaml\n        - name: sc-datasources-volume\n          mountPath: /etc/grafana/provisioning/datasources\n        ports:\n        - name: grafana\n          containerPort: 3000\n          protocol: TCP\n        - name: gossip-tcp\n          containerPort: 9094\n          protocol: TCP\n        - name: gossip-udp\n          containerPort: 9094\n          protocol: UDP\n        - name: profiling\n          containerPort: 6060\n          protocol: TCP\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.podIP\n        - name: GF_SECURITY_ADMIN_USER\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-user\n        - name: GF_SECURITY_ADMIN_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-grafana\n              key: admin-password\n        - name: GF_PATHS_DATA\n          value: /var/lib/grafana/\n        - name: GF_PATHS_LOGS\n          value: /var/log/grafana\n        - name: GF_PATHS_PLUGINS\n          value: /var/lib/grafana/plugins\n        - name: GF_PATHS_PROVISIONING\n          value: /etc/grafana/provisioning\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /api/health\n            port: 3000\n          initialDelaySeconds: 60\n          timeoutSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /api/health\n            port: 3000\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-grafana\n      - name: storage\n        emptyDir: {}\n      - name: sc-dashboard-volume\n        emptyDir: {}\n      - name: sc-dashboard-provider\n        configMap:\n          name: release-name-grafana-config-dashboards\n      - name: sc-datasources-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"grafana-sc-datasources\" has memory limit 0"
  },
  {
    "id": "722",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/063_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kube-state-metrics\" not found"
  },
  {
    "id": "724",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/063_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "725",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/063_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n    release: release-name\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n        release: release-name\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "726",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/064_deployment_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kube-promethe-operator\" not found"
  },
  {
    "id": "727",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/064_deployment_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kube-prometheus-stack\" has cpu request 0"
  },
  {
    "id": "728",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/064_deployment_release-name-kube-promethe-operator.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-promethe-operator\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app: kube-prometheus-stack-operator\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app: kube-prometheus-stack-operator\n      release: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app: kube-prometheus-stack-operator\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator\n    spec:\n      containers:\n      - name: kube-prometheus-stack\n        image: quay.io/prometheus-operator/prometheus-operator:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --kubelet-service=kube-system/release-name-kube-promethe-kubelet\n        - --kubelet-endpoints=true\n        - --kubelet-endpointslice=false\n        - --localhost=127.0.0.1\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        - --config-reloader-cpu-request=0\n        - --config-reloader-cpu-limit=0\n        - --config-reloader-memory-request=0\n        - --config-reloader-memory-limit=0\n        - --thanos-default-base-image=quay.io/thanos/thanos:v0.39.2\n        - --secret-field-selector=type!=kubernetes.io/dockercfg,type!=kubernetes.io/service-account-token,type!=helm.sh/release.v1\n        - --web.enable-tls=true\n        - --web.cert-file=/cert/cert\n        - --web.key-file=/cert/key\n        - --web.listen-address=:10250\n        - --web.tls-min-version=VersionTLS13\n        ports:\n        - containerPort: 10250\n          name: https\n        env:\n        - name: GOGC\n          value: '30'\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: tls-secret\n          mountPath: /cert\n          readOnly: true\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: https\n            scheme: HTTPS\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 3\n      volumes:\n      - name: tls-secret\n        secret:\n          defaultMode: 420\n          secretName: release-name-kube-promethe-admission\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      serviceAccountName: release-name-kube-promethe-operator\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 30\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kube-prometheus-stack\" has memory limit 0"
  },
  {
    "id": "729",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"release-name-test\" does not have a read-only root file system"
  },
  {
    "id": "730",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-grafana-test\" not found"
  },
  {
    "id": "731",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"release-name-test\" is not set to runAsNonRoot"
  },
  {
    "id": "732",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-test\" has cpu request 0"
  },
  {
    "id": "733",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/124_pod_release-name-grafana-test.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-grafana-test\n  labels:\n    helm.sh/chart: grafana-10.0.0\n    app.kubernetes.io/name: grafana\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 12.1.1\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  namespace: default\nspec:\n  serviceAccountName: release-name-grafana-test\n  containers:\n  - name: release-name-test\n    image: docker.io/bats/bats:v1.4.1\n    imagePullPolicy: IfNotPresent\n    command:\n    - /opt/bats/bin/bats\n    - -t\n    - /tests/run.sh\n    volumeMounts:\n    - mountPath: /tests\n      name: tests\n      readOnly: true\n  volumes:\n  - name: tests\n    configMap:\n      name: release-name-grafana-test\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-test\" has memory limit 0"
  },
  {
    "id": "734",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/125_job_release-name-kube-promethe-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kube-promethe-admission\" not found"
  },
  {
    "id": "735",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/125_job_release-name-kube-promethe-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"create\" has cpu request 0"
  },
  {
    "id": "736",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/125_job_release-name-kube-promethe-admission-create.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-create\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-create\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-create\n      labels:\n        app: kube-prometheus-stack-admission-create\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: create\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - create\n        - --host=release-name-kube-promethe-operator,release-name-kube-promethe-operator.default.svc\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"create\" has memory limit 0"
  },
  {
    "id": "737",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/126_job_release-name-kube-promethe-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kube-promethe-admission\" not found"
  },
  {
    "id": "738",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/126_job_release-name-kube-promethe-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"patch\" has cpu request 0"
  },
  {
    "id": "739",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/kube-prometheus-stack/126_job_release-name-kube-promethe-admission-patch.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-kube-promethe-admission-patch\n  namespace: default\n  annotations:\n    helm.sh/hook: post-install,post-upgrade\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app: kube-prometheus-stack-admission-patch\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 77.14.0\n    app.kubernetes.io/part-of: kube-prometheus-stack\n    chart: kube-prometheus-stack-77.14.0\n    release: release-name\n    heritage: Helm\n    app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n    app.kubernetes.io/component: prometheus-operator-webhook\nspec:\n  ttlSecondsAfterFinished: 60\n  template:\n    metadata:\n      name: release-name-kube-promethe-admission-patch\n      labels:\n        app: kube-prometheus-stack-admission-patch\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 77.14.0\n        app.kubernetes.io/part-of: kube-prometheus-stack\n        chart: kube-prometheus-stack-77.14.0\n        release: release-name\n        heritage: Helm\n        app.kubernetes.io/name: kube-prometheus-stack-prometheus-operator\n        app.kubernetes.io/component: prometheus-operator-webhook\n    spec:\n      containers:\n      - name: patch\n        image: registry.k8s.io/ingress-nginx/kube-webhook-certgen:v1.6.3\n        imagePullPolicy: IfNotPresent\n        args:\n        - patch\n        - --webhook-name=release-name-kube-promethe-admission\n        - --namespace=default\n        - --secret-name=release-name-kube-promethe-admission\n        - --patch-failure-policy=\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        resources: {}\n      restartPolicy: OnFailure\n      serviceAccountName: release-name-kube-promethe-admission\n      securityContext:\n        runAsGroup: 2000\n        runAsNonRoot: true\n        runAsUser: 2000\n        seccompProfile:\n          type: RuntimeDefault\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"patch\" has memory limit 0"
  },
  {
    "id": "740",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/013_service_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9093\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:alertmanager])"
  },
  {
    "id": "741",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/014_service_release-name-alertmanager-headless.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-alertmanager-headless\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  clusterIP: None\n  ports:\n  - port: 9093\n    targetPort: http\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:alertmanager])"
  },
  {
    "id": "742",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/015_service_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - name: http\n    protocol: TCP\n    port: 8080\n    targetPort: http\n  selector:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:kube-state-metrics])"
  },
  {
    "id": "743",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/016_service_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\n  annotations:\n    prometheus.io/scrape: 'true'\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9100\n    targetPort: 9100\n    protocol: TCP\n    name: metrics\n  selector:\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus-node-exporter])"
  },
  {
    "id": "744",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/017_service_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    prometheus.io/probe: pushgateway\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  type: ClusterIP\n  ports:\n  - port: 9091\n    targetPort: 9091\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus-pushgateway])"
  },
  {
    "id": "745",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/018_service_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 9090\n  selector:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n  sessionAffinity: None\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:server app.kubernetes.io/instance:release-name app.kubernetes.io/name:prometheus])"
  },
  {
    "id": "746",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "no_host_network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "747",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "no_host_pid",
    "violation_text": "object shares the host's process namespace (via hostPID=true)."
  },
  {
    "id": "748",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-prometheus-node-exporter\" not found"
  },
  {
    "id": "752",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"node-exporter\" has cpu request 0"
  },
  {
    "id": "753",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/019_daemonset_release-name-prometheus-node-exporter.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: release-name-prometheus-node-exporter\n  namespace: default\n  labels:\n    helm.sh/chart: prometheus-node-exporter-4.48.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: prometheus-node-exporter\n    app.kubernetes.io/name: prometheus-node-exporter\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 1.9.1\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-node-exporter\n      app.kubernetes.io/instance: release-name\n  revisionHistoryLimit: 10\n  updateStrategy:\n    rollingUpdate:\n      maxUnavailable: 1\n    type: RollingUpdate\n  template:\n    metadata:\n      annotations:\n        cluster-autoscaler.kubernetes.io/safe-to-evict: 'true'\n      labels:\n        helm.sh/chart: prometheus-node-exporter-4.48.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: prometheus-node-exporter\n        app.kubernetes.io/name: prometheus-node-exporter\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 1.9.1\n    spec:\n      automountServiceAccountToken: false\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: release-name-prometheus-node-exporter\n      containers:\n      - name: node-exporter\n        image: quay.io/prometheus/node-exporter:v1.9.1\n        imagePullPolicy: IfNotPresent\n        args:\n        - --path.procfs=/host/proc\n        - --path.sysfs=/host/sys\n        - --path.rootfs=/host/root\n        - --path.udev.data=/host/root/run/udev/data\n        - --web.listen-address=[$(HOST_IP)]:9100\n        securityContext:\n          allowPrivilegeEscalation: false\n          readOnlyRootFilesystem: true\n        env:\n        - name: HOST_IP\n          value: 0.0.0.0\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 0\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        volumeMounts:\n        - name: proc\n          mountPath: /host/proc\n          readOnly: true\n        - name: sys\n          mountPath: /host/sys\n          readOnly: true\n        - name: root\n          mountPath: /host/root\n          mountPropagation: HostToContainer\n          readOnly: true\n      hostNetwork: true\n      hostPID: true\n      hostIPC: false\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n              - key: type\n                operator: NotIn\n                values:\n                - virtual-kubelet\n      nodeSelector:\n        kubernetes.io/os: linux\n      tolerations:\n      - effect: NoSchedule\n        operator: Exists\n      volumes:\n      - name: proc\n        hostPath:\n          path: /proc\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: root\n        hostPath:\n          path: /\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"node-exporter\" has memory limit 0"
  },
  {
    "id": "754",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/020_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-kube-state-metrics\" not found"
  },
  {
    "id": "756",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/020_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kube-state-metrics\" has cpu request 0"
  },
  {
    "id": "757",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/020_deployment_release-name-kube-state-metrics.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-kube-state-metrics\n  namespace: default\n  labels:\n    helm.sh/chart: kube-state-metrics-6.3.0\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/component: metrics\n    app.kubernetes.io/part-of: kube-state-metrics\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: 2.17.0\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: kube-state-metrics-6.3.0\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/component: metrics\n        app.kubernetes.io/part-of: kube-state-metrics\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: 2.17.0\n    spec:\n      automountServiceAccountToken: true\n      hostNetwork: false\n      serviceAccountName: release-name-kube-state-metrics\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n        seccompProfile:\n          type: RuntimeDefault\n      dnsPolicy: ClusterFirst\n      containers:\n      - name: kube-state-metrics\n        args:\n        - --port=8080\n        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments\n        imagePullPolicy: IfNotPresent\n        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:v2.17.0\n        ports:\n        - containerPort: 8080\n          name: http\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /livez\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            httpHeaders: null\n            path: /readyz\n            port: 8081\n            scheme: HTTP\n          initialDelaySeconds: 5\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources: {}\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "758",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"pushgateway\" does not have a read-only root file system"
  },
  {
    "id": "759",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-prometheus-pushgateway\" not found"
  },
  {
    "id": "760",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pushgateway\" has cpu request 0"
  },
  {
    "id": "761",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/021_deployment_release-name-prometheus-pushgateway.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    helm.sh/chart: prometheus-pushgateway-3.4.1\n    app.kubernetes.io/name: prometheus-pushgateway\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v1.11.1\n    app.kubernetes.io/managed-by: Helm\n  name: release-name-prometheus-pushgateway\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: prometheus-pushgateway\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        helm.sh/chart: prometheus-pushgateway-3.4.1\n        app.kubernetes.io/name: prometheus-pushgateway\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v1.11.1\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-prometheus-pushgateway\n      automountServiceAccountToken: true\n      containers:\n      - name: pushgateway\n        image: quay.io/prometheus/pushgateway:v1.11.1\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: metrics\n          containerPort: 9091\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9091\n          initialDelaySeconds: 10\n          timeoutSeconds: 10\n        volumeMounts:\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      securityContext:\n        fsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      volumes:\n      - name: storage-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pushgateway\" has memory limit 0"
  },
  {
    "id": "762",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"prometheus-server\" does not have a read-only root file system"
  },
  {
    "id": "763",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"prometheus-server-configmap-reload\" does not have a read-only root file system"
  },
  {
    "id": "764",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-prometheus-server\" not found"
  },
  {
    "id": "765",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"prometheus-server\" has cpu request 0"
  },
  {
    "id": "766",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"prometheus-server-configmap-reload\" has cpu request 0"
  },
  {
    "id": "767",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"prometheus-server\" has memory limit 0"
  },
  {
    "id": "768",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/022_deployment_release-name-prometheus-server.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: server\n    app.kubernetes.io/name: prometheus\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v3.6.0\n    helm.sh/chart: prometheus-27.39.0\n    app.kubernetes.io/part-of: prometheus\n  name: release-name-prometheus-server\n  namespace: default\nspec:\n  strategy:\n    type: Recreate\n    rollingUpdate: null\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: server\n      app.kubernetes.io/name: prometheus\n      app.kubernetes.io/instance: release-name\n  replicas: 1\n  revisionHistoryLimit: 10\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: server\n        app.kubernetes.io/name: prometheus\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/version: v3.6.0\n        helm.sh/chart: prometheus-27.39.0\n        app.kubernetes.io/part-of: prometheus\n    spec:\n      enableServiceLinks: true\n      serviceAccountName: release-name-prometheus-server\n      containers:\n      - name: prometheus-server-configmap-reload\n        image: quay.io/prometheus-operator/prometheus-config-reloader:v0.85.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --watched-dir=/etc/config\n        - --listen-address=0.0.0.0:8080\n        - --reload-url=http://127.0.0.1:9090/-/reload\n        ports:\n        - containerPort: 8080\n          name: metrics\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          initialDelaySeconds: 2\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: metrics\n            scheme: HTTP\n          periodSeconds: 10\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n          readOnly: true\n      - name: prometheus-server\n        image: quay.io/prometheus/prometheus:v3.6.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --storage.tsdb.retention.time=15d\n        - --config.file=/etc/config/prometheus.yml\n        - --storage.tsdb.path=/data\n        - --web.console.libraries=/etc/prometheus/console_libraries\n        - --web.console.templates=/etc/prometheus/consoles\n        - --web.enable-lifecycle\n        ports:\n        - containerPort: 9090\n        readinessProbe:\n          httpGet:\n            path: /-/ready\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 4\n          failureThreshold: 3\n          successThreshold: 1\n        livenessProbe:\n          httpGet:\n            path: /-/healthy\n            port: 9090\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          timeoutSeconds: 10\n          failureThreshold: 3\n          successThreshold: 1\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/config\n        - name: storage-volume\n          mountPath: /data\n          subPath: ''\n      dnsPolicy: ClusterFirst\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      terminationGracePeriodSeconds: 300\n      volumes:\n      - name: config-volume\n        configMap:\n          name: release-name-prometheus-server\n      - name: storage-volume\n        persistentVolumeClaim:\n          claimName: release-name-prometheus-server\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"prometheus-server-configmap-reload\" has memory limit 0"
  },
  {
    "id": "769",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"alertmanager\" does not have a read-only root file system"
  },
  {
    "id": "770",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-alertmanager\" not found"
  },
  {
    "id": "771",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"alertmanager\" has cpu request 0"
  },
  {
    "id": "772",
    "manifest_path": "data/manifests/artifacthub/prometheus-community/prometheus/023_statefulset_release-name-alertmanager.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: release-name-alertmanager\n  labels:\n    helm.sh/chart: alertmanager-1.26.0\n    app.kubernetes.io/name: alertmanager\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/version: v0.28.1\n    app.kubernetes.io/managed-by: Helm\n  namespace: default\nspec:\n  replicas: 1\n  minReadySeconds: 0\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: alertmanager\n      app.kubernetes.io/instance: release-name\n  serviceName: release-name-alertmanager-headless\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: alertmanager\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/config: 86786111a5a0824db0a26ce4c428c97fc46ba7691960434bc7430b5c2db4bc9c\n    spec:\n      automountServiceAccountToken: true\n      serviceAccountName: release-name-alertmanager\n      securityContext:\n        fsGroup: 65534\n        runAsGroup: 65534\n        runAsNonRoot: true\n        runAsUser: 65534\n      containers:\n      - name: alertmanager\n        securityContext:\n          runAsGroup: 65534\n          runAsNonRoot: true\n          runAsUser: 65534\n        image: quay.io/prometheus/alertmanager:v0.28.1\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: POD_IP\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: status.podIP\n        args:\n        - --storage.path=/alertmanager\n        - --config.file=/etc/alertmanager/alertmanager.yml\n        ports:\n        - name: http\n          containerPort: 9093\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n        resources: {}\n        volumeMounts:\n        - name: config\n          mountPath: /etc/alertmanager\n        - name: storage\n          mountPath: /alertmanager\n      volumes:\n      - name: config\n        configMap:\n          name: release-name-alertmanager\n  volumeClaimTemplates:\n  - metadata:\n      name: storage\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 2Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"alertmanager\" has memory limit 0"
  },
  {
    "id": "773",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/004_service_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:pgadmin4])"
  },
  {
    "id": "774",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"pgadmin4\" does not have a read-only root file system"
  },
  {
    "id": "775",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pgadmin4\" has cpu request 0"
  },
  {
    "id": "776",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/005_deployment_release-name-pgadmin4.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-pgadmin4\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\nspec:\n  replicas: 1\n  revisionHistoryLimit: 10\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: pgadmin4\n      app.kubernetes.io/instance: release-name\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: pgadmin4\n        app.kubernetes.io/instance: release-name\n      annotations:\n        checksum/secret: 98898618a428e0aa6d5525c3cfd24ec2d7f159362180ead330a833c9649bb315\n    spec:\n      automountServiceAccountToken: false\n      containers:\n      - name: pgadmin4\n        image: docker.io/dpage/pgadmin4:9.8\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 30\n          periodSeconds: 20\n          timeoutSeconds: 5\n        startupProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 30\n          periodSeconds: 2\n        readinessProbe:\n          httpGet:\n            port: http\n            path: /misc/ping\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 10\n          periodSeconds: 10\n          timeoutSeconds: 3\n        env:\n        - name: PGADMIN_CONFIG_ENHANCED_COOKIE_PROTECTION\n          value: 'False'\n        - name: PGADMIN_DEFAULT_EMAIL\n          value: chart@domain.com\n        - name: PGADMIN_DEFAULT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: release-name-pgadmin4\n              key: password\n        volumeMounts:\n        - name: pgadmin-data\n          mountPath: /var/lib/pgadmin\n          subPath: ''\n        resources: {}\n      volumes:\n      - name: pgadmin-data\n        persistentVolumeClaim:\n          claimName: release-name-pgadmin4\n      securityContext:\n        fsGroup: 5050\n        runAsGroup: 5050\n        runAsUser: 5050\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pgadmin4\" has memory limit 0"
  },
  {
    "id": "777",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n  restartPolicy: Never\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"wget\" is using an invalid container image, \"docker.io/busybox:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "778",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wget\" has cpu request 0"
  },
  {
    "id": "779",
    "manifest_path": "data/manifests/artifacthub/runix/pgadmin4/006_pod_release-name-pgadmin4-test-connection.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: release-name-pgadmin4-test-connection\n  namespace: default\n  labels:\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/name: pgadmin4\n    app.kubernetes.io/version: '9.8'\n    helm.sh/chart: pgadmin4-1.50.0\n  annotations:\n    helm.sh/hook: test\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  securityContext:\n    runAsNonRoot: true\n    fsGroup: 5051\n    runAsGroup: 5051\n    runAsUser: 5051\n  containers:\n  - name: wget\n    image: docker.io/busybox:latest\n    env:\n    - name: PGADMIN_HOST\n      value: release-name-pgadmin4\n    - name: PGADMIN_PORT\n      value: '80'\n    command:\n    - /bin/sh\n    - -ec\n    - 'response=$(wget -qSO - http://${PGADMIN_HOST}:${PGADMIN_PORT} 2>&1)\n\n      check=$(echo $response | grep -c ''200 OK''); echo $check; if [[ $check -gt\n      0 ]]; then echo \"Response OK\"; else exit 1; fi\n\n      '\n    resources: {}\n    securityContext:\n      readOnlyRootFilesystem: true\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wget\" has memory limit 0"
  },
  {
    "id": "780",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/004_service_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  type: LoadBalancer\n  selector:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n  ports:\n  - port: 80\n    name: web\n    targetPort: web\n    protocol: TCP\n  - port: 443\n    name: websecure\n    targetPort: websecure\n    protocol: TCP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name-default app.kubernetes.io/name:traefik])"
  },
  {
    "id": "781",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/005_deployment_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources: null\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-traefik\" not found"
  },
  {
    "id": "782",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/005_deployment_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources: null\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-traefik\" has cpu request 0"
  },
  {
    "id": "783",
    "manifest_path": "data/manifests/artifacthub/traefik/traefik/005_deployment_release-name-traefik.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: release-name-traefik\n  namespace: default\n  labels:\n    app.kubernetes.io/name: traefik\n    app.kubernetes.io/instance: release-name-default\n    helm.sh/chart: traefik-37.1.2\n    app.kubernetes.io/managed-by: Helm\n  annotations: null\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: traefik\n      app.kubernetes.io/instance: release-name-default\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxUnavailable: 0\n      maxSurge: 1\n  minReadySeconds: 0\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/path: /metrics\n        prometheus.io/port: '9100'\n      labels:\n        app.kubernetes.io/name: traefik\n        app.kubernetes.io/instance: release-name-default\n        helm.sh/chart: traefik-37.1.2\n        app.kubernetes.io/managed-by: Helm\n    spec:\n      serviceAccountName: release-name-traefik\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 60\n      hostNetwork: false\n      containers:\n      - image: docker.io/traefik:v3.5.3\n        imagePullPolicy: IfNotPresent\n        name: release-name-traefik\n        resources: null\n        readinessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 1\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        livenessProbe:\n          httpGet:\n            path: /ping\n            port: 8080\n            scheme: HTTP\n          failureThreshold: 3\n          initialDelaySeconds: 2\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 2\n        lifecycle: null\n        ports:\n        - name: metrics\n          containerPort: 9100\n          protocol: TCP\n        - name: traefik\n          containerPort: 8080\n          protocol: TCP\n        - name: web\n          containerPort: 8000\n          protocol: TCP\n        - name: websecure\n          containerPort: 8443\n          protocol: TCP\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          readOnlyRootFilesystem: true\n        volumeMounts:\n        - name: data\n          mountPath: /data\n        - name: tmp\n          mountPath: /tmp\n        args:\n        - --entryPoints.metrics.address=:9100/tcp\n        - --entryPoints.traefik.address=:8080/tcp\n        - --entryPoints.web.address=:8000/tcp\n        - --entryPoints.websecure.address=:8443/tcp\n        - --api.dashboard=true\n        - --ping=true\n        - --metrics.prometheus=true\n        - --metrics.prometheus.entrypoint=metrics\n        - --providers.kubernetescrd\n        - --providers.kubernetescrd.allowEmptyServices=true\n        - --providers.kubernetesingress\n        - --providers.kubernetesingress.allowEmptyServices=true\n        - --providers.kubernetesingress.ingressendpoint.publishedservice=default/release-name-traefik\n        - --entryPoints.websecure.http.tls=true\n        - --log.level=INFO\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: USER\n          value: traefik\n      volumes:\n      - name: data\n        emptyDir: {}\n      - name: tmp\n        emptyDir: {}\n      securityContext:\n        runAsGroup: 65532\n        runAsNonRoot: true\n        runAsUser: 65532\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"release-name-traefik\" has memory limit 0"
  },
  {
    "id": "784",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/007_service_release-name-velero.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: release-name-velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  type: ClusterIP\n  ports:\n  - name: http-monitoring\n    port: 8085\n    targetPort: http-monitoring\n  selector:\n    name: velero\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/instance:release-name app.kubernetes.io/name:velero name:velero])"
  },
  {
    "id": "785",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"velero\" does not have a read-only root file system"
  },
  {
    "id": "786",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-velero-server\" not found"
  },
  {
    "id": "787",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"velero\" is not set to runAsNonRoot"
  },
  {
    "id": "788",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"velero\" has cpu request 0"
  },
  {
    "id": "789",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/008_deployment_velero.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: velero\n  namespace: default\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    app.kubernetes.io/version: 1.17.0\n    helm.sh/chart: velero-11.0.0\n    component: velero\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: release-name\n      app.kubernetes.io/name: velero\n  template:\n    metadata:\n      labels:\n        name: velero\n        app.kubernetes.io/name: velero\n        app.kubernetes.io/instance: release-name\n        app.kubernetes.io/managed-by: Helm\n        app.kubernetes.io/version: 1.17.0\n        helm.sh/chart: velero-11.0.0\n      annotations:\n        prometheus.io/path: /metrics\n        prometheus.io/port: '8085'\n        prometheus.io/scrape: 'true'\n        checksum/secret: b9754ca4d83dab2f2e3ce5d2f763c69ed92c2298dc2737084680716557250af3\n    spec:\n      restartPolicy: Always\n      serviceAccountName: release-name-velero-server\n      automountServiceAccountToken: true\n      terminationGracePeriodSeconds: 3600\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http-monitoring\n          containerPort: 8085\n        command:\n        - /velero\n        args:\n        - server\n        - --uploader-type=kopia\n        - --repo-maintenance-job-configmap=velero-repo-maintenance\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        readinessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /metrics\n            port: http-monitoring\n            scheme: HTTP\n          initialDelaySeconds: 10\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        volumeMounts:\n        - name: plugins\n          mountPath: /plugins\n        - name: cloud-credentials\n          mountPath: /credentials\n        - name: scratch\n          mountPath: /scratch\n        env:\n        - name: VELERO_SCRATCH_DIR\n          value: /scratch\n        - name: VELERO_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: LD_LIBRARY_PATH\n          value: /plugins\n        - name: AWS_SHARED_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /credentials/cloud\n        - name: AZURE_CREDENTIALS_FILE\n          value: /credentials/cloud\n        - name: ALIBABA_CLOUD_CREDENTIALS_FILE\n          value: /credentials/cloud\n      dnsPolicy: ClusterFirst\n      volumes:\n      - name: cloud-credentials\n        secret:\n          secretName: release-name-velero\n      - name: plugins\n        emptyDir: {}\n      - name: scratch\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"velero\" has memory limit 0"
  },
  {
    "id": "790",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "791",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"kubectl\" does not have a read-only root file system"
  },
  {
    "id": "792",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"velero\" does not have a read-only root file system"
  },
  {
    "id": "793",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"release-name-velero-server-upgrade-crds\" not found"
  },
  {
    "id": "794",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"kubectl\" is not set to runAsNonRoot"
  },
  {
    "id": "795",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"velero\" is not set to runAsNonRoot"
  },
  {
    "id": "796",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kubectl\" has cpu request 0"
  },
  {
    "id": "797",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"velero\" has cpu request 0"
  },
  {
    "id": "798",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kubectl\" has memory limit 0"
  },
  {
    "id": "799",
    "manifest_path": "data/manifests/artifacthub/vmware-tanzu/velero/014_job_release-name-velero-upgrade-crds.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: release-name-velero-upgrade-crds\n  namespace: default\n  annotations:\n    helm.sh/hook: pre-install,pre-upgrade,pre-rollback\n    helm.sh/hook-weight: '5'\n    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded\n  labels:\n    app.kubernetes.io/name: velero\n    app.kubernetes.io/instance: release-name\n    app.kubernetes.io/managed-by: Helm\n    helm.sh/chart: velero-11.0.0\nspec:\n  backoffLimit: 3\n  template:\n    metadata:\n      name: velero-upgrade-crds\n    spec:\n      serviceAccountName: release-name-velero-server-upgrade-crds\n      automountServiceAccountToken: true\n      initContainers:\n      - name: kubectl\n        image: docker.io/bitnamilegacy/kubectl:1.34\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - cp `which sh` /tmp && cp `which kubectl` /tmp\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      containers:\n      - name: velero\n        image: velero/velero:v1.17.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /tmp/sh\n        args:\n        - -c\n        - /velero install --crds-only --dry-run -o yaml | /tmp/kubectl apply -f -\n        volumeMounts:\n        - mountPath: /tmp\n          name: crds\n      volumes:\n      - name: crds\n        emptyDir: {}\n      restartPolicy: OnFailure\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"velero\" has memory limit 0"
  },
  {
    "id": "800",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"pulsar-admin\" is using an invalid container image, \"apachepulsar/pulsar:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "801",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"pulsar-admin\" does not have a read-only root file system"
  },
  {
    "id": "802",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"pulsar-admin\" is not set to runAsNonRoot"
  },
  {
    "id": "803",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pulsar-admin\" has cpu request 0"
  },
  {
    "id": "804",
    "manifest_path": "data/manifests/the_stack_sample/sample_0000.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pulsar-admin\nspec:\n  containers:\n  - name: pulsar-admin\n    image: apachepulsar/pulsar:latest\n    command:\n    - sh\n    - -c\n    args:\n    - 'bin/apply-config-from-env.py conf/client.conf && bin/apply-config-from-env.py\n      conf/pulsar_env.sh && bin/apply-config-from-env.py conf/pulsar_tools_env.sh\n      && sleep 10000000000\n\n      '\n    envFrom:\n    - configMapRef:\n        name: broker-config\n    env:\n    - name: webServiceUrl\n      value: http://broker:8080/\n    - name: brokerServiceUrl\n      value: pulsar://broker:6650/\n    - name: PULSAR_MEM\n      value: '\"-Xms64m -Xmx128m\"'\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pulsar-admin\" has memory limit 0"
  },
  {
    "id": "805",
    "manifest_path": "data/manifests/the_stack_sample/sample_0001.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: open-api-doc\nspec:\n  type: ClusterIP\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: http\n  selector:\n    app: open-api-doc\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:open-api-doc])"
  },
  {
    "id": "806",
    "manifest_path": "data/manifests/the_stack_sample/sample_0002.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: argocd-metrics\nspec:\n  ports:\n  - name: http\n    protocol: TCP\n    port: 8082\n    targetPort: 8082\n  selector:\n    app: argocd-server\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:argocd-server])"
  },
  {
    "id": "807",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"datasets-lib-unb-ca\" is using an invalid container image, \"||DEPLOYMENTIMAGE||\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "808",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"datasets-lib-unb-ca\" does not have a read-only root file system"
  },
  {
    "id": "809",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"datasets-lib-unb-ca\" is not set to runAsNonRoot"
  },
  {
    "id": "810",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"datasets-lib-unb-ca\" has cpu request 0"
  },
  {
    "id": "811",
    "manifest_path": "data/manifests/the_stack_sample/sample_0004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: drupal\n    appMajor: '8'\n    instance: datasets.lib.unb.ca\n    tier: frontend\n    uri: datasets.lib.unb.ca\n    vcsOwner: unb-libraries\n    vcsRepository: datasets.lib.unb.ca\n    vcsRef: prod\n  name: datasets-lib-unb-ca\n  namespace: prod\nspec:\n  selector:\n    matchLabels:\n      uri: datasets.lib.unb.ca\n  replicas: 1\n  revisionHistoryLimit: 2\n  minReadySeconds: 30\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 0\n  template:\n    metadata:\n      labels:\n        app: drupal\n        appMajor: '8'\n        instance: datasets.lib.unb.ca\n        tier: frontend\n        uri: datasets.lib.unb.ca\n        vcsOwner: unb-libraries\n        vcsRepository: datasets.lib.unb.ca\n        vcsRef: prod\n    spec:\n      nodeSelector:\n        deployenv: prod\n      containers:\n      - name: datasets-lib-unb-ca\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 30\n          timeoutSeconds: 3\n          periodSeconds: 15\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 80\n          initialDelaySeconds: 100\n          timeoutSeconds: 3\n          periodSeconds: 15\n        env:\n        - name: DEPLOY_ENV\n          value: prod\n        - name: MYSQL_HOSTNAME\n          value: drupal-mysql-lib-unb-ca\n        - name: MYSQL_PORT\n          value: '3306'\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mysql\n              key: root-password\n        - name: NR_INSTALL_KEY\n          valueFrom:\n            secretKeyRef:\n              name: newrelic\n              key: install-key\n        image: '||DEPLOYMENTIMAGE||'\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 80\n        volumeMounts:\n        - mountPath: /app/html/sites/default\n          name: drupal-persistent-storage\n      priorityClassName: med-priority-services\n      imagePullSecrets:\n      - name: github-container-registry-auth\n      restartPolicy: Always\n      volumes:\n      - name: drupal-persistent-storage\n        persistentVolumeClaim:\n          claimName: datasets-lib-unb-ca\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"datasets-lib-unb-ca\" has memory limit 0"
  },
  {
    "id": "812",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"metrics\" does not have a read-only root file system"
  },
  {
    "id": "813",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "814",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"metrics\" has cpu request 0"
  },
  {
    "id": "815",
    "manifest_path": "data/manifests/the_stack_sample/sample_0005.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: boskos-metrics\n  labels:\n    app.kubernetes.io/part-of: boskos\n    app: boskos-metrics\n  namespace: boskos\nspec:\n  selector:\n    matchLabels:\n      app: boskos-metrics\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: boskos-metrics\n      namespace: test-pods\n    spec:\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: metrics\n        image: gcr.io/k8s-staging-boskos/metrics:v20200819-984516e\n        args:\n        - --resource-type=gke-perf-preset,gcp-perf-test-project,gcp-project,gke-e2e-test\n        ports:\n        - containerPort: 8080\n          protocol: TCP\n        readinessProbe:\n          httpGet:\n            path: /prometheus\n            port: 8080\n          periodSeconds: 1\n          timeoutSeconds: 1\n          successThreshold: 1\n          failureThreshold: 10\n      tolerations:\n      - key: dedicated\n        operator: Equal\n        value: boskos\n        effect: NoSchedule\n      nodeSelector:\n        prod: boskos\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"metrics\" has memory limit 0"
  },
  {
    "id": "816",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "817",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "818",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "819",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "820",
    "manifest_path": "data/manifests/the_stack_sample/sample_0006.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20210414-378dc3ffc3\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "821",
    "manifest_path": "data/manifests/the_stack_sample/sample_0009.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubwz0un1-cffc\n  labels:\n    app: kubwz0un1-cffc\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n    targetPort: 8080\n    protocol: TCP\n    name: http\n  selector:\n    app: kubwz0un1-cffc\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:kubwz0un1-cffc])"
  },
  {
    "id": "822",
    "manifest_path": "data/manifests/the_stack_sample/sample_0010.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: elasticsearch\n  labels:\n    component: elasticsearch\n    role: data\n  annotations:\n    cloud.google.com/load-balancer-type: Internal\nspec:\n  selector:\n    component: elasticsearch\n    role: data\n  ports:\n  - name: http\n    port: 9200\n  type: LoadBalancer\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[component:elasticsearch role:data])"
  },
  {
    "id": "823",
    "manifest_path": "data/manifests/the_stack_sample/sample_0012.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    getambassador.io/config: \"---\\napiVersion: ambassador/v0\\nkind:  Mapping\\nname:\\\n      \\ webapp_mapping\\nprefix: /jupyter/\\nservice: jupyter-web-app-service.kubeflow\\n\\\n      add_request_headers:\\n  x-forwarded-prefix: /jupyter\"\n  labels:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n    run: jupyter-web-app\n  name: jupyter-web-app-service\n  namespace: kubeflow\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 5000\n  selector:\n    app: jupyter-web-app\n    app.kubernetes.io/component: jupyter-web-app\n    app.kubernetes.io/name: jupyter-web-app\n    kustomize.component: jupyter-web-app\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:jupyter-web-app app.kubernetes.io/component:jupyter-web-app app.kubernetes.io/name:jupyter-web-app kustomize.component:jupyter-web-app])"
  },
  {
    "id": "824",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "825",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"healthcheck-ready\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "826",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"holder-vcs-add-profiles\" is using an invalid container image, \"alpine:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "827",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"wait\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "828",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"healthcheck-ready\" does not have a read-only root file system"
  },
  {
    "id": "829",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"holder-vcs-add-profiles\" does not have a read-only root file system"
  },
  {
    "id": "830",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"wait\" does not have a read-only root file system"
  },
  {
    "id": "831",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"healthcheck-ready\" is not set to runAsNonRoot"
  },
  {
    "id": "832",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"holder-vcs-add-profiles\" is not set to runAsNonRoot"
  },
  {
    "id": "833",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"wait\" is not set to runAsNonRoot"
  },
  {
    "id": "834",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"healthcheck-ready\" has cpu request 0"
  },
  {
    "id": "835",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"holder-vcs-add-profiles\" has cpu request 0"
  },
  {
    "id": "836",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait\" has cpu request 0"
  },
  {
    "id": "837",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"healthcheck-ready\" has memory limit 0"
  },
  {
    "id": "838",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"holder-vcs-add-profiles\" has memory limit 0"
  },
  {
    "id": "839",
    "manifest_path": "data/manifests/the_stack_sample/sample_0013.yaml",
    "manifest_yaml": "kind: Job\napiVersion: batch/v1\nmetadata:\n  name: holder-vcs-add-profiles\nspec:\n  template:\n    spec:\n      volumes:\n      - name: script\n        configMap:\n          name: holder-vcs-add-profiles-script\n      restartPolicy: Never\n      initContainers:\n      - name: wait\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        args:\n        - -c\n        - sleep 5\n      - name: healthcheck-ready\n        image: busybox\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - 'while [[ \"$(wget -T 5 -S --spider http://${HOLDER_VCS_SERVICE_HOST}/healthcheck\n          2>&1 | grep ''200 OK'')\" == \"\" ]];\n\n          do echo \"waiting for endpoint\";\n\n          sleep 5;\n\n          done;\n\n          '\n      containers:\n      - name: holder-vcs-add-profiles\n        image: alpine:latest\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        args:\n        - /opt/vcs_holder_configure.sh\n        volumeMounts:\n        - name: script\n          mountPath: /opt\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"wait\" has memory limit 0"
  },
  {
    "id": "840",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "841",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "842",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "843",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "844",
    "manifest_path": "data/manifests/the_stack_sample/sample_0014.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-654\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "845",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"pipeline\" does not have a read-only root file system"
  },
  {
    "id": "846",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"prow-pipeline\" not found"
  },
  {
    "id": "847",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"pipeline\" is not set to runAsNonRoot"
  },
  {
    "id": "848",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pipeline\" has cpu request 0"
  },
  {
    "id": "849",
    "manifest_path": "data/manifests/the_stack_sample/sample_0015.yaml",
    "manifest_yaml": "kind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: prow-pipeline\n  namespace: default\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: prow-pipeline\n  template:\n    metadata:\n      labels:\n        app: prow-pipeline\n    spec:\n      serviceAccountName: prow-pipeline\n      containers:\n      - name: pipeline\n        image: gcr.io/k8s-prow/pipeline:v20210525-8b942ff77a\n        args:\n        - --all-contexts\n        - --config=/etc/prow-config/config.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/prow-config\n          name: prow-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: prow-config\n        configMap:\n          name: config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pipeline\" has memory limit 0"
  },
  {
    "id": "850",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "851",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "852",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "853",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "854",
    "manifest_path": "data/manifests/the_stack_sample/sample_0017.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-4894\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "856",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 4 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "857",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"hook\" does not have a read-only root file system"
  },
  {
    "id": "858",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"hook\" not found"
  },
  {
    "id": "860",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"hook\" is not set to runAsNonRoot"
  },
  {
    "id": "861",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"hook\" has cpu request 0"
  },
  {
    "id": "862",
    "manifest_path": "data/manifests/the_stack_sample/sample_0019.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: hook\n  labels:\n    app: hook\nspec:\n  replicas: 4\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: hook\n  template:\n    metadata:\n      labels:\n        app: hook\n    spec:\n      serviceAccountName: hook\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: hook\n        image: gcr.io/k8s-prow/hook:v20220404-e2e605a820\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy.prow.svc.cluster.local\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/token\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/kubeconfig\n        ports:\n        - name: http\n          containerPort: 8888\n        - name: metrics\n          containerPort: 9090\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n        volumeMounts:\n        - name: hmac\n          mountPath: /etc/webhook\n          readOnly: true\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        - name: kubeconfig\n          mountPath: /etc/kubeconfig\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: hmac\n        secret:\n          secretName: k8s-infra-prow-hmac-token\n      - name: github-token\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-ci-robot-github-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: k8s-infra-build-clusters-kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"hook\" has memory limit 0"
  },
  {
    "id": "863",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"test-container\" is using an invalid container image, \"gcr.io/google_containers/test-webserver\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "864",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"test-container\" does not have a read-only root file system"
  },
  {
    "id": "865",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"test-container\" is not set to runAsNonRoot"
  },
  {
    "id": "866",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"test-container\" has cpu request 0"
  },
  {
    "id": "867",
    "manifest_path": "data/manifests/the_stack_sample/sample_0020.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  annotations:\n    git-commit-hash: de53e301b2cbc0e991f2cf609eef099c91a7fcfe\n  name: test-pd\nspec:\n  containers:\n  - image: gcr.io/google_containers/test-webserver\n    name: test-container\n    volumeMounts:\n    - mountPath: /test-pd\n      name: test-volume\n  volumes:\n  - hostPath:\n      path: /tmp\n    name: test-volume\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"test-container\" has memory limit 0"
  },
  {
    "id": "868",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"openmcp-snapshot\" does not have a read-only root file system"
  },
  {
    "id": "869",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"openmcp-snapshot-sa\" not found"
  },
  {
    "id": "870",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"openmcp-snapshot\" is not set to runAsNonRoot"
  },
  {
    "id": "871",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"openmcp-snapshot\" has cpu request 0"
  },
  {
    "id": "872",
    "manifest_path": "data/manifests/the_stack_sample/sample_0022.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: openmcp-snapshot\n  namespace: openmcp\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: openmcp-snapshot\n  template:\n    metadata:\n      labels:\n        name: openmcp-snapshot\n    spec:\n      serviceAccountName: openmcp-snapshot-sa\n      imagePullSecrets:\n      - name: regcred\n      containers:\n      - name: openmcp-snapshot\n        image: openmcp/openmcp-snapshot:v0.0.2\n        command:\n        - openmcp-snapshot\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: openmcp-snapshot\n      tolerations:\n      - key: node.kubernetes.io/not-ready\n        effect: NoExecute\n        tolerationSeconds: 0\n      - key: node.kubernetes.io/unreachable\n        effect: NoExecute\n        tolerationSeconds: 0\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"openmcp-snapshot\" has memory limit 0"
  },
  {
    "id": "873",
    "manifest_path": "data/manifests/the_stack_sample/sample_0023.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nginx-service-lb\n  labels:\n    app: nginx\nspec:\n  type: LoadBalancer\n  loadBalancerSourceRanges:\n  - 10.30.88.0/24\n  ports:\n  - port: 80\n    targetPort: 80\n    protocol: TCP\n    name: tcp\n  selector:\n    app: nginx\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:nginx])"
  },
  {
    "id": "874",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "875",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "876",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "877",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "878",
    "manifest_path": "data/manifests/the_stack_sample/sample_0025.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6696\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "879",
    "manifest_path": "data/manifests/the_stack_sample/sample_0026.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app: ws-app01\n  name: vote\n  namespace: attin-studio\nspec:\n  ports:\n  - name: 8080-tcp\n    port: 8080\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    deploymentconfig: vote\n  sessionAffinity: None\n  type: ClusterIP\nstatus:\n  loadBalancer: {}\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[deploymentconfig:vote])"
  },
  {
    "id": "881",
    "manifest_path": "data/manifests/the_stack_sample/sample_0027.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"web-ui\" does not have a read-only root file system"
  },
  {
    "id": "882",
    "manifest_path": "data/manifests/the_stack_sample/sample_0027.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"web-ui\" is not set to runAsNonRoot"
  },
  {
    "id": "883",
    "manifest_path": "data/manifests/the_stack_sample/sample_0027.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: web-ui\n  namespace: oih-dev-ns\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: web-ui\n  template:\n    metadata:\n      labels:\n        app: web-ui\n    spec:\n      containers:\n      - name: web-ui\n        image: node:16-alpine\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - npm install --non-interactive && npm start:container\n        workingDir: /usr/src/app/\n        ports:\n        - containerPort: 3000\n        env:\n        - name: PORT\n          value: '3000'\n        - name: ENDPOINT_IAM\n          value: http://iam.example.com\n        - name: ENDPOINT_FLOW\n          value: http://flow-repository.example.com\n        - name: ENDPOINT_COMPONENT\n          value: http://component-repository.example.com\n        - name: ENDPOINT_SECRETS\n          value: http://skm.example.com/api/v1\n        - name: ENDPOINT_DISPATCHER\n          value: http://dispatcher-service.example.com\n        - name: ENDPOINT_METADATA\n          value: http://metadata.example.com/api/v1\n        - name: ENDPOINT_APP_DIRECTORY\n          value: http://app-directory.example.com/api/v1\n        - name: NODE_ENV\n          value: development\n        - name: LOG_LEVEL\n          value: debug\n        resources:\n          limits:\n            cpu: '0.1'\n            memory: 500Mi\n        volumeMounts:\n        - name: code\n          mountPath: /usr/src/app\n          subPath: services/web-ui\n        livenessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 300\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            port: 3000\n            path: /healthcheck\n          initialDelaySeconds: 120\n          timeoutSeconds: 1\n        imagePullPolicy: IfNotPresent\n      volumes:\n      - name: code\n        persistentVolumeClaim:\n          claimName: source-volume-claim\n  minReadySeconds: 10\n  revisionHistoryLimit: 2\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"web-ui\" has cpu request 0"
  },
  {
    "id": "884",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "deprecated_service_account_field",
    "violation_text": "serviceAccount is specified (csi-bos-external-runner), but this field is deprecated; use serviceAccountName instead"
  },
  {
    "id": "886",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"csi-bosplugin\" has DROP capabilities: [], but does not drop capability \"NET_RAW\" which is required"
  },
  {
    "id": "887",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "no_host_network",
    "violation_text": "resource shares host's network namespace (via hostNetwork=true)."
  },
  {
    "id": "888",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"csi-bosplugin\" does not have a read-only root file system"
  },
  {
    "id": "889",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "890",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"csi-bos-external-runner\" not found"
  },
  {
    "id": "891",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "drop_capabilities",
    "violation_text": "container \"csi-bosplugin\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "892",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "no_privileged",
    "violation_text": "container \"csi-bosplugin\" is privileged"
  },
  {
    "id": "893",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"csi-bosplugin\" is not set to runAsNonRoot"
  },
  {
    "id": "894",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "896",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"csi-bosplugin\" has cpu request 0"
  },
  {
    "id": "897",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"node-driver-registrar\" has cpu request 0"
  },
  {
    "id": "898",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"csi-bosplugin\" has memory limit 0"
  },
  {
    "id": "899",
    "manifest_path": "data/manifests/the_stack_sample/sample_0031.yaml",
    "manifest_yaml": "kind: DaemonSet\napiVersion: apps/v1\nmetadata:\n  name: csi-bosplugin-node-server\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-bosplugin-node-server\n  template:\n    metadata:\n      labels:\n        app: csi-bosplugin-node-server\n    spec:\n      serviceAccount: csi-bos-external-runner\n      hostNetwork: true\n      restartPolicy: Always\n      priorityClassName: system-node-critical\n      tolerations:\n      - operator: Exists\n      containers:\n      - name: node-driver-registrar\n        image: registry.baidubce.com/cce-plugin-pro/csi-node-driver-registrar:v2.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(ADDRESS)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: reg-dir\n          mountPath: /registration\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n      - name: csi-bosplugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: registry.baidubce.com/cce-plugin-pro/cce-csi-plugin:v1.1.1\n        args:\n        - --driver-type=bos\n        - --csi-endpoint=$(CSI_ENDPOINT)\n        - --driver-mode=node\n        - --max-volumes-per-node=10\n        - --region=gz\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix://var/lib/kubelet/plugins/bos.csi.baidubce.com/csi.sock\n        - name: NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: Always\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: host-sys\n          mountPath: /sys\n          mountPropagation: HostToContainer\n        - name: docker-sock\n          mountPath: /var/run/docker.sock\n        - name: bosfs-cred-dir\n          mountPath: /etc/bosfs/credentials\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: DirectoryOrCreate\n      - name: reg-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry\n          type: DirectoryOrCreate\n      - name: host-sys\n        hostPath:\n          path: /sys\n      - name: docker-sock\n        hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n      - name: bosfs-cred-dir\n        hostPath:\n          path: /etc/bosfs/credentials\n          type: DirectoryOrCreate\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "900",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"influx\" is using an invalid container image, \"nexclipper/influxdb\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "901",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"influx\" does not have a read-only root file system"
  },
  {
    "id": "902",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"influx\" is not set to runAsNonRoot"
  },
  {
    "id": "903",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"influx\" has cpu request 0"
  },
  {
    "id": "904",
    "manifest_path": "data/manifests/the_stack_sample/sample_0032.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: influx\n  namespace: nexclipper\n  labels:\n    app: influx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: influx\n  template:\n    metadata:\n      labels:\n        app: influx\n    spec:\n      containers:\n      - env:\n        - name: PRE_CREATE_DB\n          value: dashboard\n        image: nexclipper/influxdb\n        imagePullPolicy: IfNotPresent\n        name: influx\n        ports:\n        - containerPort: 8083\n        - containerPort: 8086\n        volumeMounts:\n        - name: influx-data\n          mountPath: /var/lib/influxdb\n      volumes:\n      - name: influx-data\n        hostPath:\n          path: /nfs/inlfuxdb\nstatus: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"influx\" has memory limit 0"
  },
  {
    "id": "905",
    "manifest_path": "data/manifests/the_stack_sample/sample_0033.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kubernetes-dashboard-test\n  labels:\n    addon: kubernetes-dashboard.addons.k8s.io\n    app: kubernetes-dashboard-test\n    kubernetes.io/cluster-service: 'true'\n    facing: external\nspec:\n  type: LoadBalancer\n  selector:\n    app: kubernetes-dashboard-test\n  ports:\n  - port: 80\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:kubernetes-dashboard-test])"
  },
  {
    "id": "906",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"demo-server\" is using an invalid container image, \"docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "907",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "908",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"demo-server\" does not have a read-only root file system"
  },
  {
    "id": "909",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"demo-server\" is not set to runAsNonRoot"
  },
  {
    "id": "910",
    "manifest_path": "data/manifests/the_stack_sample/sample_0035.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: demo-server\n  name: demo-server\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: demo-server\n  template:\n    metadata:\n      labels:\n        app: demo-server\n        room: bjzt\n    spec:\n      containers:\n      - image: docker.io/swiftabc/cncamp-lilong-2022-02-28-d43c98c\n        name: demo-server\n        command:\n        - /http-server\n        args:\n        - --logtostderr=true\n        - --config=file:////config/mod8.ini\n        volumeMounts:\n        - mountPath: /config\n          name: web-conf\n        resources:\n          limits:\n            memory: 200Mi\n          requests:\n            memory: 100Mi\n        lifecycle:\n          preStop:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - kill -TERM 1\n        ports:\n        - containerPort: 1880\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 1880\n          initialDelaySeconds: 30\n          periodSeconds: 5\n          timeoutSeconds: 1\n      nodeSelector:\n        room: bjzt\n      volumes:\n      - name: web-conf\n        configMap:\n          name: web-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"demo-server\" has cpu request 0"
  },
  {
    "id": "911",
    "manifest_path": "data/manifests/the_stack_sample/sample_0036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n  restartPolicy: Never\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"succeeded\" does not have a read-only root file system"
  },
  {
    "id": "912",
    "manifest_path": "data/manifests/the_stack_sample/sample_0036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n  restartPolicy: Never\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"succeeded\" is not set to runAsNonRoot"
  },
  {
    "id": "913",
    "manifest_path": "data/manifests/the_stack_sample/sample_0036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"succeeded\" has cpu request 0"
  },
  {
    "id": "914",
    "manifest_path": "data/manifests/the_stack_sample/sample_0036.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: 100mb-of-logs\nspec:\n  containers:\n  - name: succeeded\n    image: alpine:3.4\n    command:\n    - /bin/sh\n    - -c\n    - echo start----; base64 /dev/urandom | head -c 10000000; echo ----end\n  restartPolicy: Never\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"succeeded\" has memory limit 0"
  },
  {
    "id": "915",
    "manifest_path": "data/manifests/the_stack_sample/sample_0038.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0\n  labels:\n    app: nginx-test\n  name: nginx-test\nspec:\n  ports:\n  - name: nginx-test\n    port: 80\n    protocol: TCP\n    targetPort: 80\n  selector:\n    app: nginx-test\n  type: LoadBalancer\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:nginx-test])"
  },
  {
    "id": "916",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"kaniko\" does not have a read-only root file system"
  },
  {
    "id": "917",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"kubeflow-pipelines-container-builder\" not found"
  },
  {
    "id": "918",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"kaniko\" is not set to runAsNonRoot"
  },
  {
    "id": "919",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kaniko\" has cpu request 0"
  },
  {
    "id": "920",
    "manifest_path": "data/manifests/the_stack_sample/sample_0040.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  generateName: kaniko-\n  namespace: default\n  annotations:\n    sidecar.istio.io/inject: 'false'\nspec:\n  restartPolicy: Never\n  serviceAccountName: kubeflow-pipelines-container-builder\n  containers:\n  - name: kaniko\n    image: gcr.io/kaniko-project/executor@sha256:78d44ec4e9cb5545d7f85c1924695c89503ded86a59f92c7ae658afa3cff5400\n    args:\n    - --cache=true\n    - --dockerfile=dockerfile\n    - --context=gs://mlpipeline/kaniko_build.tar.gz\n    - --destination=gcr.io/mlpipeline/kaniko_image:latest\n    - --digest-file=/dev/termination-log\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"kaniko\" has memory limit 0"
  },
  {
    "id": "922",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"cloudwatch-agent\" does not have a read-only root file system"
  },
  {
    "id": "923",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"cloudwatch-agent\" not found"
  },
  {
    "id": "924",
    "manifest_path": "data/manifests/the_stack_sample/sample_0041.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: cloudwatch-agent\n  namespace: amazon-cloudwatch\nspec:\n  selector:\n    matchLabels:\n      name: cloudwatch-agent\n  template:\n    metadata:\n      labels:\n        name: cloudwatch-agent\n    spec:\n      containers:\n      - name: cloudwatch-agent\n        image: amazon/cloudwatch-agent:1.231221.0\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 200m\n            memory: 200Mi\n        env:\n        - name: HOST_IP\n          valueFrom:\n            fieldRef:\n              fieldPath: status.hostIP\n        - name: HOST_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: CI_VERSION\n          value: k8s/1.1.0\n        volumeMounts:\n        - name: cwagentconfig\n          mountPath: /etc/cwagentconfig\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n          readOnly: true\n        - name: varlibdocker\n          mountPath: /var/lib/docker\n          readOnly: true\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: devdisk\n          mountPath: /dev/disk\n          readOnly: true\n      volumes:\n      - name: cwagentconfig\n        configMap:\n          name: cwagentconfig\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: varlibdocker\n        hostPath:\n          path: /var/lib/docker\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: devdisk\n        hostPath:\n          path: /dev/disk/\n      terminationGracePeriodSeconds: 60\n      serviceAccountName: cloudwatch-agent\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"cloudwatch-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "927",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "928",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "929",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "930",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "931",
    "manifest_path": "data/manifests/the_stack_sample/sample_0042.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3467\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "932",
    "manifest_path": "data/manifests/the_stack_sample/sample_0043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: ekco\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"ekc-operator\" does not have a read-only root file system"
  },
  {
    "id": "933",
    "manifest_path": "data/manifests/the_stack_sample/sample_0043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: ekco\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"ekco\" not found"
  },
  {
    "id": "934",
    "manifest_path": "data/manifests/the_stack_sample/sample_0043.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ekc-operator\nspec:\n  selector:\n    matchLabels:\n      app: ekc-operator\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: ekc-operator\n    spec:\n      serviceAccountName: ekco\n      restartPolicy: Always\n      nodeSelector:\n        node-role.kubernetes.io/master: ''\n      containers:\n      - name: ekc-operator\n        image: replicated/ekco:v0.2.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /usr/bin/ekco\n        - operator\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n        volumeMounts:\n        - name: ekco-config\n          mountPath: /etc/ekco\n          readOnly: true\n        - name: certificates-dir\n          mountPath: /etc/kubernetes/pki\n          readOnly: true\n      volumes:\n      - name: ekco-config\n        configMap:\n          name: ekco-config\n      - name: certificates-dir\n        hostPath:\n          path: /etc/kubernetes/pki\n          type: Directory\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"ekc-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "935",
    "manifest_path": "data/manifests/the_stack_sample/sample_0045.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    environment: prod\n    region: eu-central-1\n  name: app\n  namespace: apps\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9797'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: app\n    spec:\n      containers:\n      - image: nginx:1.21.4\n        imagePullPolicy: IfNotPresent\n        name: nginx\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "936",
    "manifest_path": "data/manifests/the_stack_sample/sample_0045.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    environment: prod\n    region: eu-central-1\n  name: app\n  namespace: apps\nspec:\n  selector:\n    matchLabels:\n      app: app\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9797'\n        prometheus.io/scrape: 'true'\n      labels:\n        app: app\n    spec:\n      containers:\n      - image: nginx:1.21.4\n        imagePullPolicy: IfNotPresent\n        name: nginx\n        resources:\n          limits:\n            cpu: 2000m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 64Mi\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "937",
    "manifest_path": "data/manifests/the_stack_sample/sample_0050.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: app\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    app: app\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:app])"
  },
  {
    "id": "938",
    "manifest_path": "data/manifests/the_stack_sample/sample_0052.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"rethinkdb\" does not have a read-only root file system"
  },
  {
    "id": "939",
    "manifest_path": "data/manifests/the_stack_sample/sample_0052.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"rethinkdb\" is not set to runAsNonRoot"
  },
  {
    "id": "940",
    "manifest_path": "data/manifests/the_stack_sample/sample_0052.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"rethinkdb\" has cpu request 0"
  },
  {
    "id": "941",
    "manifest_path": "data/manifests/the_stack_sample/sample_0052.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  labels:\n    db: rethinkdb\n  name: rethinkdb-replica-2\nspec:\n  replicas: 1\n  selector:\n    db: rethinkdb\n    role: replica\n    instance: two\n  template:\n    metadata:\n      labels:\n        db: rethinkdb\n        role: replica\n        instance: two\n    spec:\n      containers:\n      - image: us.gcr.io/dray-app/rethinkdb:2.3.2\n        command:\n        - /usr/bin/run.sh\n        - --server-name\n        - replica_2\n        imagePullPolicy: Always\n        name: rethinkdb\n        env:\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        ports:\n        - containerPort: 8080\n          name: admin-port\n        - containerPort: 28015\n          name: driver-port\n        - containerPort: 29015\n          name: cluster-port\n        volumeMounts:\n        - mountPath: /data\n          name: rethinkdb-storage\n      volumes:\n      - gcePersistentDisk:\n          fsType: ext4\n          pdName: rethinkdb-storage-stage-2\n        name: rethinkdb-storage\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"rethinkdb\" has memory limit 0"
  },
  {
    "id": "942",
    "manifest_path": "data/manifests/the_stack_sample/sample_0053.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "943",
    "manifest_path": "data/manifests/the_stack_sample/sample_0053.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "944",
    "manifest_path": "data/manifests/the_stack_sample/sample_0053.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: heapster-v9\n  namespace: kube-system\n  labels:\n    k8s-app: heapster\n    version: v9\n    kubernetes.io/cluster-service: 'true'\nspec:\n  replicas: 1\n  selector:\n    k8s-app: heapster\n    version: v9\n  template:\n    metadata:\n      labels:\n        k8s-app: heapster\n        version: v9\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - image: gcr.io/google_containers/heapster:v0.18.1\n        name: heapster\n        resources:\n          limits:\n            cpu: 100m\n            memory: 300Mi\n        command:\n        - /heapster\n        - --source=kubernetes:''\n        - --sink=gcl\n        - --sink=gcmautoscaling\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        - --sink_frequency=2m\n        volumeMounts:\n        - name: ssl-certs\n          mountPath: /etc/ssl/certs\n          readOnly: true\n      volumes:\n      - name: ssl-certs\n        hostPath:\n          path: /etc/ssl/certs\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"heapster\" has cpu request 0"
  },
  {
    "id": "945",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "946",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"git-clone\" does not have a read-only root file system"
  },
  {
    "id": "947",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"job\" does not have a read-only root file system"
  },
  {
    "id": "948",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"jx-boot-job\" not found"
  },
  {
    "id": "949",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"git-clone\" is not set to runAsNonRoot"
  },
  {
    "id": "950",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"job\" is not set to runAsNonRoot"
  },
  {
    "id": "951",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"git-clone\" has cpu request 0"
  },
  {
    "id": "952",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"job\" has cpu request 0"
  },
  {
    "id": "953",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"git-clone\" has memory limit 0"
  },
  {
    "id": "954",
    "manifest_path": "data/manifests/the_stack_sample/sample_0054.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  labels:\n    app: jx-boot\n    jenkins-x.io/kind: jx-git-operator\nspec:\n  backoffLimit: 4\n  completions: 1\n  parallelism: 1\n  template:\n    metadata:\n      labels:\n        app: jx-boot\n        jenkins-x.io/kind: jx-git-operator\n    spec:\n      initContainers:\n      - args:\n        - gitops\n        - git\n        - clone\n        command:\n        - jx\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        name: git-clone\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace\n      containers:\n      - args:\n        - apply\n        command:\n        - make\n        env:\n        - name: XDG_CONFIG_HOME\n          value: /workspace/xdg_config\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        image: ghcr.io/jenkins-x/jx-boot:3.2.46\n        imagePullPolicy: Always\n        name: job\n        volumeMounts:\n        - mountPath: /workspace\n          name: workspace-volume\n        workingDir: /workspace/source\n      dnsPolicy: ClusterFirst\n      restartPolicy: Never\n      schedulerName: default-scheduler\n      serviceAccountName: jx-boot-job\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: workspace-volume\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"job\" has memory limit 0"
  },
  {
    "id": "955",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"master\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "956",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"master\" does not have a read-only root file system"
  },
  {
    "id": "957",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"master\" is not set to runAsNonRoot"
  },
  {
    "id": "958",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"master\" has cpu request 0"
  },
  {
    "id": "959",
    "manifest_path": "data/manifests/the_stack_sample/sample_0056.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis-master\n  labels:\n    app: redis\nspec:\n  selector:\n    matchLabels:\n      app: redis\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: redis\n        role: master\n    spec:\n      containers:\n      - name: master\n        image: redis\n        ports:\n        - containerPort: 6379\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"master\" has memory limit 0"
  },
  {
    "id": "960",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "961",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"jornada-rental\" is using an invalid container image, \"wesleywillians/rental_jornada:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "962",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"jornada-rental\" does not have a read-only root file system"
  },
  {
    "id": "963",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"jornada-rental\" is not set to runAsNonRoot"
  },
  {
    "id": "964",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"jornada-rental\" has cpu request 0"
  },
  {
    "id": "965",
    "manifest_path": "data/manifests/the_stack_sample/sample_0057.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: rental-job-consumer-products\nspec:\n  template:\n    spec:\n      restartPolicy: OnFailure\n      containers:\n      - name: jornada-rental\n        image: wesleywillians/rental_jornada:latest\n        imagePullPolicy: Always\n        command:\n        - php\n        - artisan\n        - kafka:consume\n        - products\n        - product-group\n        envFrom:\n        - configMapRef:\n            name: rental-app-conf\n        env:\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: rental-mysql-pass\n              key: password\n        volumeMounts:\n        - name: rental-app-conf\n          subPath: .env\n          mountPath: /var/www/.env\n      volumes:\n      - name: rental-app-conf\n        configMap:\n          name: rental-app-conf\n          items:\n          - key: env\n            path: .env\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"jornada-rental\" has memory limit 0"
  },
  {
    "id": "966",
    "manifest_path": "data/manifests/the_stack_sample/sample_0058.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-master-deploy\n  namespace: jenkins\n  labels:\n    app.kubernetes.io/name: jenkins-master\n    app.kubernetes.io/instance: jenkins-master-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: jenkins-master-pod\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins-master\n        app.kubernetes.io/instance: jenkins-master-pod\n    spec:\n      containers:\n      - name: jenkins-master-pod\n        image: jenkins/jenkins:jdk11\n        ports:\n        - containerPort: 8080\n        - containerPort: 50000\n        resources:\n          requests:\n            memory: 1000Mi\n            cpu: 300m\n          limits:\n            memory: 1000Mi\n            cpu: 300m\n        env:\n        - name: JENKINS_JAVA_OPTIONS\n          value: -Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Seoul\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-master-pvc\n      volumes:\n      - name: jenkins-master-pvc\n        persistentVolumeClaim:\n          claimName: jenkins-master-pvc\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"jenkins-master-pod\" does not have a read-only root file system"
  },
  {
    "id": "967",
    "manifest_path": "data/manifests/the_stack_sample/sample_0058.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-master-deploy\n  namespace: jenkins\n  labels:\n    app.kubernetes.io/name: jenkins-master\n    app.kubernetes.io/instance: jenkins-master-deploy\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/instance: jenkins-master-pod\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: jenkins-master\n        app.kubernetes.io/instance: jenkins-master-pod\n    spec:\n      containers:\n      - name: jenkins-master-pod\n        image: jenkins/jenkins:jdk11\n        ports:\n        - containerPort: 8080\n        - containerPort: 50000\n        resources:\n          requests:\n            memory: 1000Mi\n            cpu: 300m\n          limits:\n            memory: 1000Mi\n            cpu: 300m\n        env:\n        - name: JENKINS_JAVA_OPTIONS\n          value: -Dorg.apache.commons.jelly.tags.fmt.timeZone=Asia/Seoul\n        volumeMounts:\n        - mountPath: /var/jenkins_home\n          name: jenkins-master-pvc\n      volumes:\n      - name: jenkins-master-pvc\n        persistentVolumeClaim:\n          claimName: jenkins-master-pvc\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"jenkins-master-pod\" is not set to runAsNonRoot"
  },
  {
    "id": "968",
    "manifest_path": "data/manifests/the_stack_sample/sample_0061.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"bpg-parser-container\" does not have a read-only root file system"
  },
  {
    "id": "969",
    "manifest_path": "data/manifests/the_stack_sample/sample_0061.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"bpg-parser-container\" is not set to runAsNonRoot"
  },
  {
    "id": "970",
    "manifest_path": "data/manifests/the_stack_sample/sample_0061.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"bpg-parser-container\" has cpu request 0"
  },
  {
    "id": "971",
    "manifest_path": "data/manifests/the_stack_sample/sample_0061.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: bpg-parser-dep\n  namespace: ballerina-playground\n  labels:\n    app: bpg-parser\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: bpg-parser\n  template:\n    metadata:\n      labels:\n        app: bpg-parser\n    spec:\n      containers:\n      - name: bpg-parser-container\n        imagePullPolicy: Always\n        image: gcr.io/${BPG_GCP_PROJECT_ID}/parser:v0.1-20181210-2241\n        ports:\n        - name: http-port\n          containerPort: 80\n        - name: https-port\n          containerPort: 8443\n        readinessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 3\n          periodSeconds: 10\n        livenessProbe:\n          tcpSocket:\n            port: http-port\n          initialDelaySeconds: 5\n          periodSeconds: 20\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"bpg-parser-container\" has memory limit 0"
  },
  {
    "id": "972",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "973",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "974",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "975",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "976",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "977",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "978",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "979",
    "manifest_path": "data/manifests/the_stack_sample/sample_0064.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostpath0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n  securityContext:\n    runAsNonRoot: true\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "980",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"rpi3firmware-manager\" is using an invalid container image, \"registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "981",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"rpi3firmware-manager\" does not have a read-only root file system"
  },
  {
    "id": "982",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"rpi3firmware-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "983",
    "manifest_path": "data/manifests/the_stack_sample/sample_0066.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rpi3firmware-manager\nspec:\n  selector:\n    matchLabels:\n      app: rpi3firmware-manager\n  template:\n    metadata:\n      labels:\n        app: rpi3firmware-manager\n    spec:\n      containers:\n      - name: rpi3firmware-manager\n        image: registry.gitlab.com/clusterplatform/clusterplatform/rpi3firmware-manager.dev\n        resources:\n          limits:\n            memory: 200Mi\n            cpu: 200m\n        envFrom:\n        - configMapRef:\n            name: nats\n        - configMapRef:\n            name: redis\n        - configMapRef:\n            name: postgres\n        - configMapRef:\n            name: verdaccio\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"rpi3firmware-manager\" has cpu request 0"
  },
  {
    "id": "984",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"istio-translate-inspect\" does not have a read-only root file system"
  },
  {
    "id": "985",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"istio-translate-inspect\" is not set to runAsNonRoot"
  },
  {
    "id": "986",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"istio-translate-inspect\" has cpu request 0"
  },
  {
    "id": "987",
    "manifest_path": "data/manifests/the_stack_sample/sample_0068.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: istio-translate-inspect\n  namespace: issueflow\n  labels:\n    app: istio-translate-inspect\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: istio-translate-inspect\n  template:\n    metadata:\n      labels:\n        app: istio-translate-inspect\n    spec:\n      containers:\n      - name: istio-translate-inspect\n        image: shidaqiu/istio-inspect:1.0\n        command:\n        - python3\n        - -u\n        - entry.py\n        imagePullPolicy: Always\n        env:\n        - name: GITHUB_TOKEN\n          value: xxx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"istio-translate-inspect\" has memory limit 0"
  },
  {
    "id": "988",
    "manifest_path": "data/manifests/the_stack_sample/sample_0069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"heapster\" does not have a read-only root file system"
  },
  {
    "id": "989",
    "manifest_path": "data/manifests/the_stack_sample/sample_0069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"heapster\" not found"
  },
  {
    "id": "990",
    "manifest_path": "data/manifests/the_stack_sample/sample_0069.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: heapster\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      task: monitoring\n      k8s-app: heapster\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        task: monitoring\n        k8s-app: heapster\n    spec:\n      serviceAccountName: heapster\n      containers:\n      - name: heapster\n        image: gcr.io/google_containers/heapster-amd64:v1.4.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - /heapster\n        - --source=kubernetes:https://kubernetes.default\n        - --sink=influxdb:http://monitoring-influxdb:8086\n        resources:\n          limits:\n            cpu: 100m\n            memory: 128Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"heapster\" is not set to runAsNonRoot"
  },
  {
    "id": "991",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "992",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "993",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.core.somaxconn\"."
  },
  {
    "id": "994",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.fwmark_reflect\"."
  },
  {
    "id": "995",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.icmp_ratelimit\"."
  },
  {
    "id": "996",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.ip_local_port_range\"."
  },
  {
    "id": "997",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.ip_unprivileged_port_start\"."
  },
  {
    "id": "998",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_abort_on_overflow\"."
  },
  {
    "id": "999",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_adv_win_scale\"."
  },
  {
    "id": "1000",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_ecn\"."
  },
  {
    "id": "1001",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_fack\"."
  },
  {
    "id": "1002",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_fastopen\"."
  },
  {
    "id": "1003",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_fin_timeout\"."
  },
  {
    "id": "1004",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_fwmark_accept\"."
  },
  {
    "id": "1005",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_keepalive_intvl\"."
  },
  {
    "id": "1006",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_keepalive_probes\"."
  },
  {
    "id": "1007",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_keepalive_time\"."
  },
  {
    "id": "1008",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_max_syn_backlog\"."
  },
  {
    "id": "1009",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_max_tw_buckets\"."
  },
  {
    "id": "1010",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_retries2\"."
  },
  {
    "id": "1011",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_rfc1337\"."
  },
  {
    "id": "1012",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_slow_start_after_idle\"."
  },
  {
    "id": "1013",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_syn_retries\"."
  },
  {
    "id": "1014",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_synack_retries\"."
  },
  {
    "id": "1015",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_syncookies\"."
  },
  {
    "id": "1016",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.ipv4.tcp_tw_reuse\"."
  },
  {
    "id": "1017",
    "manifest_path": "data/manifests/the_stack_sample/sample_0070.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: redis-master\n  namespace: default\n  labels:\n    name: redis-master\nspec:\n  serviceName: redis-master\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis-master\n  template:\n    metadata:\n      labels:\n        app: redis-master\n        redis-master: 'true'\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      terminationGracePeriodSeconds: 10\n      securityContext:\n        sysctls:\n        - name: net.ipv4.tcp_syncookies\n          value: '0'\n        - name: net.ipv4.ip_local_port_range\n          value: 1024 65535\n        - name: net.core.somaxconn\n          value: '65535'\n        - name: net.ipv4.ip_unprivileged_port_start\n          value: '0'\n        - name: net.ipv4.tcp_tw_reuse\n          value: '1'\n        - name: net.ipv4.tcp_fin_timeout\n          value: '30'\n        - name: net.ipv4.tcp_keepalive_intvl\n          value: '10'\n        - name: net.ipv4.tcp_keepalive_probes\n          value: '2'\n        - name: net.ipv4.tcp_keepalive_time\n          value: '120'\n        - name: net.ipv4.tcp_ecn\n          value: '1'\n        - name: net.ipv4.tcp_max_syn_backlog\n          value: '65536'\n        - name: net.ipv4.tcp_rfc1337\n          value: '1'\n        - name: net.ipv4.tcp_slow_start_after_idle\n          value: '0'\n        - name: net.ipv4.tcp_fack\n          value: '1'\n        - name: net.ipv4.tcp_max_tw_buckets\n          value: '262144'\n        - name: net.ipv4.tcp_fastopen\n          value: '3'\n        - name: net.ipv4.icmp_ratelimit\n          value: '100'\n        - name: net.ipv4.tcp_abort_on_overflow\n          value: '1'\n        - name: net.ipv4.tcp_adv_win_scale\n          value: '2'\n        - name: net.ipv4.tcp_retries2\n          value: '8'\n        - name: net.ipv4.tcp_syn_retries\n          value: '3'\n        - name: net.ipv4.tcp_synack_retries\n          value: '2'\n        - name: net.unix.max_dgram_qlen\n          value: '512'\n        - name: net.ipv4.tcp_fwmark_accept\n          value: '1'\n        - name: net.ipv4.fwmark_reflect\n          value: '1'\n      containers:\n      - name: redis\n        image: redis:6.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 6379\n        env:\n        - name: MASTER\n          value: 'true'\n        resources:\n          requests:\n            memory: 256Mi\n            cpu: 250m\n          limits:\n            memory: 2Gi\n            cpu: 2000m\n        volumeMounts:\n        - name: redis-master-volume\n          mountPath: /data\n  volumeClaimTemplates:\n  - metadata:\n      name: redis-master-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 10Gi\n",
    "policy_id": "unsafe_sysctls",
    "violation_text": "resource specifies unsafe sysctl \"net.unix.max_dgram_qlen\"."
  },
  {
    "id": "1018",
    "manifest_path": "data/manifests/the_stack_sample/sample_0071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"container-aplicacao-loja\" does not have a read-only root file system"
  },
  {
    "id": "1019",
    "manifest_path": "data/manifests/the_stack_sample/sample_0071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"container-aplicacao-loja\" is not set to runAsNonRoot"
  },
  {
    "id": "1020",
    "manifest_path": "data/manifests/the_stack_sample/sample_0071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"container-aplicacao-loja\" has cpu request 0"
  },
  {
    "id": "1021",
    "manifest_path": "data/manifests/the_stack_sample/sample_0071.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: aplicacao\nspec:\n  containers:\n  - name: container-aplicacao-loja\n    image: rafanercessian/aplicacao-loja:v1\n    ports:\n    - containerPort: 80\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"container-aplicacao-loja\" has memory limit 0"
  },
  {
    "id": "1022",
    "manifest_path": "data/manifests/the_stack_sample/sample_0072.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: open5gs-pgw-svc-pool\n  namespace: open5gs\n  labels:\n    epc-mode: pgw\nspec:\n  type: ClusterIP\n  ports:\n  - name: gtpc\n    port: 2123\n    protocol: UDP\n  - name: gtpu\n    port: 2152\n    protocol: UDP\n  - name: gx\n    port: 3868\n    protocol: TCP\n  selector:\n    epc-mode: pgw\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[epc-mode:pgw])"
  },
  {
    "id": "1023",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"mongo\" is using an invalid container image, \"mongo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1024",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1025",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "1026",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "1027",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "1028",
    "manifest_path": "data/manifests/the_stack_sample/sample_0073.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: mongo\nspec:\n  selector:\n    matchLabels:\n      app: mongo\n  serviceName: mongo\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        app: mongo\n    spec:\n      terminationGracePeriodSeconds: 10\n      containers:\n      - name: mongo\n        image: mongo\n        command:\n        - mongod\n        - --bind_ip_all\n        - --replSet\n        - rs0\n        ports:\n        - containerPort: 27017\n        volumeMounts:\n        - name: mongo-volume\n          mountPath: /data/db\n  volumeClaimTemplates:\n  - metadata:\n      name: mongo-volume\n    spec:\n      accessModes:\n      - ReadWriteOnce\n      resources:\n        requests:\n          storage: 1Gi\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "1029",
    "manifest_path": "data/manifests/the_stack_sample/sample_0077.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    k8s-app: node-exporter\n  name: node-exporter\n  namespace: monitoring\nspec:\n  clusterIP: None\n  ports:\n  - name: https\n    port: 9100\n    targetPort: https\n  selector:\n    app: node-exporter\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:node-exporter])"
  },
  {
    "id": "1030",
    "manifest_path": "data/manifests/the_stack_sample/sample_0078.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: payment\n  labels:\n    name: payment\nspec:\n  ports:\n  - port: 80\n    targetPort: 80\n  selector:\n    name: payment\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[name:payment])"
  },
  {
    "id": "1031",
    "manifest_path": "data/manifests/the_stack_sample/sample_0079.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: qod-api\n  labels:\n    app: qod-api\n    tier: api\nspec:\n  type: ClusterIP\n  ports:\n  - port: 3000\n    targetPort: 3000\n    protocol: TCP\n    name: http\n  selector:\n    app: qod-api\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:qod-api])"
  },
  {
    "id": "1032",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n      restartPolicy: OnFailure\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "1033",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n      restartPolicy: OnFailure\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"job-04-exec-limit-c\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1034",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n      restartPolicy: OnFailure\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"job-04-exec-limit-c\" does not have a read-only root file system"
  },
  {
    "id": "1035",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n      restartPolicy: OnFailure\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"job-04-exec-limit-c\" is not set to runAsNonRoot"
  },
  {
    "id": "1036",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n      restartPolicy: OnFailure\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"job-04-exec-limit-c\" has cpu request 0"
  },
  {
    "id": "1037",
    "manifest_path": "data/manifests/the_stack_sample/sample_0080.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: job-04-exec-limit\n  namespace: doit-lab-02-d\n  labels:\n    k8s-app: job-03-exec-limit\n    k8s-scope: gke-ws-doit-lab-02-d\nspec:\n  backoffLimit: 5\n  activeDeadlineSeconds: 20\n  template:\n    metadata:\n      name: job-04-exec-limit-tpl\n    spec:\n      containers:\n      - name: job-04-exec-limit-c\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - echo 'just running a (always failing) parallel-consumer job'; sleep 1; exit\n          1;\n      restartPolicy: OnFailure\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"job-04-exec-limit-c\" has memory limit 0"
  },
  {
    "id": "1038",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1039",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1040",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1041",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1042",
    "manifest_path": "data/manifests/the_stack_sample/sample_0082.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-984\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1043",
    "manifest_path": "data/manifests/the_stack_sample/sample_0083.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    app.kubernetes.io/component: observability\n    app.kubernetes.io/instance: parca\n    app.kubernetes.io/name: parca\n    app.kubernetes.io/version: v0.7.1\n  name: parca\n  namespace: parca\nspec:\n  ports:\n  - name: http\n    port: 7070\n    targetPort: 7070\n  selector:\n    app.kubernetes.io/component: observability\n    app.kubernetes.io/instance: parca\n    app.kubernetes.io/name: parca\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/component:observability app.kubernetes.io/instance:parca app.kubernetes.io/name:parca])"
  },
  {
    "id": "1044",
    "manifest_path": "data/manifests/the_stack_sample/sample_0084.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"order\" does not have a read-only root file system"
  },
  {
    "id": "1045",
    "manifest_path": "data/manifests/the_stack_sample/sample_0084.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"order\" is not set to runAsNonRoot"
  },
  {
    "id": "1046",
    "manifest_path": "data/manifests/the_stack_sample/sample_0084.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"order\" has cpu request 0"
  },
  {
    "id": "1047",
    "manifest_path": "data/manifests/the_stack_sample/sample_0084.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: order\n  labels:\n    app: order\n    tier: frontend\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: order\n      tier: frontend\n  template:\n    metadata:\n      labels:\n        app: order\n        tier: frontend\n    spec:\n      containers:\n      - name: order\n        image: aankitatiwari24/air-pg:order\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"order\" has memory limit 0"
  },
  {
    "id": "1048",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"gateway-controller\" does not have a read-only root file system"
  },
  {
    "id": "1049",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"argo-events-sa\" not found"
  },
  {
    "id": "1050",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"gateway-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "1051",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"gateway-controller\" has cpu request 0"
  },
  {
    "id": "1052",
    "manifest_path": "data/manifests/the_stack_sample/sample_0085.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: gateway-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: gateway-controller\n  template:\n    metadata:\n      labels:\n        app: gateway-controller\n    spec:\n      serviceAccountName: argo-events-sa\n      containers:\n      - name: gateway-controller\n        image: argoproj/gateway-controller:v0.10-test\n        imagePullPolicy: Always\n        env:\n        - name: GATEWAY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: GATEWAY_CONTROLLER_CONFIG_MAP\n          value: gateway-controller-configmap\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"gateway-controller\" has memory limit 0"
  },
  {
    "id": "1053",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"gcr.io/wise-coyote-827/frontend-server\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1054",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "1055",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "1056",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "1057",
    "manifest_path": "data/manifests/the_stack_sample/sample_0087.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: frontend\nspec:\n  selector:\n    matchLabels:\n      app: frontend\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: frontend\n    spec:\n      containers:\n      - name: server\n        ports:\n        - containerPort: 8080\n        image: gcr.io/wise-coyote-827/frontend-server\n        env:\n        - name: BACKEND_SERVICE_ADDR\n          value: backend-internal:8080\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "1058",
    "manifest_path": "data/manifests/the_stack_sample/sample_0088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: efaf2824743f262428efac87b577e83419a9d9c1f7600f4815a09c7177935a0a\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: demonccc\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "1059",
    "manifest_path": "data/manifests/the_stack_sample/sample_0088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: efaf2824743f262428efac87b577e83419a9d9c1f7600f4815a09c7177935a0a\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: demonccc\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"lighthouse-keeper\" not found"
  },
  {
    "id": "1060",
    "manifest_path": "data/manifests/the_stack_sample/sample_0088.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.3.0\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: efaf2824743f262428efac87b577e83419a9d9c1f7600f4815a09c7177935a0a\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.3.0\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: demonccc\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.3.0\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "1061",
    "manifest_path": "data/manifests/the_stack_sample/sample_0089.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: radarr\nspec:\n  ports:\n  - name: http\n    protocol: TCP\n    targetPort: http\n    port: 80\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "1062",
    "manifest_path": "data/manifests/the_stack_sample/sample_0093.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"metabase\" does not have a read-only root file system"
  },
  {
    "id": "1063",
    "manifest_path": "data/manifests/the_stack_sample/sample_0093.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"metabase\" is not set to runAsNonRoot"
  },
  {
    "id": "1064",
    "manifest_path": "data/manifests/the_stack_sample/sample_0093.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"metabase\" has cpu request 0"
  },
  {
    "id": "1065",
    "manifest_path": "data/manifests/the_stack_sample/sample_0093.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: metabase\n  namespace: metabase\nspec:\n  selector:\n    matchLabels:\n      app: metabase\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: metabase\n    spec:\n      containers:\n      - name: metabase\n        image: metabase/metabase:v0.30.3\n        env:\n        - name: MB_DB_TYPE\n          value: mysql\n        - name: MB_DB_DBNAME\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: dbname\n        - name: MB_DB_PORT\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: port\n        - name: MB_DB_USER\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: user\n        - name: MB_DB_PASS\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: pass\n        - name: MB_DB_HOST\n          valueFrom:\n            secretKeyRef:\n              name: metabase-db\n              key: host\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"metabase\" has memory limit 0"
  },
  {
    "id": "1066",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1067",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"netbox\" does not have a read-only root file system"
  },
  {
    "id": "1068",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"netbox\" is not set to runAsNonRoot"
  },
  {
    "id": "1069",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"netbox\" has cpu request 0"
  },
  {
    "id": "1070",
    "manifest_path": "data/manifests/the_stack_sample/sample_0094.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: netbox\n  namespace: netbox-community\n  labels:\n    k8s-app: netbox\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      k8s-app: netbox\n  template:\n    metadata:\n      labels:\n        k8s-app: netbox\n    spec:\n      containers:\n      - name: netbox\n        image: quay.io/netboxcommunity/netbox:v2.11.12-ldap\n        ports:\n        - name: http\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 5\n          periodSeconds: 20\n        envFrom:\n        - configMapRef:\n            name: netbox-configmap\n        lifecycle:\n          postStart:\n            exec:\n              command:\n              - /bin/sh\n              - -c\n              - 'bash /home/install-plugins.sh && bash /home/start-rqworker.sh &&\n                bash /home/nginx-caching-fix.sh\n\n                '\n        volumeMounts:\n        - name: auth-ldap-bind-password\n          mountPath: /run/secrets/auth_ldap_bind_password\n          subPath: auth_ldap_bind_password\n          readOnly: true\n        - name: db-password\n          mountPath: /run/secrets/db_password\n          subPath: db_password\n          readOnly: true\n        - name: email-password\n          mountPath: /run/secrets/email_password\n          subPath: email_password\n          readOnly: true\n        - name: napalm-password\n          mountPath: /run/secrets/napalm_password\n          subPath: napalm_password\n          readOnly: true\n        - name: redis-password\n          mountPath: /run/secrets/redis_password\n          subPath: redis_password\n        - name: secret-key\n          mountPath: /run/secrets/secret_key\n          subPath: secret_key\n          readOnly: true\n        - name: superuser-api-token\n          mountPath: /run/secrets/superuser_api_token\n          subPath: superuser_api_token\n        - name: superuser-password\n          mountPath: /run/secrets/superuser_password\n          subPath: superuser_password\n          readOnly: true\n        - name: netbox-media-files\n          mountPath: /opt/netbox/netbox/media\n        - name: ldap-config\n          subPath: ldap_config.py\n          mountPath: /opt/netbox/netbox/netbox/ldap_config.py\n        - mountPath: /etc/netbox/config/netbox-plugins.py\n          name: netbox-plugins\n          subPath: netbox-plugins.py\n        - mountPath: /home/install-plugins.sh\n          name: install-plugins\n          subPath: install-plugins.sh\n        - mountPath: /home/start-rqworker.sh\n          name: start-rqworker\n          subPath: start-rqworker.sh\n        - mountPath: /home/nginx-caching-fix.sh\n          name: nginx-caching-fix\n          subPath: nginx-caching-fix.sh\n        - name: sso-saml2-xml\n          mountPath: /opt/netbox/sso-saml2.xml\n          subPath: sso-saml2-metadata.xml\n      restartPolicy: Always\n      volumes:\n      - name: netbox-media-files\n        persistentVolumeClaim:\n          claimName: netbox-media-pvc\n          readOnly: false\n      - name: ldap-config\n        configMap:\n          name: netbox-configmap\n          items:\n          - key: ldap_config.py\n            path: ldap_config.py\n      - name: netbox-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: netbox-plugins.py\n            path: netbox-plugins.py\n      - name: install-plugins\n        configMap:\n          name: startup-configmap\n          items:\n          - key: install-plugins.sh\n            path: install-plugins.sh\n      - name: start-rqworker\n        configMap:\n          name: startup-configmap\n          items:\n          - key: start-rqworker.sh\n            path: start-rqworker.sh\n      - name: nginx-caching-fix\n        configMap:\n          name: startup-configmap\n          items:\n          - key: nginx-caching-fix.sh\n            path: nginx-caching-fix.sh\n      - name: sso-saml2-xml\n        configMap:\n          name: sso-saml2-xml\n          items:\n          - key: sso-saml2-metadata.xml\n            path: sso-saml2-metadata.xml\n      - name: auth-ldap-bind-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: auth_ldap_bind_password\n            path: auth_ldap_bind_password\n      - name: db-password\n        secret:\n          secretName: netbox-postgresql\n          items:\n          - key: password\n            path: db_password\n      - name: email-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: email_password\n            path: email_password\n      - name: napalm-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: napalm_password\n            path: napalm_password\n      - name: redis-password\n        secret:\n          secretName: netbox-redis\n          items:\n          - key: redis-password\n            path: redis_password\n      - name: secret-key\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: secret_key\n            path: secret_key\n      - name: superuser-password\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_password\n            path: superuser_password\n      - name: superuser-api-token\n        secret:\n          secretName: netbox-secret\n          items:\n          - key: superuser_api_token\n            path: superuser_api_token\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"netbox\" has memory limit 0"
  },
  {
    "id": "1071",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n  restartPolicy: OnFailure\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"tf-mnist-1gpu\" is using an invalid container image, \"centaurusinfra/tf-onegpu-mnist:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1072",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n  restartPolicy: OnFailure\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"tf-mnist-1gpu\" does not have a read-only root file system"
  },
  {
    "id": "1073",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n  restartPolicy: OnFailure\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"tf-mnist-1gpu\" is not set to runAsNonRoot"
  },
  {
    "id": "1074",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n  restartPolicy: OnFailure\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"tf-mnist-1gpu\" has cpu request 0"
  },
  {
    "id": "1075",
    "manifest_path": "data/manifests/the_stack_sample/sample_0098.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-rn50-1\nspec:\n  containers:\n  - name: tf-mnist-1gpu\n    image: centaurusinfra/tf-onegpu-mnist:latest\n    workingDir: /tmp/test\n    command:\n    - sh\n    - -c\n    - python3 tf-cf-rn50-pack.py\n  restartPolicy: OnFailure\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"tf-mnist-1gpu\" has memory limit 0"
  },
  {
    "id": "1076",
    "manifest_path": "data/manifests/the_stack_sample/sample_0099.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"mongo\" does not have a read-only root file system"
  },
  {
    "id": "1077",
    "manifest_path": "data/manifests/the_stack_sample/sample_0099.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"mongo\" is not set to runAsNonRoot"
  },
  {
    "id": "1078",
    "manifest_path": "data/manifests/the_stack_sample/sample_0099.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mongo\" has cpu request 0"
  },
  {
    "id": "1079",
    "manifest_path": "data/manifests/the_stack_sample/sample_0099.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mongo\n  labels:\n    app: reddit\n    component: mongo\n    comment-db: 'true'\n    post-db: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: reddit\n      component: mongo\n  template:\n    metadata:\n      name: mongo\n      labels:\n        app: reddit\n        component: mongo\n        comment-db: 'true'\n        post-db: 'true'\n    spec:\n      containers:\n      - image: mongo:3.2\n        name: mongo\n        volumeMounts:\n        - name: mongo-gce-pd-storage\n          mountPath: /data/db\n      volumes:\n      - name: mongo-gce-pd-storage\n        persistentVolumeClaim:\n          claimName: mongo-pvc-dynamic\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mongo\" has memory limit 0"
  },
  {
    "id": "1080",
    "manifest_path": "data/manifests/the_stack_sample/sample_0100.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: kyuubi-example-service\nspec:\n  ports:\n  - nodePort: 30009\n    port: 10009\n    protocol: TCP\n  type: NodePort\n  selector:\n    app: kyuubi-server\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:kyuubi-server])"
  },
  {
    "id": "1081",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1082",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "1083",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "1084",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "1085",
    "manifest_path": "data/manifests/the_stack_sample/sample_0101.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: paymentservice\n  labels:\n    app: paymentservice\nspec:\n  replicas: 3\n  strategy:\n    type: RollingUpdate\n  selector:\n    matchLabels:\n      app: paymentservice\n  template:\n    metadata:\n      labels:\n        app: paymentservice\n    spec:\n      containers:\n      - name: server\n        image: opcode/hipster-payment:v.0.0.2\n        env:\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "1086",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1087",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"cowweb\" does not have a read-only root file system"
  },
  {
    "id": "1088",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"cowweb\" is not set to runAsNonRoot"
  },
  {
    "id": "1089",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cowweb\" has cpu request 0"
  },
  {
    "id": "1090",
    "manifest_path": "data/manifests/the_stack_sample/sample_0102.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: cowweb\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: cowweb\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: cowweb\n        version: v1.0\n    spec:\n      containers:\n      - name: cowweb\n        image: iad.ocir.io/gse00014640/handson-002/cowweb:v1.0\n        ports:\n        - name: api\n          containerPort: 8080\n        readinessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 5\n          periodSeconds: 10\n        livenessProbe:\n          httpGet:\n            path: /cowsay/ping\n            port: api\n          initialDelaySeconds: 15\n          periodSeconds: 20\n        imagePullPolicy: Always\n      imagePullSecrets:\n      - name: cowweb-secret\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"cowweb\" has memory limit 0"
  },
  {
    "id": "1091",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"jenkins-operator\" does not have a read-only root file system"
  },
  {
    "id": "1092",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"jenkins-operator\" not found"
  },
  {
    "id": "1093",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"jenkins-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "1094",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"jenkins-operator\" has cpu request 0"
  },
  {
    "id": "1095",
    "manifest_path": "data/manifests/the_stack_sample/sample_0103.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jenkins-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jenkins-operator\n  template:\n    metadata:\n      labels:\n        name: jenkins-operator\n    spec:\n      serviceAccountName: jenkins-operator\n      containers:\n      - name: jenkins-operator\n        image: virtuslab/jenkins-operator:v0.0.9\n        ports:\n        - containerPort: 60000\n          name: metrics\n        command:\n        - jenkins-operator\n        args: []\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jenkins-operator\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"jenkins-operator\" has memory limit 0"
  },
  {
    "id": "1096",
    "manifest_path": "data/manifests/the_stack_sample/sample_0104.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"libri-experimenter\" does not have a read-only root file system"
  },
  {
    "id": "1097",
    "manifest_path": "data/manifests/the_stack_sample/sample_0104.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"libri-experimenter\" is not set to runAsNonRoot"
  },
  {
    "id": "1098",
    "manifest_path": "data/manifests/the_stack_sample/sample_0104.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: libri-experimenter\nspec:\n  restartPolicy: Never\n  volumes:\n  - name: data\n    emptyDir: {}\n  containers:\n  - name: libri-experimenter\n    image: daedalus2718/libri-exp:snapshot-949fb64\n    args:\n    - run\n    - --librarians\n    - librarians-0.libri.default.svc.cluster.local:20100,librarians-1.libri.default.svc.cluster.local:20100,librarians-2.libri.default.svc.cluster.local:20100,librarians-3.libri.default.svc.cluster.local:20100,librarians-4.libri.default.svc.cluster.local:20100,librarians-5.libri.default.svc.cluster.local:20100,librarians-6.libri.default.svc.cluster.local:20100,librarians-7.libri.default.svc.cluster.local:20100\n    - --duration\n    - 30m\n    - --numAuthors\n    - '100'\n    - --docsPerDay\n    - '2560'\n    - --contentSizeKBGammaShape\n    - '1.5'\n    - --contentSizeKBGammaRate\n    - '0.00588'\n    - --sharesPerUpload\n    - '2'\n    - --nUploaders\n    - '64'\n    - --nDownloaders\n    - '192'\n    - --profile\n    env:\n    - name: GODEBUG\n      value: netdns=go\n    volumeMounts:\n    - name: data\n      mountPath: /data\n    resources:\n      limits:\n        memory: 2G\n        cpu: 400m\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"libri-experimenter\" has cpu request 0"
  },
  {
    "id": "1099",
    "manifest_path": "data/manifests/the_stack_sample/sample_0105.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n      restartPolicy: Never\n  backoffLimit: 1\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"postgresql-client\" does not have a read-only root file system"
  },
  {
    "id": "1100",
    "manifest_path": "data/manifests/the_stack_sample/sample_0105.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n      restartPolicy: Never\n  backoffLimit: 1\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"postgresql-client\" is not set to runAsNonRoot"
  },
  {
    "id": "1101",
    "manifest_path": "data/manifests/the_stack_sample/sample_0105.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n      restartPolicy: Never\n  backoffLimit: 1\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"postgresql-client\" has cpu request 0"
  },
  {
    "id": "1102",
    "manifest_path": "data/manifests/the_stack_sample/sample_0105.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: todo-table\n  namespace: todo\n  annotations:\n    argocd.argoproj.io/sync-wave: '1'\nspec:\n  ttlSecondsAfterFinished: 100\n  template:\n    spec:\n      containers:\n      - name: postgresql-client\n        image: postgres:12\n        imagePullPolicy: Always\n        env:\n        - name: PGPASSWORD\n          value: admin\n        command:\n        - psql\n        args:\n        - --host=postgresql\n        - --username=admin\n        - --no-password\n        - --dbname=todo\n        - --command=create table Todo (id bigint not null,completed boolean not null,ordering\n          integer,title varchar(255),url varchar(255),primary key (id));create sequence\n          hibernate_sequence start with 1 increment by 1;\n      restartPolicy: Never\n  backoffLimit: 1\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"postgresql-client\" has memory limit 0"
  },
  {
    "id": "1103",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "1104",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "1105",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "1106",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "1107",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "1108",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "1109",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "1110",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "1111",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "1112",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "1113",
    "manifest_path": "data/manifests/the_stack_sample/sample_0106.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-keras-api-custom-layers-v2-32\n  namespace: automated\nspec:\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      activeDeadlineSeconds: 7200\n      backoffLimit: 1\n      template:\n        metadata:\n          annotations:\n            tf-version.cloud-tpus.google.com: 2.4.0\n        spec:\n          containers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            image: gcr.io/xl-ml-test/health-monitor:stable\n            imagePullPolicy: Always\n            name: monitor\n          - args:\n            - /bin/bash\n            - -c\n            - 'set -u\n\n              set -e\n\n              set -x\n\n\n              export PATH=$PATH:/root/google-cloud-sdk/bin\n\n              gcloud source repos clone tf2-api-tests --project=xl-ml-test\n\n              cd tf2-api-tests\n\n              pip3 install behave\n\n              behave -e ipynb_checkpoints --tags=-fails --tags=-failspod -i custom_layers_model\n\n              '\n            env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n            imagePullPolicy: Always\n            name: train\n            resources:\n              limits:\n                cloud-tpus.google.com/preemptible-v2: 32\n              requests:\n                cpu: 2\n                memory: 20G\n            volumeMounts:\n            - mountPath: /dev/shm\n              name: dshm\n              readOnly: false\n          initContainers:\n          - env:\n            - name: POD_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.name\n            - name: POD_UID\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.uid\n            - name: POD_NAMESPACE\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.namespace\n            - name: JOB_NAME\n              valueFrom:\n                fieldRef:\n                  fieldPath: metadata.labels['job-name']\n            - name: MODEL_DIR\n              value: $(OUTPUT_BUCKET)/tf-r2.4.0/keras-api/custom-layers/v2-32/$(JOB_NAME)\n            - name: METRIC_CONFIG\n              value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n                : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n                \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"\\\n                use_run_name_prefix\\\": true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n\\\n                \\ \\\"regression_test_config\\\": {\\n  \\\"alert_for_failed_jobs\\\": true\\n\\\n                \\ },\\n \\\"test_name\\\": \\\"tf-r2.4.0-keras-api-custom-layers-v2-32\\\"\\n\\\n                }\\n\"\n            envFrom:\n            - configMapRef:\n                name: gcs-buckets\n            image: gcr.io/xl-ml-test/publisher:stable\n            imagePullPolicy: Always\n            name: publisher\n          nodeSelector:\n            tpu-available: 'true'\n          restartPolicy: Never\n          volumes:\n          - emptyDir:\n              medium: Memory\n            name: dshm\n  schedule: 0 10 * * *\n  successfulJobsHistoryLimit: 1\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "1114",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1115",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"skipper-demo\" does not have a read-only root file system"
  },
  {
    "id": "1116",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"skipper-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "1117",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"skipper-demo\" has cpu request 0"
  },
  {
    "id": "1118",
    "manifest_path": "data/manifests/the_stack_sample/sample_0107.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: skipper-demo\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      application: skipper-demo\n  template:\n    metadata:\n      labels:\n        application: skipper-demo\n    spec:\n      containers:\n      - name: skipper-demo\n        image: registry.opensource.zalan.do/teapot/skipper:v0.12.0\n        args:\n        - skipper\n        - -inline-routes\n        - '* -> inlineContent(\"<body style=''color: white; background-color: green;''><h1>Hello!</h1>\")\n          -> <shunt>'\n        ports:\n        - containerPort: 9090\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"skipper-demo\" has memory limit 0"
  },
  {
    "id": "1119",
    "manifest_path": "data/manifests/the_stack_sample/sample_0108.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: minio\n  namespace: minio\nspec:\n  ports:\n  - port: 9000\n    protocol: TCP\n    targetPort: 9000\n  selector:\n    app.kubernetes.io/name: minio\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app.kubernetes.io/name:minio])"
  },
  {
    "id": "1120",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1121",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1122",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1123",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1124",
    "manifest_path": "data/manifests/the_stack_sample/sample_0109.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-8031\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1125",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1126",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1127",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1128",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1129",
    "manifest_path": "data/manifests/the_stack_sample/sample_0110.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9918\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1131",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"prow-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "1132",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"prow-controller-manager\" not found"
  },
  {
    "id": "1134",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"prow-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "1135",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"prow-controller-manager\" has cpu request 0"
  },
  {
    "id": "1136",
    "manifest_path": "data/manifests/the_stack_sample/sample_0113.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: prow-controller-manager\n  labels:\n    app: prow-controller-manager\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  revisionHistoryLimit: 2\n  selector:\n    matchLabels:\n      app: prow-controller-manager\n  template:\n    metadata:\n      labels:\n        app: prow-controller-manager\n    spec:\n      serviceAccountName: prow-controller-manager\n      containers:\n      - name: prow-controller-manager\n        image: gcr.io/k8s-prow/prow-controller-manager:v20220203-9315ecd1a0\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --dry-run=false\n        - --enable-controller=plank\n        - --job-config-path=/etc/job-config\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config:/etc/kubeconfig-build-test-infra-trusted/kubeconfig\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - mountPath: /etc/kubeconfig-build-test-infra-trusted\n          name: kubeconfig-build-test-infra-trusted\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 5\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: kubeconfig-build-test-infra-trusted\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig-build-test-infra-trusted\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"prow-controller-manager\" has memory limit 0"
  },
  {
    "id": "1137",
    "manifest_path": "data/manifests/the_stack_sample/sample_0114.yaml",
    "manifest_yaml": "kind: Service\napiVersion: v1\nmetadata:\n  name: homeautomation-backend\n  labels:\n    app: homeautomation\n    tier: backend\nspec:\n  selector:\n    app: homeautomation\n    tier: backend\n  ports:\n  - name: http\n    protocol: TCP\n    port: 80\n    targetPort: 80\n  - name: https\n    protocol: TCP\n    port: 443\n    targetPort: 443\n  type: LoadBalancer\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:homeautomation tier:backend])"
  },
  {
    "id": "1138",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1139",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "1140",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "1141",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "1142",
    "manifest_path": "data/manifests/the_stack_sample/sample_0116.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: ReplicaSet\nmetadata:\n  name: payment\n  labels:\n    app: payment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: payment\n  template:\n    metadata:\n      labels:\n        app: payment\n    spec:\n      containers:\n      - name: server\n        image: hipster-paymentservice:v0.0.1\n        resources: {}\n        env:\n        - name: PORT\n          value: '8080'\n        - name: PRODUCT_CATALOG_SERVICE_ADDR\n          value: productcatalogservice:3550\n        - name: CURRENCY_SERVICE_ADDR\n          value: currencyservice:7000\n        - name: CART_SERVICE_ADDR\n          value: cartservice:7070\n        - name: RECOMMENDATION_SERVICE_ADDR\n          value: recommendationservice:8080\n        - name: SHIPPING_SERVICE_ADDR\n          value: shippingservice:50051\n        - name: CHECKOUT_SERVICE_ADDR\n          value: checkoutservice:5050\n        - name: AD_SERVICE_ADDR\n          value: adservice:9555\n        - name: ENV_PLATFORM\n          value: gcp\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "1143",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"sotr-project\" is using an invalid container image, \"wsosnowski2/sotr\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1144",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1145",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"sotr-project\" does not have a read-only root file system"
  },
  {
    "id": "1146",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"sotr-project\" is not set to runAsNonRoot"
  },
  {
    "id": "1147",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"sotr-project\" has cpu request 0"
  },
  {
    "id": "1148",
    "manifest_path": "data/manifests/the_stack_sample/sample_0121.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: score-app\n    env: qa\n  name: sotr-app\n  namespace: mlops\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: sotr-app\n  template:\n    metadata:\n      labels:\n        app: sotr-app\n        env: qa\n    spec:\n      containers:\n      - image: wsosnowski2/sotr\n        name: sotr-project\n        ports:\n        - containerPort: 8010\n          protocol: TCP\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"sotr-project\" has memory limit 0"
  },
  {
    "id": "1149",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"akstest-3225\" is using an invalid container image, \"aaaatiwarishubregistry.azurecr.io/akstest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1150",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1151",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"akstest-3225\" does not have a read-only root file system"
  },
  {
    "id": "1152",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"akstest-3225\" is not set to runAsNonRoot"
  },
  {
    "id": "1153",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"akstest-3225\" has cpu request 0"
  },
  {
    "id": "1154",
    "manifest_path": "data/manifests/the_stack_sample/sample_0122.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: akstest-3225\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: akstest-3225\n  template:\n    metadata:\n      labels:\n        app: akstest-3225\n    spec:\n      containers:\n      - name: akstest-3225\n        image: aaaatiwarishubregistry.azurecr.io/akstest\n        ports:\n        - containerPort: 8123\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"akstest-3225\" has memory limit 0"
  },
  {
    "id": "1155",
    "manifest_path": "data/manifests/the_stack_sample/sample_0125.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: music-server\n  namespace: mytunes\n  labels:\n    name: music-db\nspec:\n  selector:\n    name: music-db\n  ports:\n  - name: db\n    port: 9200\n    targetPort: es\n  type: LoadBalancer\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[name:music-db])"
  },
  {
    "id": "1156",
    "manifest_path": "data/manifests/the_stack_sample/sample_0128.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    app: hello\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello\n        image: gcr.io/google-samples/hello-app:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        resources:\n          requests:\n            memory: 61Mi\n            cpu: 200m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1157",
    "manifest_path": "data/manifests/the_stack_sample/sample_0128.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    app: hello\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello\n        image: gcr.io/google-samples/hello-app:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        resources:\n          requests:\n            memory: 61Mi\n            cpu: 200m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"hello\" does not have a read-only root file system"
  },
  {
    "id": "1158",
    "manifest_path": "data/manifests/the_stack_sample/sample_0128.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hello-deployment\n  labels:\n    app: hello\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: hello\n  template:\n    metadata:\n      labels:\n        app: hello\n    spec:\n      containers:\n      - name: hello\n        image: gcr.io/google-samples/hello-app:1.0\n        ports:\n        - containerPort: 8080\n        env:\n        - name: MYSQL_ROOT_PASSWORD\n          value: password\n        resources:\n          requests:\n            memory: 61Mi\n            cpu: 200m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"hello\" is not set to runAsNonRoot"
  },
  {
    "id": "1159",
    "manifest_path": "data/manifests/the_stack_sample/sample_0129.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: phpmyadmin-service\nspec:\n  type: NodePort\n  selector:\n    app: phpmyadmin\n  ports:\n  - protocol: TCP\n    port: 80\n    targetPort: 80\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:phpmyadmin])"
  },
  {
    "id": "1160",
    "manifest_path": "data/manifests/the_stack_sample/sample_0131.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.1.41\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: cca6b2f42d7393045e9e749df2be9e7f342f64b4c723a82c3c7c4d0cb91793f6\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 5e6bccde9c3bac58bf04598ba5c0d440ede72b84637d58a98043893d7122417d\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.1.41\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: OHHYUN\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.41\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 974b52c387d1fabee626da3b78536faf4c7848e9\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 180\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"lighthouse-webhooks\" does not have a read-only root file system"
  },
  {
    "id": "1161",
    "manifest_path": "data/manifests/the_stack_sample/sample_0131.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.1.41\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: cca6b2f42d7393045e9e749df2be9e7f342f64b4c723a82c3c7c4d0cb91793f6\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 5e6bccde9c3bac58bf04598ba5c0d440ede72b84637d58a98043893d7122417d\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.1.41\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: OHHYUN\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.41\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 974b52c387d1fabee626da3b78536faf4c7848e9\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 180\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"lighthouse-webhooks\" not found"
  },
  {
    "id": "1162",
    "manifest_path": "data/manifests/the_stack_sample/sample_0131.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-webhooks\n  labels:\n    chart: lighthouse-1.1.41\n    app: lighthouse-webhooks\n    git.jenkins-x.io/sha: annotate\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    checksum/config: cca6b2f42d7393045e9e749df2be9e7f342f64b4c723a82c3c7c4d0cb91793f6\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-webhooks\n  template:\n    metadata:\n      labels:\n        app: lighthouse-webhooks\n      annotations:\n        prometheus.io/port: '2112'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 5e6bccde9c3bac58bf04598ba5c0d440ede72b84637d58a98043893d7122417d\n    spec:\n      serviceAccountName: lighthouse-webhooks\n      containers:\n      - name: lighthouse-webhooks\n        image: ghcr.io/jenkins-x/lighthouse-webhooks:1.1.41\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: LH_CUSTOM_TRIGGER_COMMAND\n          value: jx\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: OHHYUN\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.41\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 974b52c387d1fabee626da3b78536faf4c7848e9\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        ports:\n        - containerPort: 8080\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          initialDelaySeconds: 60\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 8080\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 100m\n            memory: 512Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 180\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"lighthouse-webhooks\" is not set to runAsNonRoot"
  },
  {
    "id": "1164",
    "manifest_path": "data/manifests/the_stack_sample/sample_0132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  revisionHistoryLimit: 4\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n      terminationGracePeriodSeconds: 10\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"manager\" does not have a read-only root file system"
  },
  {
    "id": "1165",
    "manifest_path": "data/manifests/the_stack_sample/sample_0132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  revisionHistoryLimit: 4\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n      terminationGracePeriodSeconds: 10\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"manager\" is not set to runAsNonRoot"
  },
  {
    "id": "1166",
    "manifest_path": "data/manifests/the_stack_sample/sample_0132.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hive-controllers\n  namespace: hive\n  labels:\n    control-plane: controller-manager\n    controller-tools.k8s.io: '1.0'\nspec:\n  selector:\n    matchLabels:\n      control-plane: controller-manager\n      controller-tools.k8s.io: '1.0'\n  replicas: 1\n  revisionHistoryLimit: 4\n  template:\n    metadata:\n      labels:\n        control-plane: controller-manager\n        controller-tools.k8s.io: '1.0'\n    spec:\n      serviceAccountName: default\n      volumes:\n      - name: kubectl-cache\n        emptyDir: {}\n      containers:\n      - image: registry.svc.ci.openshift.org/openshift/hive-v4.0:hive\n        imagePullPolicy: Always\n        name: manager\n        resources:\n          requests:\n            cpu: 50m\n            memory: 512Mi\n        command:\n        - /opt/services/manager\n        volumeMounts:\n        - name: kubectl-cache\n          mountPath: /var/cache/kubectl\n        env:\n        - name: CLI_CACHE_DIR\n          value: /var/cache/kubectl\n        livenessProbe:\n          httpGet:\n            path: /debug/health\n            port: 8080\n          initialDelaySeconds: 10\n          failureThreshold: 1\n          periodSeconds: 10\n      terminationGracePeriodSeconds: 10\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"manager\" has memory limit 0"
  },
  {
    "id": "1167",
    "manifest_path": "data/manifests/the_stack_sample/sample_0133.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: sugampipelinesjavascriptdocker\nspec:\n  type: LoadBalancer\n  ports:\n  - port: 8080\n  selector:\n    app: sugampipelinesjavascriptdocker\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:sugampipelinesjavascriptdocker])"
  },
  {
    "id": "1168",
    "manifest_path": "data/manifests/the_stack_sample/sample_0134.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: bookinfo-uat-ratings\n  namespace: student063-bookinfo-uat\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8080\n  selector:\n    app: bookinfo-uat-ratings\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:bookinfo-uat-ratings])"
  },
  {
    "id": "1169",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"api-database\" is using an invalid container image, \"postgres:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1170",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"pgsql-data-permission-fix\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1171",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"api-database\" does not have a read-only root file system"
  },
  {
    "id": "1172",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"pgsql-data-permission-fix\" does not have a read-only root file system"
  },
  {
    "id": "1173",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"api-database\" is not set to runAsNonRoot"
  },
  {
    "id": "1174",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"pgsql-data-permission-fix\" is not set to runAsNonRoot"
  },
  {
    "id": "1175",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"api-database\" has cpu request 0"
  },
  {
    "id": "1176",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pgsql-data-permission-fix\" has cpu request 0"
  },
  {
    "id": "1177",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"api-database\" has memory limit 0"
  },
  {
    "id": "1178",
    "manifest_path": "data/manifests/the_stack_sample/sample_0138.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: api-db\nspec:\n  containers:\n  - name: api-database\n    image: postgres:latest\n    env:\n    - name: POSTGRES_PASSWORD\n      valueFrom:\n        secretKeyRef:\n          name: api-secret\n          key: POSTGRES_PASSWORD\n    volumeMounts:\n    - mountPath: /var/lib/postgresql\n      name: api-data\n  initContainers:\n  - name: pgsql-data-permission-fix\n    image: busybox\n    command:\n    - /bin/chmod\n    - -R\n    - '777'\n    - /data\n    volumeMounts:\n    - name: api-data\n      mountPath: /data\n  volumes:\n  - name: api-data\n    persistentVolumeClaim:\n      claimName: api-pvc\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"pgsql-data-permission-fix\" has memory limit 0"
  },
  {
    "id": "1179",
    "manifest_path": "data/manifests/the_stack_sample/sample_0139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 60461a69ffa09bc8e76a211946cab1220740169ece13144fe59417d2d20f18ab\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard.jx.change.me\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 9e58caa93ba0a07182b4512285c1b1d01a279995\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 180\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"lighthouse-tekton-controller\" does not have a read-only root file system"
  },
  {
    "id": "1180",
    "manifest_path": "data/manifests/the_stack_sample/sample_0139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 60461a69ffa09bc8e76a211946cab1220740169ece13144fe59417d2d20f18ab\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard.jx.change.me\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 9e58caa93ba0a07182b4512285c1b1d01a279995\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 180\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"lighthouse-tekton-controller\" not found"
  },
  {
    "id": "1181",
    "manifest_path": "data/manifests/the_stack_sample/sample_0139.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-tekton-controller\n  labels:\n    chart: lighthouse-1.5.7\n    app: lighthouse-tekton-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-tekton-controller\n  template:\n    metadata:\n      labels:\n        app: lighthouse-tekton-controller\n      annotations:\n        jenkins-x.io/hash: 60461a69ffa09bc8e76a211946cab1220740169ece13144fe59417d2d20f18ab\n    spec:\n      serviceAccountName: lighthouse-tekton-controller\n      containers:\n      - name: lighthouse-tekton-controller\n        image: ghcr.io/jenkins-x/lighthouse-tekton-controller:1.5.7\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        - --dashboard-url=http://dashboard.jx.change.me\n        - --dashboard-template=namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun\n          }}\n        ports:\n        - name: metrics\n          containerPort: 8080\n        env:\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.5.7\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 9e58caa93ba0a07182b4512285c1b1d01a279995\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n      terminationGracePeriodSeconds: 180\n      nodeSelector: {}\n      affinity: {}\n      tolerations: []\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"lighthouse-tekton-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "1182",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"sinker\" does not have a read-only root file system"
  },
  {
    "id": "1183",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"sinker\" not found"
  },
  {
    "id": "1184",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"sinker\" is not set to runAsNonRoot"
  },
  {
    "id": "1185",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"sinker\" has cpu request 0"
  },
  {
    "id": "1186",
    "manifest_path": "data/manifests/the_stack_sample/sample_0140.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: sinker\n  labels:\n    app: sinker\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sinker\n  template:\n    metadata:\n      labels:\n        app: sinker\n    spec:\n      serviceAccountName: sinker\n      containers:\n      - name: sinker\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        image: gcr.io/k8s-prow/sinker:v20200628-cc1c099dad\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"sinker\" has memory limit 0"
  },
  {
    "id": "1187",
    "manifest_path": "data/manifests/the_stack_sample/sample_0142.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: nacos-svc\n  namespace: default\n  labels:\n    app: nacos-svc\nspec:\n  type: ClusterIP\n  ports:\n  - port: 8848\n    name: server\n    targetPort: 8848\n  selector:\n    app: nacos\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:nacos])"
  },
  {
    "id": "1188",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "1189",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"tide\" not found"
  },
  {
    "id": "1190",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "1191",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "1192",
    "manifest_path": "data/manifests/the_stack_sample/sample_0145.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  strategy:\n    type: Recreate\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20200225-55a0aacbd\n        args:\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --history-uri=gs://k8s-prow/tide-history.json\n        - --status-path=gs://k8s-prow/tide-status-checkpoint.yaml\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "1193",
    "manifest_path": "data/manifests/the_stack_sample/sample_0149.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: openldap\n  labels:\n    app: openldap\nspec:\n  selector:\n    app: openldap\n  type: ClusterIP\n  ports:\n  - name: notsecure\n    port: 389\n    targetPort: 389\n  - name: secure\n    port: 636\n    targetPort: 636\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:openldap])"
  },
  {
    "id": "1194",
    "manifest_path": "data/manifests/the_stack_sample/sample_0150.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  labels:\n    control-plane: controller-manager\n  name: metrics-service\n  namespace: system\nspec:\n  ports:\n  - name: metrics\n    port: 8080\n    targetPort: 8080\n  selector:\n    control-plane: controller-manager\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[control-plane:controller-manager])"
  },
  {
    "id": "1195",
    "manifest_path": "data/manifests/the_stack_sample/sample_0153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "1196",
    "manifest_path": "data/manifests/the_stack_sample/sample_0153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "1197",
    "manifest_path": "data/manifests/the_stack_sample/sample_0153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "1198",
    "manifest_path": "data/manifests/the_stack_sample/sample_0153.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: my-pod\nspec:\n  containers:\n  - name: busybox\n    image: busybox:1.34.1\n    command:\n    - /bin/sh\n    - -c\n    - echo $MY_ENV_VAR && sleep 99d\n    env:\n    - name: MY_ENV_VAR\n      valueFrom:\n        configMapKeyRef:\n          key: a-key\n          name: a-configmap\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "1199",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"busybox\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1200",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"busybox\" does not have a read-only root file system"
  },
  {
    "id": "1201",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"busybox\" is not set to runAsNonRoot"
  },
  {
    "id": "1202",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"busybox\" has cpu request 0"
  },
  {
    "id": "1203",
    "manifest_path": "data/manifests/the_stack_sample/sample_0154.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dns-test\n  namespace: kube-system\nspec:\n  containers:\n  - image: busybox\n    command:\n    - sleep\n    - '3600'\n    imagePullPolicy: IfNotPresent\n    name: busybox\n  restartPolicy: Always\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"busybox\" has memory limit 0"
  },
  {
    "id": "1204",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "1205",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"statusreconciler\" not found"
  },
  {
    "id": "1206",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "1207",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "1208",
    "manifest_path": "data/manifests/the_stack_sample/sample_0155.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      serviceAccountName: statusreconciler\n      terminationGracePeriodSeconds: 180\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20211014-7ca1952a94\n        imagePullPolicy: Always\n        args:\n        - --dry-run=false\n        - --continue-on-error=true\n        - --plugin-config=/etc/plugins/plugins.yaml\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --job-config-path=/etc/job-config\n        - --denylist=kubernetes/kubernetes\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "1209",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1210",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1211",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1212",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1213",
    "manifest_path": "data/manifests/the_stack_sample/sample_0156.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-5718\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1214",
    "manifest_path": "data/manifests/the_stack_sample/sample_0157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  restartPolicy: Never\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"deis-slugrunner\" does not have a read-only root file system"
  },
  {
    "id": "1215",
    "manifest_path": "data/manifests/the_stack_sample/sample_0157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  restartPolicy: Never\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"deis-slugrunner\" is not set to runAsNonRoot"
  },
  {
    "id": "1216",
    "manifest_path": "data/manifests/the_stack_sample/sample_0157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  restartPolicy: Never\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"deis-slugrunner\" has cpu request 0"
  },
  {
    "id": "1217",
    "manifest_path": "data/manifests/the_stack_sample/sample_0157.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: deis-slugrunner\n  labels:\n    heritage: deis\n    version: 2015-sept\nspec:\n  restartPolicy: Never\n  containers:\n  - name: deis-slugrunner\n    imagePullPolicy: Always\n    image: smothiki/slugrunner:4\n    args:\n    - start\n    - web\n    env:\n    - name: PORT\n      value: '5000'\n    - name: DEBUG\n      value: '1'\n    - name: SLUG_URL\n      value: https://s3.amazonaws.com/mydaffa/myslug.tgz\n    volumeMounts:\n    - name: object-store\n      mountPath: /var/run/secrets/object/store\n      readOnly: true\n  volumes:\n  - name: object-store\n    secret:\n      secretName: object-store\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"deis-slugrunner\" has memory limit 0"
  },
  {
    "id": "1218",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 3 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1219",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"go-server\" does not have a read-only root file system"
  },
  {
    "id": "1220",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"go-server\" is not set to runAsNonRoot"
  },
  {
    "id": "1221",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"go-server\" has cpu request 0"
  },
  {
    "id": "1222",
    "manifest_path": "data/manifests/the_stack_sample/sample_0158.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: go-server-rc\nspec:\n  replicas: 3\n  selector:\n    name: go-server\n    version: v8\n  template:\n    metadata:\n      labels:\n        name: go-server\n        version: v8\n    spec:\n      containers:\n      - name: go-server\n        image: gcr.io/pragmatic-mote-207921/amal-img:46d49ab\n        ports:\n        - containerPort: 5000\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"go-server\" has memory limit 0"
  },
  {
    "id": "1224",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"vamp-cli-ee\" is using an invalid container image, \"vlesierse/vamp-cli-ee:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1225",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"docker\" does not have a read-only root file system"
  },
  {
    "id": "1226",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"vamp-cli-ee\" does not have a read-only root file system"
  },
  {
    "id": "1227",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"docker\" is not set to runAsNonRoot"
  },
  {
    "id": "1228",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"vamp-cli-ee\" is not set to runAsNonRoot"
  },
  {
    "id": "1229",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"docker\" has cpu request 0"
  },
  {
    "id": "1230",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"vamp-cli-ee\" has cpu request 0"
  },
  {
    "id": "1231",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"docker\" has memory limit 0"
  },
  {
    "id": "1232",
    "manifest_path": "data/manifests/the_stack_sample/sample_0162.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nspec:\n  containers:\n  - name: docker\n    image: docker:stable-git\n    command:\n    - cat\n    tty: true\n    volumeMounts:\n    - name: dockersock\n      mountPath: /var/run/docker.sock\n  - name: vamp-cli-ee\n    image: vlesierse/vamp-cli-ee:latest\n    command:\n    - cat\n    tty: true\n  volumes:\n  - name: dockersock\n    hostPath:\n      path: /var/run/docker.sock\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"vamp-cli-ee\" has memory limit 0"
  },
  {
    "id": "1233",
    "manifest_path": "data/manifests/the_stack_sample/sample_0163.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 96b3634b43df2329d5c73888957b70798af1e33b9bc37b02d5c340ae6962f9cd\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: Sun-ifly\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"lighthouse-keeper\" does not have a read-only root file system"
  },
  {
    "id": "1234",
    "manifest_path": "data/manifests/the_stack_sample/sample_0163.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 96b3634b43df2329d5c73888957b70798af1e33b9bc37b02d5c340ae6962f9cd\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: Sun-ifly\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "non_existent_service_account",
    "violation_text": "serviceAccount \"lighthouse-keeper\" not found"
  },
  {
    "id": "1235",
    "manifest_path": "data/manifests/the_stack_sample/sample_0163.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-keeper\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-keeper\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  strategy:\n    type: RollingUpdate\n    rollingUpdate:\n      maxSurge: 1\n      maxUnavailable: 1\n  selector:\n    matchLabels:\n      app: lighthouse-keeper\n  template:\n    metadata:\n      annotations:\n        ad.datadoghq.com/keeper.logs: '[{\"source\":\"lighthouse\",\"service\":\"keeper\"}]'\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        jenkins-x.io/hash: 96b3634b43df2329d5c73888957b70798af1e33b9bc37b02d5c340ae6962f9cd\n      labels:\n        app: lighthouse-keeper\n    spec:\n      serviceAccountName: lighthouse-keeper\n      terminationGracePeriodSeconds: 30\n      containers:\n      - name: lighthouse-keeper\n        image: ghcr.io/jenkins-x/lighthouse-keeper:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        ports:\n        - name: http\n          containerPort: 8888\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /\n            port: http\n          initialDelaySeconds: 120\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        readinessProbe:\n          httpGet:\n            path: /\n            port: http\n          periodSeconds: 10\n          successThreshold: 1\n          timeoutSeconds: 1\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: Sun-ifly\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: LIGHTHOUSE_KEEPER_STATUS_CONTEXT_LABEL\n          value: Lighthouse Merge Status\n        - name: LIGHTHOUSE_TRIGGER_ON_MISSING\n          value: enable\n        envFrom:\n        - secretRef:\n            name: jx-boot-job-env-vars\n            optional: true\n        resources:\n          limits:\n            cpu: 400m\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 128Mi\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"lighthouse-keeper\" is not set to runAsNonRoot"
  },
  {
    "id": "1236",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 2 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1237",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"flask\" does not have a read-only root file system"
  },
  {
    "id": "1238",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"flask\" is not set to runAsNonRoot"
  },
  {
    "id": "1239",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"flask\" has cpu request 0"
  },
  {
    "id": "1240",
    "manifest_path": "data/manifests/the_stack_sample/sample_0164.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: flask-dep\n  labels:\n    app: flask-helloworld\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: flask-helloworld\n  template:\n    metadata:\n      labels:\n        app: flask-helloworld\n    spec:\n      containers:\n      - name: flask\n        image: grimsleepless/workshop_flask:v1\n        ports:\n        - containerPort: 5000\n        env:\n        - name: REDIS_HOST\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: host\n        - name: REDIS_PORT\n          valueFrom:\n            configMapKeyRef:\n              name: redis-config\n              key: port\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"flask\" has memory limit 0"
  },
  {
    "id": "1241",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"mycron-container\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1242",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"mycron-container\" does not have a read-only root file system"
  },
  {
    "id": "1243",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"mycron-container\" is not set to runAsNonRoot"
  },
  {
    "id": "1244",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mycron-container\" has cpu request 0"
  },
  {
    "id": "1245",
    "manifest_path": "data/manifests/the_stack_sample/sample_0170.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: mycronjob\nspec:\n  schedule: '*/1 * * * *'\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: mycron-container\n            image: alpine\n            imagePullPolicy: IfNotPresent\n            command:\n            - sh\n            - -c\n            - echo elastic world ; sleep 5\n          restartPolicy: OnFailure\n          terminationGracePeriodSeconds: 0\n  concurrencyPolicy: Allow\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"mycron-container\" has memory limit 0"
  },
  {
    "id": "1246",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"ecsdemo-nodejs\" is using an invalid container image, \"brentley/ecsdemo-nodejs:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1247",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "no_anti_affinity",
    "violation_text": "object has 6 replicas but does not specify inter pod anti-affinity"
  },
  {
    "id": "1248",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"ecsdemo-nodejs\" does not have a read-only root file system"
  },
  {
    "id": "1249",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"ecsdemo-nodejs\" is not set to runAsNonRoot"
  },
  {
    "id": "1250",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"ecsdemo-nodejs\" has cpu request 0"
  },
  {
    "id": "1251",
    "manifest_path": "data/manifests/the_stack_sample/sample_0172.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ecsdemo-nodejs\n  labels:\n    app: ecsdemo-nodejs\n  namespace: ecsdemo-nodejs\nspec:\n  replicas: 6\n  selector:\n    matchLabels:\n      app: ecsdemo-nodejs\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 25%\n    type: RollingUpdate\n  template:\n    metadata:\n      labels:\n        app: ecsdemo-nodejs\n    spec:\n      containers:\n      - image: brentley/ecsdemo-nodejs:latest\n        imagePullPolicy: Always\n        name: ecsdemo-nodejs\n        ports:\n        - containerPort: 3000\n          protocol: TCP\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"ecsdemo-nodejs\" has memory limit 0"
  },
  {
    "id": "1252",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"zookeeper\" is using an invalid container image, \"zcguan/storm-cluster:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1253",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"zookeeper\" does not have a read-only root file system"
  },
  {
    "id": "1254",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"zookeeper\" is not set to runAsNonRoot"
  },
  {
    "id": "1255",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"zookeeper\" has cpu request 0"
  },
  {
    "id": "1256",
    "manifest_path": "data/manifests/the_stack_sample/sample_0173.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: apache-storm-nimbus\nspec:\n  replicas: 1\n  selector:\n    storm: nimbus\n  template:\n    metadata:\n      labels:\n        storm: nimbus\n        app: storm\n    spec:\n      containers:\n      - name: zookeeper\n        image: zcguan/storm-cluster:latest\n        env:\n        - name: CONFIGURE_ZOOKEEPER\n          value: 'true'\n        - name: STORM_CMD\n          value: nimbus\n        volumeMounts:\n        - name: storm-data\n          mountPath: /opt/apache-storm/storm-local\n      volumes:\n      - name: storm-data\n        emptyDir: {}\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"zookeeper\" has memory limit 0"
  },
  {
    "id": "1259",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  schedule: 0 2 * * *\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: git\n            env:\n            - name: GIT_REPOSITORY\n              value: github.com/tektoncd/plumbing\n            - name: GIT_REVISION\n              value: main\n          containers:\n          - name: trigger\n            env:\n            - name: SINK_URL\n              value: el-image-builder.default.svc.cluster.local:8080\n            - name: TARGET_IMAGE\n              value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n            - name: CONTEXT_PATH\n              value: tekton/images/buildx-gcloud\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"git\" does not have a read-only root file system"
  },
  {
    "id": "1260",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  schedule: 0 2 * * *\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: git\n            env:\n            - name: GIT_REPOSITORY\n              value: github.com/tektoncd/plumbing\n            - name: GIT_REVISION\n              value: main\n          containers:\n          - name: trigger\n            env:\n            - name: SINK_URL\n              value: el-image-builder.default.svc.cluster.local:8080\n            - name: TARGET_IMAGE\n              value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n            - name: CONTEXT_PATH\n              value: tekton/images/buildx-gcloud\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"trigger\" does not have a read-only root file system"
  },
  {
    "id": "1261",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  schedule: 0 2 * * *\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: git\n            env:\n            - name: GIT_REPOSITORY\n              value: github.com/tektoncd/plumbing\n            - name: GIT_REVISION\n              value: main\n          containers:\n          - name: trigger\n            env:\n            - name: SINK_URL\n              value: el-image-builder.default.svc.cluster.local:8080\n            - name: TARGET_IMAGE\n              value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n            - name: CONTEXT_PATH\n              value: tekton/images/buildx-gcloud\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"git\" is not set to runAsNonRoot"
  },
  {
    "id": "1262",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  schedule: 0 2 * * *\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: git\n            env:\n            - name: GIT_REPOSITORY\n              value: github.com/tektoncd/plumbing\n            - name: GIT_REVISION\n              value: main\n          containers:\n          - name: trigger\n            env:\n            - name: SINK_URL\n              value: el-image-builder.default.svc.cluster.local:8080\n            - name: TARGET_IMAGE\n              value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n            - name: CONTEXT_PATH\n              value: tekton/images/buildx-gcloud\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"trigger\" is not set to runAsNonRoot"
  },
  {
    "id": "1263",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  schedule: 0 2 * * *\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: git\n            env:\n            - name: GIT_REPOSITORY\n              value: github.com/tektoncd/plumbing\n            - name: GIT_REVISION\n              value: main\n          containers:\n          - name: trigger\n            env:\n            - name: SINK_URL\n              value: el-image-builder.default.svc.cluster.local:8080\n            - name: TARGET_IMAGE\n              value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n            - name: CONTEXT_PATH\n              value: tekton/images/buildx-gcloud\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"git\" has cpu request 0"
  },
  {
    "id": "1264",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  schedule: 0 2 * * *\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: git\n            env:\n            - name: GIT_REPOSITORY\n              value: github.com/tektoncd/plumbing\n            - name: GIT_REVISION\n              value: main\n          containers:\n          - name: trigger\n            env:\n            - name: SINK_URL\n              value: el-image-builder.default.svc.cluster.local:8080\n            - name: TARGET_IMAGE\n              value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n            - name: CONTEXT_PATH\n              value: tekton/images/buildx-gcloud\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"trigger\" has cpu request 0"
  },
  {
    "id": "1265",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  schedule: 0 2 * * *\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: git\n            env:\n            - name: GIT_REPOSITORY\n              value: github.com/tektoncd/plumbing\n            - name: GIT_REVISION\n              value: main\n          containers:\n          - name: trigger\n            env:\n            - name: SINK_URL\n              value: el-image-builder.default.svc.cluster.local:8080\n            - name: TARGET_IMAGE\n              value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n            - name: CONTEXT_PATH\n              value: tekton/images/buildx-gcloud\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"git\" has memory limit 0"
  },
  {
    "id": "1266",
    "manifest_path": "data/manifests/the_stack_sample/sample_0176.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: image-build-cron-trigger\nspec:\n  schedule: 0 2 * * *\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          initContainers:\n          - name: git\n            env:\n            - name: GIT_REPOSITORY\n              value: github.com/tektoncd/plumbing\n            - name: GIT_REVISION\n              value: main\n          containers:\n          - name: trigger\n            env:\n            - name: SINK_URL\n              value: el-image-builder.default.svc.cluster.local:8080\n            - name: TARGET_IMAGE\n              value: gcr.io/tekton-releases/dogfooding/buildx-gcloud\n            - name: CONTEXT_PATH\n              value: tekton/images/buildx-gcloud\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"trigger\" has memory limit 0"
  },
  {
    "id": "1267",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"k8s-demo\" is using an invalid container image, \"diyblockchain/k8s-demo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1268",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"k8s-demo\" does not have a read-only root file system"
  },
  {
    "id": "1269",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"k8s-demo\" is not set to runAsNonRoot"
  },
  {
    "id": "1270",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"k8s-demo\" has cpu request 0"
  },
  {
    "id": "1271",
    "manifest_path": "data/manifests/the_stack_sample/sample_0178.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nodehelloworld.example.com\n  labels:\n    app: helloworld\nspec:\n  containers:\n  - name: k8s-demo\n    image: diyblockchain/k8s-demo\n    ports:\n    - name: nodejs-port\n      containerPort: 3000\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"k8s-demo\" has memory limit 0"
  },
  {
    "id": "1272",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1273",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1274",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1275",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1276",
    "manifest_path": "data/manifests/the_stack_sample/sample_0181.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-647\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1277",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "policy_id": "job_ttl_after_finished",
    "violation_text": "Standalone Job does not specify ttlSecondsAfterFinished"
  },
  {
    "id": "1278",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"image-seg\" is using an invalid container image, \"centaurusinfra/test1_maskrcnn\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1279",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"image-seg\" does not have a read-only root file system"
  },
  {
    "id": "1280",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"image-seg\" is not set to runAsNonRoot"
  },
  {
    "id": "1281",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"image-seg\" has cpu request 0"
  },
  {
    "id": "1282",
    "manifest_path": "data/manifests/the_stack_sample/sample_0183.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: maskrcnn-image-seg\nspec:\n  template:\n    spec:\n      containers:\n      - name: image-seg\n        image: centaurusinfra/test1_maskrcnn\n        command:\n        - python\n        args:\n        - -m\n        - torch.distributed.launch\n        - --nproc_per_node\n        - '2'\n        - tools/train_net.py\n        - --config-file\n        - configs/e2e_mask_rcnn_R_50_FPN_1x.yaml\n        - --max_steps\n        - '100'\n        - OUTPUT_DIR\n        - /results\n        resources:\n          limits:\n            nvidia.com/gpu: 2\n        volumeMounts:\n        - mountPath: /datasets/data\n          name: remote-data\n        - mountPath: /results\n          name: remote-results\n        - mountPath: /dev/shm\n          name: dshm\n      volumes:\n      - name: remote-data\n        hostPath:\n          path: /nfs_1/alnair/pytorch/segmentation/maskrcnn/coco_dataset/\n          type: Directory\n      - name: remote-results\n        hostPath:\n          path: /nfs_1/alnair/results\n          type: Directory\n      - name: dshm\n        emptyDir:\n          medium: Memory\n      restartPolicy: Never\n  backoffLimit: 0\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"image-seg\" has memory limit 0"
  },
  {
    "id": "1283",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1284",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "1285",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "1286",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "1287",
    "manifest_path": "data/manifests/the_stack_sample/sample_0184.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6685\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "1288",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no_host_ipc",
    "violation_text": "resource shares host's IPC namespace (via hostIPC=true)."
  },
  {
    "id": "1289",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"container1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1290",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"initcontainer1\" is using an invalid container image, \"k8s.gcr.io/pause\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1291",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"container1\" does not have a read-only root file system"
  },
  {
    "id": "1292",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"initcontainer1\" does not have a read-only root file system"
  },
  {
    "id": "1293",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"container1\" has cpu request 0"
  },
  {
    "id": "1294",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"initcontainer1\" has cpu request 0"
  },
  {
    "id": "1295",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"container1\" has memory limit 0"
  },
  {
    "id": "1296",
    "manifest_path": "data/manifests/the_stack_sample/sample_0189.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: hostnamespaces0\nspec:\n  containers:\n  - image: k8s.gcr.io/pause\n    name: container1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  hostIPC: true\n  initContainers:\n  - image: k8s.gcr.io/pause\n    name: initcontainer1\n    securityContext:\n      allowPrivilegeEscalation: false\n      capabilities:\n        drop:\n        - ALL\n  securityContext:\n    runAsNonRoot: true\n    seccompProfile:\n      type: RuntimeDefault\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"initcontainer1\" has memory limit 0"
  },
  {
    "id": "1297",
    "manifest_path": "data/manifests/the_stack_sample/sample_0190.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\nspec:\n  ports:\n  - port: 3000\n    protocol: TCP\n    targetPort: 3000\n  type: NodePort\n",
    "policy_id": "dangling_service",
    "violation_text": "service has no selector specified"
  },
  {
    "id": "1298",
    "manifest_path": "data/manifests/the_stack_sample/sample_0191.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  annotations:\n    xposer.stakater.com/annotations: 'kubernetes.io/ingress.class: external-ingress\n\n      ingress.kubernetes.io/force-ssl-redirect: true\n\n      exposeIngressUrl: globally'\n  labels:\n    app: hello-kubernetes\n    expose: 'true'\n  name: hello-kubernetes\nspec:\n  ports:\n  - name: http\n    port: 80\n    protocol: TCP\n    targetPort: 8080\n  selector:\n    app: hello-kubernetes\n  type: ClusterIP\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:hello-kubernetes])"
  },
  {
    "id": "1299",
    "manifest_path": "data/manifests/the_stack_sample/sample_0192.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: redis\nspec:\n  selector:\n    app: redis\n  ports:\n  - protocol: TCP\n    port: 6379\n    targetPort: 6379\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:redis])"
  },
  {
    "id": "1300",
    "manifest_path": "data/manifests/the_stack_sample/sample_0193.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Service\nmetadata:\n  name: azconf-app\n  namespace: azconf\nspec:\n  type: LoadBalancer\n  selector:\n    app: azconf-app\n  ports:\n  - port: 8080\n",
    "policy_id": "dangling_service",
    "violation_text": "no pods found matching service labels (map[app:azconf-app])"
  },
  {
    "id": "1301",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      restartPolicy: Always\n      automountServiceAccountToken: false\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"sabnzbd\" does not have a read-only root file system"
  },
  {
    "id": "1302",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      restartPolicy: Always\n      automountServiceAccountToken: false\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"sabnzbd\" is not set to runAsNonRoot"
  },
  {
    "id": "1303",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      restartPolicy: Always\n      automountServiceAccountToken: false\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"sabnzbd\" has cpu request 0"
  },
  {
    "id": "1304",
    "manifest_path": "data/manifests/the_stack_sample/sample_0195.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sabnzbd\n  namespace: usenet\n  annotations:\n    reloader.stakater.com/auto: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sabnzbd\n  strategy:\n    type: Recreate\n  template:\n    metadata:\n      labels:\n        app: sabnzbd\n    spec:\n      containers:\n      - name: sabnzbd\n        image: lscr.io/linuxserver/sabnzbd:3.4.2-ls51\n        env:\n        - name: PGID\n          value: '2000'\n        - name: PUID\n          value: '2000'\n        - name: TZ\n          value: Europe/Zurich\n        - name: DOCKER_MODS\n          value: containeroo/docker-mods:sabnzbd-mkvtoolnix|containeroo/docker-mods:sabnzbd-nzbnotify\n        ports:\n        - containerPort: 8080\n        volumeMounts:\n        - mountPath: /config\n          name: data\n        - mountPath: /incomplete-downloads\n          name: incomplete-downloads\n        - mountPath: /downloads\n          name: downloads\n        - mountPath: /app/sabnzbd/scripts/merge_subtitles.sh\n          name: merge-subtitles\n          subPath: merge_subtitles.sh\n        - mountPath: /tmp/startup_probe.sh\n          name: sabnzbd-startup\n          subPath: startup_probe.sh\n        startupProbe:\n          exec:\n            command:\n            - bash\n            - -c\n            - /tmp/startup_probe.sh\n          initialDelaySeconds: 30\n          failureThreshold: 30\n          timeoutSeconds: 10\n          periodSeconds: 5\n        livenessProbe:\n          tcpSocket:\n            port: 8080\n          failureThreshold: 2\n          timeoutSeconds: 5\n          periodSeconds: 30\n        resources: {}\n      restartPolicy: Always\n      automountServiceAccountToken: false\n      volumes:\n      - name: data\n        persistentVolumeClaim:\n          claimName: sabnzbd-data\n      - name: merge-subtitles\n        configMap:\n          name: merge-subtitles\n          items:\n          - key: merge_subtitles.sh\n            path: merge_subtitles.sh\n          defaultMode: 511\n      - name: sabnzbd-startup\n        configMap:\n          name: sabnzbd-startup\n          items:\n          - key: startup_probe.sh\n            path: startup_probe.sh\n          defaultMode: 511\n      - name: incomplete-downloads\n        hostPath:\n          path: /mnt/disk06/downloads/incomplete\n          type: Directory\n      - name: downloads\n        hostPath:\n          path: /mnt/disk06/downloads/complete\n          type: Directory\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"sabnzbd\" has memory limit 0"
  },
  {
    "id": "1305",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "no_latest_tag",
    "violation_text": "The container \"myapp-container\" is using an invalid container image, \"cewuandy/nextepc-base\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "1306",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"init-network-client\" does not have a read-only root file system"
  },
  {
    "id": "1307",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "read_only_root_fs",
    "violation_text": "container \"myapp-container\" does not have a read-only root file system"
  },
  {
    "id": "1308",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"init-network-client\" is not set to runAsNonRoot"
  },
  {
    "id": "1309",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "run_as_non_root",
    "violation_text": "container \"myapp-container\" is not set to runAsNonRoot"
  },
  {
    "id": "1310",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"init-network-client\" has cpu request 0"
  },
  {
    "id": "1311",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"myapp-container\" has cpu request 0"
  },
  {
    "id": "1312",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"init-network-client\" has memory limit 0"
  },
  {
    "id": "1313",
    "manifest_path": "data/manifests/the_stack_sample/sample_0196.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nextepc-mme-deployment\nspec:\n  selector:\n    matchLabels:\n      app: nextepc-mme\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nextepc-mme\n    spec:\n      containers:\n      - name: myapp-container\n        image: cewuandy/nextepc-base\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - /root/nextepc/nextepc-mmed\n        volumeMounts:\n        - name: nextepc-conf\n          mountPath: /root/nextepc/install/etc/nextepc/\n      initContainers:\n      - name: init-network-client\n        image: sdnvortex/network-controller:v0.4.9\n        command:\n        - /go/bin/client\n        args:\n        - -s=unix:///tmp/vortex.sock\n        - -b=br0\n        - -n=eth1\n        - -i=192.188.2.2/24\n        env:\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: POD_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_UUID\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.uid\n        volumeMounts:\n        - mountPath: /tmp/\n          name: grpc-sock\n      volumes:\n      - name: grpc-sock\n        hostPath:\n          path: /tmp/vortex/\n      - name: nextepc-conf\n        configMap:\n          name: nextepc-config\n",
    "policy_id": "set_requests_limits",
    "violation_text": "container \"myapp-container\" has memory limit 0"
  }
]