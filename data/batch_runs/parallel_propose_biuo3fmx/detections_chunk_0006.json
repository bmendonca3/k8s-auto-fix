[
  {
    "id": "03751",
    "manifest_path": "data/manifests/the_stack_sample/sample_1667.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vector-add\nspec:\n  containers:\n  - name: cuda-vector-add\n    image: k8s.gcr.io/cuda-vector-add:v0.1\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cuda-vector-add\" has memory limit 0"
  },
  {
    "id": "03752",
    "manifest_path": "data/manifests/the_stack_sample/sample_1671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  selector:\n    matchLabels:\n      app: plank\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      serviceAccountName: plank\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20200319-1aea24112\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/workload-clusters/config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/workload-clusters\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: cluster\n          mountPath: /etc/cluster\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: workload-clusters-kubeconfig\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: workload-cluster\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"plank\" does not have a read-only root file system"
  },
  {
    "id": "03753",
    "manifest_path": "data/manifests/the_stack_sample/sample_1671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  selector:\n    matchLabels:\n      app: plank\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      serviceAccountName: plank\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20200319-1aea24112\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/workload-clusters/config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/workload-clusters\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: cluster\n          mountPath: /etc/cluster\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: workload-clusters-kubeconfig\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: workload-cluster\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"plank\" is not set to runAsNonRoot"
  },
  {
    "id": "03754",
    "manifest_path": "data/manifests/the_stack_sample/sample_1671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  selector:\n    matchLabels:\n      app: plank\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      serviceAccountName: plank\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20200319-1aea24112\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/workload-clusters/config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/workload-clusters\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: cluster\n          mountPath: /etc/cluster\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: workload-clusters-kubeconfig\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: workload-cluster\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"plank\" has cpu request 0"
  },
  {
    "id": "03755",
    "manifest_path": "data/manifests/the_stack_sample/sample_1671.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: plank\n  labels:\n    app: plank\nspec:\n  selector:\n    matchLabels:\n      app: plank\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: plank\n    spec:\n      serviceAccountName: plank\n      containers:\n      - name: plank\n        image: gcr.io/k8s-prow/plank:v20200319-1aea24112\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/workload-clusters/config\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        volumeMounts:\n        - name: kubeconfig\n          mountPath: /etc/workload-clusters\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: cluster\n          mountPath: /etc/cluster\n          readOnly: true\n      volumes:\n      - name: kubeconfig\n        secret:\n          secretName: workload-clusters-kubeconfig\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: cluster\n        secret:\n          defaultMode: 420\n          secretName: workload-cluster\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"plank\" has memory limit 0"
  },
  {
    "id": "03756",
    "manifest_path": "data/manifests/the_stack_sample/sample_1672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zabbix-web\n  labels:\n    app: zabbix\n    tier: zabbix-web\n  namespace: zabbix\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: zabbix-web\n        app: zabbix\n    spec:\n      containers:\n      - name: zabbix-web\n        image: zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: web-http\n        - containerPort: 8443\n          name: web-https\n        env:\n        - name: ZBX_SERVER_NAME\n          value: Zabbix kubernetes\n        - name: PHP_TZ\n          value: Europe/Riga\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-user\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-pass\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-root-pass\n        - name: MYSQL_DATABASE\n          value: zabbix\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zabbix-web\" does not have a read-only root file system"
  },
  {
    "id": "03757",
    "manifest_path": "data/manifests/the_stack_sample/sample_1672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zabbix-web\n  labels:\n    app: zabbix\n    tier: zabbix-web\n  namespace: zabbix\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: zabbix-web\n        app: zabbix\n    spec:\n      containers:\n      - name: zabbix-web\n        image: zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: web-http\n        - containerPort: 8443\n          name: web-https\n        env:\n        - name: ZBX_SERVER_NAME\n          value: Zabbix kubernetes\n        - name: PHP_TZ\n          value: Europe/Riga\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-user\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-pass\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-root-pass\n        - name: MYSQL_DATABASE\n          value: zabbix\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zabbix-web\" is not set to runAsNonRoot"
  },
  {
    "id": "03758",
    "manifest_path": "data/manifests/the_stack_sample/sample_1672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zabbix-web\n  labels:\n    app: zabbix\n    tier: zabbix-web\n  namespace: zabbix\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: zabbix-web\n        app: zabbix\n    spec:\n      containers:\n      - name: zabbix-web\n        image: zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: web-http\n        - containerPort: 8443\n          name: web-https\n        env:\n        - name: ZBX_SERVER_NAME\n          value: Zabbix kubernetes\n        - name: PHP_TZ\n          value: Europe/Riga\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-user\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-pass\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-root-pass\n        - name: MYSQL_DATABASE\n          value: zabbix\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zabbix-web\" has cpu request 0"
  },
  {
    "id": "03759",
    "manifest_path": "data/manifests/the_stack_sample/sample_1672.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zabbix-web\n  labels:\n    app: zabbix\n    tier: zabbix-web\n  namespace: zabbix\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: zabbix-web\n        app: zabbix\n    spec:\n      containers:\n      - name: zabbix-web\n        image: zabbix/zabbix-web-nginx-mysql:alpine-5.0-latest\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n          name: web-http\n        - containerPort: 8443\n          name: web-https\n        env:\n        - name: ZBX_SERVER_NAME\n          value: Zabbix kubernetes\n        - name: PHP_TZ\n          value: Europe/Riga\n        - name: MYSQL_USER\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-user\n        - name: MYSQL_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-zbx-pass\n        - name: MYSQL_ROOT_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secret\n              key: db-root-pass\n        - name: MYSQL_DATABASE\n          value: zabbix\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"zabbix-web\" has memory limit 0"
  },
  {
    "id": "03760",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03761",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03762",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03763",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03764",
    "manifest_path": "data/manifests/the_stack_sample/sample_1673.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-1357\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03765",
    "manifest_path": "data/manifests/the_stack_sample/sample_1674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210514-64850e516d\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "03766",
    "manifest_path": "data/manifests/the_stack_sample/sample_1674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210514-64850e516d\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "03767",
    "manifest_path": "data/manifests/the_stack_sample/sample_1674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210514-64850e516d\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "03768",
    "manifest_path": "data/manifests/the_stack_sample/sample_1674.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: gcr.io/k8s-prow/tide:v20210514-64850e516d\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "03769",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03770",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03771",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03772",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03773",
    "manifest_path": "data/manifests/the_stack_sample/sample_1677.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03774",
    "manifest_path": "data/manifests/the_stack_sample/sample_1678.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210624-69a9f34cd9\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://fakeghserver\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "03775",
    "manifest_path": "data/manifests/the_stack_sample/sample_1678.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210624-69a9f34cd9\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://fakeghserver\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "03776",
    "manifest_path": "data/manifests/the_stack_sample/sample_1678.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210624-69a9f34cd9\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://fakeghserver\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "03777",
    "manifest_path": "data/manifests/the_stack_sample/sample_1678.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210624-69a9f34cd9\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://fakeghserver\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        env:\n        - name: KUBECONFIG\n          value: /etc/kubeconfig/config\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "03778",
    "manifest_path": "data/manifests/the_stack_sample/sample_1681.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-jx-controller\n  labels:\n    draft: draft-app\n    chart: lighthouse-0.0.683\n    app: lighthouse-jx-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\n    jenkins-x.io/hash: 02b017d8f4c2c6a9dff81ac769baa5da7d60fcc9fee59859671642d393c6f4e2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      draft: draft-app\n      app: lighthouse-jx-controller\n  template:\n    metadata:\n      labels:\n        draft: draft-app\n        app: lighthouse-jx-controller\n    spec:\n      serviceAccountName: lighthouse-jx-controller\n      containers:\n      - name: lighthouse-jx-controller\n        image: gcr.io/jenkinsxio/lighthouse-jx-controller:0.0.683\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: hnaung\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: JX_DEFAULT_IMAGE\n          value: gcr.io/jenkinsxio/builder-maven:2.1.142-761\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-jx-controller\" does not have a read-only root file system"
  },
  {
    "id": "03779",
    "manifest_path": "data/manifests/the_stack_sample/sample_1681.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-jx-controller\n  labels:\n    draft: draft-app\n    chart: lighthouse-0.0.683\n    app: lighthouse-jx-controller\n    gitops.jenkins-x.io/pipeline: namespaces\n  namespace: jx\n  annotations:\n    wave.pusher.com/update-on-config-change: 'true'\n    jenkins-x.io/hash: 02b017d8f4c2c6a9dff81ac769baa5da7d60fcc9fee59859671642d393c6f4e2\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      draft: draft-app\n      app: lighthouse-jx-controller\n  template:\n    metadata:\n      labels:\n        draft: draft-app\n        app: lighthouse-jx-controller\n    spec:\n      serviceAccountName: lighthouse-jx-controller\n      containers:\n      - name: lighthouse-jx-controller\n        image: gcr.io/jenkinsxio/lighthouse-jx-controller:0.0.683\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: hnaung\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: JX_DEFAULT_IMAGE\n          value: gcr.io/jenkinsxio/builder-maven:2.1.142-761\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-jx-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "03780",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"redis\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03781",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "03782",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "03783",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "03784",
    "manifest_path": "data/manifests/the_stack_sample/sample_1688.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: redis\n  labels:\n    app: redis\n    visualize: 'true'\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: redis\n  template:\n    metadata:\n      labels:\n        app: redis\n        visualize: 'true'\n    spec:\n      containers:\n      - name: redis\n        image: redis\n        ports:\n        - name: redis-server\n          containerPort: 6379\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "03785",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"example-noobaa-data\" is using an invalid container image, \"yiannisgkoufas/awscli-alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03786",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"example-noobaa-data\" does not have a read-only root file system"
  },
  {
    "id": "03787",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"example-noobaa-data\" is not set to runAsNonRoot"
  },
  {
    "id": "03788",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"example-noobaa-data\" has cpu request 0"
  },
  {
    "id": "03789",
    "manifest_path": "data/manifests/the_stack_sample/sample_1689.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-noobaa-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: example-noobaa-data\n        image: yiannisgkoufas/awscli-alpine\n        imagePullPolicy: Always\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 10 && export AWS_ACCESS_KEY_ID={KEY_ID} && export AWS_SECRET_ACCESS_KEY={ACCESS_KEY}\n          && echo ''hello'' > file1.txt && echo ''world'' > file2.txt && aws --no-verify-ssl\n          --endpoint https://s3.default.svc:443 s3 cp file1.txt s3://{BUCKET} && aws\n          --no-verify-ssl --endpoint https://s3.default.svc:443 s3 cp file2.txt s3://{BUCKET} '\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"example-noobaa-data\" has memory limit 0"
  },
  {
    "id": "03790",
    "manifest_path": "data/manifests/the_stack_sample/sample_1690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: ticommunityinfra/tide:v20220130-v1.0.4\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tide\" does not have a read-only root file system"
  },
  {
    "id": "03791",
    "manifest_path": "data/manifests/the_stack_sample/sample_1690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: ticommunityinfra/tide:v20220130-v1.0.4\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tide\" is not set to runAsNonRoot"
  },
  {
    "id": "03792",
    "manifest_path": "data/manifests/the_stack_sample/sample_1690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: ticommunityinfra/tide:v20220130-v1.0.4\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tide\" has cpu request 0"
  },
  {
    "id": "03793",
    "manifest_path": "data/manifests/the_stack_sample/sample_1690.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: prow\n  name: tide\n  labels:\n    app: tide\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: tide\n  template:\n    metadata:\n      labels:\n        app: tide\n    spec:\n      serviceAccountName: tide\n      containers:\n      - name: tide\n        image: ticommunityinfra/tide:v20220130-v1.0.4\n        args:\n        - --dry-run=false\n        - --config-path=/etc/config/config.yaml\n        - --github-token-path=/etc/github/token\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-graphql-endpoint=http://ghproxy/graphql\n        - --gcs-credentials-file=/etc/gcs-credentials/service-account.json\n        - --status-path=gs://prow-tidb-logs/tide-status\n        - --history-uri=gs://prow-tidb-logs/tide-history.json\n        - --job-config-path=/etc/job-config\n        ports:\n        - name: http\n          containerPort: 8888\n        volumeMounts:\n        - name: github-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: gcs-credentials\n          mountPath: /etc/gcs-credentials\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: github-token\n        secret:\n          secretName: github-token\n      - name: config\n        configMap:\n          name: config\n      - name: gcs-credentials\n        secret:\n          secretName: gcs-credentials\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tide\" has memory limit 0"
  },
  {
    "id": "03794",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"env\" is using an invalid container image, \"nicholasdille/sleeper\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03795",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"env\" does not have a read-only root file system"
  },
  {
    "id": "03796",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"env\" is not set to runAsNonRoot"
  },
  {
    "id": "03797",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"env\" has cpu request 0"
  },
  {
    "id": "03798",
    "manifest_path": "data/manifests/the_stack_sample/sample_1691.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: env\nspec:\n  containers:\n  - name: env\n    image: nicholasdille/sleeper\n    env:\n    - name: ENV_SECRET\n      valueFrom:\n        configMapKeyRef:\n          name: config\n          key: foo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"env\" has memory limit 0"
  },
  {
    "id": "03799",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"openliberty\" does not have a read-only root file system"
  },
  {
    "id": "03800",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"openliberty\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03801",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"openliberty\" is privileged"
  },
  {
    "id": "03802",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"openliberty\" is not set to runAsNonRoot"
  },
  {
    "id": "03803",
    "manifest_path": "data/manifests/the_stack_sample/sample_1692.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  annotations:\n    injection.smarttuning.ibm.com: 'true'\n  labels:\n    app: mock\n  name: mock\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: mock\n  template:\n    metadata:\n      labels:\n        app: mock\n    spec:\n      containers:\n      - image: open-liberty:full-java11-openj9\n        imagePullPolicy: IfNotPresent\n        name: openliberty\n        ports:\n        - containerPort: 9080\n        envFrom:\n        - configMapRef:\n            name: mock-envvar\n        volumeMounts:\n        - mountPath: /config/jvm.options\n          subPath: jvm.options\n          name: mock-jvm\n        securityContext:\n          runAsUser: 0\n          privileged: true\n        resources:\n          limits:\n            cpu: 500m\n            memory: 256Mi\n        readinessProbe:\n          initialDelaySeconds: 30\n          httpGet:\n            port: 9080\n      volumes:\n      - name: mock-jvm\n        configMap:\n          name: mock-jvm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"openliberty\" has cpu request 0"
  },
  {
    "id": "03804",
    "manifest_path": "data/manifests/the_stack_sample/sample_1693.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: descheduler\n  namespace: kube-system\n  labels:\n    app: descheduler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: descheduler\n  template:\n    metadata:\n      labels:\n        app: descheduler\n    spec:\n      serviceAccountName: descheduler-sa\n      containers:\n      - name: descheduler\n        image: cr.d.xiaomi.net/cloud-ml/descheduler:v20211207-v0.22.0-16-g50f9513cb\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/descheduler\n        args:\n        - --policy-config-file\n        - /policy-dir/policy.yaml\n        - --nodeSelector\n        - type=virtual-kubelet\n        - --descheduling-interval\n        - 30m\n        - --v\n        - '3'\n        ports:\n        - containerPort: 10258\n          protocol: TCP\n        resources:\n          requests:\n            cpu: 500m\n            memory: 256Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n          capabilities:\n            drop:\n            - ALL\n          privileged: false\n          readOnlyRootFilesystem: true\n          runAsNonRoot: true\n        volumeMounts:\n        - mountPath: /policy-dir\n          name: policy-volume\n      volumes:\n      - name: policy-volume\n        configMap:\n          name: descheduler-policy-configmap\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"descheduler\" has memory limit 0"
  },
  {
    "id": "03805",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"apm-eureka\" does not have a read-only root file system"
  },
  {
    "id": "03806",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sidecar\" does not have a read-only root file system"
  },
  {
    "id": "03807",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"apm-eureka\" is not set to runAsNonRoot"
  },
  {
    "id": "03808",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sidecar\" is not set to runAsNonRoot"
  },
  {
    "id": "03809",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"apm-eureka\" has cpu request 0"
  },
  {
    "id": "03810",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sidecar\" has cpu request 0"
  },
  {
    "id": "03811",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"apm-eureka\" has memory limit 0"
  },
  {
    "id": "03812",
    "manifest_path": "data/manifests/the_stack_sample/sample_1696.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: apm-eureka\n  labels:\n    deploy: apm-eureka\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: apm-eureka\n  template:\n    metadata:\n      name: apm-eureka\n      labels:\n        app: apm-eureka\n    spec:\n      initContainers:\n      - name: sidecar\n        image: apache/skywalking-base:6.5.0\n        imagePullPolicy: IfNotPresent\n        command:\n        - cp\n        - -r\n        - /skywalking/agent\n        - /sidecar\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      containers:\n      - name: apm-eureka\n        image: evanxuhe/apm-eureka:6.1.0\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: JAVA_OPTS\n          value: -javaagent:/sidecar/agent/skywalking-agent.jar\n        - name: SW_AGENT_NAME\n          value: apm-eureka\n        - name: SW_AGENT_COLLECTOR_BACKEND_SERVICES\n          value: skywalk-1h6lqf-skywalking-skywalking-oap.demo\n        ports:\n        - name: http\n          containerPort: 8761\n        volumeMounts:\n        - name: sidecar\n          mountPath: /sidecar\n      volumes:\n      - name: sidecar\n        emptyDir: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sidecar\" has memory limit 0"
  },
  {
    "id": "03813",
    "manifest_path": "data/manifests/the_stack_sample/sample_1697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: e0f23ac6addb6dce6a649e73f5c7584203878aee72cf33274a296a208bedc454\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: zpzjzj\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 78ee09edfc1815a0057f1df4a3749b0dba55c117\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "03814",
    "manifest_path": "data/manifests/the_stack_sample/sample_1697.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.51\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: e0f23ac6addb6dce6a649e73f5c7584203878aee72cf33274a296a208bedc454\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.51\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: zpzjzj\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.51\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 78ee09edfc1815a0057f1df4a3749b0dba55c117\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "03815",
    "manifest_path": "data/manifests/the_stack_sample/sample_1699.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  volumes:\n  - name: persistentsharedspace\n    persistentVolumeClaim:\n      claimName: my-pv-claim\n  containers:\n  - name: nginx\n    image: nginx:1.7.9\n    volumeMounts:\n    - mountPath: /containerMount\n      name: persistentsharedspace\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03816",
    "manifest_path": "data/manifests/the_stack_sample/sample_1699.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  volumes:\n  - name: persistentsharedspace\n    persistentVolumeClaim:\n      claimName: my-pv-claim\n  containers:\n  - name: nginx\n    image: nginx:1.7.9\n    volumeMounts:\n    - mountPath: /containerMount\n      name: persistentsharedspace\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03817",
    "manifest_path": "data/manifests/the_stack_sample/sample_1699.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  volumes:\n  - name: persistentsharedspace\n    persistentVolumeClaim:\n      claimName: my-pv-claim\n  containers:\n  - name: nginx\n    image: nginx:1.7.9\n    volumeMounts:\n    - mountPath: /containerMount\n      name: persistentsharedspace\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03818",
    "manifest_path": "data/manifests/the_stack_sample/sample_1699.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx\nspec:\n  volumes:\n  - name: persistentsharedspace\n    persistentVolumeClaim:\n      claimName: my-pv-claim\n  containers:\n  - name: nginx\n    image: nginx:1.7.9\n    volumeMounts:\n    - mountPath: /containerMount\n      name: persistentsharedspace\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03819",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cntr-apache\" is using an invalid container image, \"httpd:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03820",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cntr-apache\" does not have a read-only root file system"
  },
  {
    "id": "03821",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cntr-apache\" is not set to runAsNonRoot"
  },
  {
    "id": "03822",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cntr-apache\" has cpu request 0"
  },
  {
    "id": "03823",
    "manifest_path": "data/manifests/the_stack_sample/sample_1702.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-web\n  labels:\n    app: web\nspec:\n  containers:\n  - name: cntr-apache\n    image: httpd:latest\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cntr-apache\" has memory limit 0"
  },
  {
    "id": "03824",
    "manifest_path": "data/manifests/the_stack_sample/sample_1704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: github-actions-runner-controller-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: github-actions-runner-controller-registry\n  template:\n    metadata:\n      labels:\n        app: github-actions-runner-controller-registry\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/registry\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"registry\" does not have a read-only root file system"
  },
  {
    "id": "03825",
    "manifest_path": "data/manifests/the_stack_sample/sample_1704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: github-actions-runner-controller-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: github-actions-runner-controller-registry\n  template:\n    metadata:\n      labels:\n        app: github-actions-runner-controller-registry\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/registry\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"registry\" is not set to runAsNonRoot"
  },
  {
    "id": "03826",
    "manifest_path": "data/manifests/the_stack_sample/sample_1704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: github-actions-runner-controller-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: github-actions-runner-controller-registry\n  template:\n    metadata:\n      labels:\n        app: github-actions-runner-controller-registry\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/registry\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"registry\" has cpu request 0"
  },
  {
    "id": "03827",
    "manifest_path": "data/manifests/the_stack_sample/sample_1704.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: github-actions-runner-controller-registry\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: github-actions-runner-controller-registry\n  template:\n    metadata:\n      labels:\n        app: github-actions-runner-controller-registry\n    spec:\n      containers:\n      - name: registry\n        image: registry:2\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        volumeMounts:\n        - name: data\n          mountPath: /var/lib/registry\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"registry\" has memory limit 0"
  },
  {
    "id": "03828",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx-ingress-operator\" is using an invalid container image, \"nginx/nginx-ingress-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03829",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx-ingress-operator\" does not have a read-only root file system"
  },
  {
    "id": "03830",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx-ingress-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03831",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx-ingress-operator\" has cpu request 0"
  },
  {
    "id": "03832",
    "manifest_path": "data/manifests/the_stack_sample/sample_1706.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-ingress-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: nginx-ingress-operator\n  template:\n    metadata:\n      labels:\n        name: nginx-ingress-operator\n    spec:\n      serviceAccountName: nginx-ingress-operator\n      containers:\n      - name: nginx-ingress-operator\n        image: nginx/nginx-ingress-operator:latest\n        command:\n        - nginx-ingress-operator\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: nginx-ingress-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx-ingress-operator\" has memory limit 0"
  },
  {
    "id": "03833",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable AWS_SECRET_ACCESS_KEY in container \"app\" found"
  },
  {
    "id": "03834",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable AWS_SECRET_ACCESS_KEY in container \"init-ubuntu\" found"
  },
  {
    "id": "03835",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"app\" is using an invalid container image, \"alpine\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03836",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-ubuntu\" is using an invalid container image, \"ubuntu\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03837",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"app\" does not have a read-only root file system"
  },
  {
    "id": "03838",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-ubuntu\" does not have a read-only root file system"
  },
  {
    "id": "03839",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"app\" is not set to runAsNonRoot"
  },
  {
    "id": "03840",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-ubuntu\" is not set to runAsNonRoot"
  },
  {
    "id": "03841",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"app\" has cpu request 0"
  },
  {
    "id": "03842",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-ubuntu\" has cpu request 0"
  },
  {
    "id": "03843",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"app\" has memory limit 0"
  },
  {
    "id": "03844",
    "manifest_path": "data/manifests/the_stack_sample/sample_1711.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app\n  namespace: app\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: app\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: app\n      annotations:\n        vault.security.banzaicloud.io/vault-addr: https://vault.vault:8200\n        vault.security.banzaicloud.io/vault-tls-secret: vault-tls\n    spec:\n      initContainers:\n      - name: init-ubuntu\n        image: ubuntu\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo initContainers ready\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#${.AWS_SECRET_ACCESS_KEY}\n      containers:\n      - name: app\n        image: alpine\n        command:\n        - sh\n        - -c\n        - echo $AWS_SECRET_ACCESS_KEY && echo going to sleep... && sleep 10000\n        env:\n        - name: AWS_SECRET_ACCESS_KEY\n          value: vault:secret/data/accounts/aws#AWS_SECRET_ACCESS_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-ubuntu\" has memory limit 0"
  },
  {
    "id": "03845",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"liveness\" is using an invalid container image, \"k8s.gcr.io/liveness\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03846",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness\" does not have a read-only root file system"
  },
  {
    "id": "03847",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness\" is not set to runAsNonRoot"
  },
  {
    "id": "03848",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness\" has cpu request 0"
  },
  {
    "id": "03849",
    "manifest_path": "data/manifests/the_stack_sample/sample_1712.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    test: liveness\n  name: liveness-http\nspec:\n  containers:\n  - name: liveness\n    image: k8s.gcr.io/liveness\n    args:\n    - /server\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 8080\n        httpHeaders:\n        - name: Custom-Header\n          value: Awesome\n      initialDelaySeconds: 3\n      periodSeconds: 3\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness\" has memory limit 0"
  },
  {
    "id": "03850",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03851",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03852",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03853",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03854",
    "manifest_path": "data/manifests/the_stack_sample/sample_1713.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-347\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03855",
    "manifest_path": "data/manifests/the_stack_sample/sample_1714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-nginx-dp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-dp\n  template:\n    metadata:\n      labels:\n        app: nginx-dp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03856",
    "manifest_path": "data/manifests/the_stack_sample/sample_1714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-nginx-dp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-dp\n  template:\n    metadata:\n      labels:\n        app: nginx-dp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03857",
    "manifest_path": "data/manifests/the_stack_sample/sample_1714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-nginx-dp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-dp\n  template:\n    metadata:\n      labels:\n        app: nginx-dp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03858",
    "manifest_path": "data/manifests/the_stack_sample/sample_1714.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sample-nginx-dp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx-dp\n  template:\n    metadata:\n      labels:\n        app: nginx-dp\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:1.15\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03859",
    "manifest_path": "data/manifests/the_stack_sample/sample_1715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220330-40eb179576\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"horologium\" does not have a read-only root file system"
  },
  {
    "id": "03860",
    "manifest_path": "data/manifests/the_stack_sample/sample_1715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220330-40eb179576\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"horologium\" is not set to runAsNonRoot"
  },
  {
    "id": "03861",
    "manifest_path": "data/manifests/the_stack_sample/sample_1715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220330-40eb179576\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"horologium\" has cpu request 0"
  },
  {
    "id": "03862",
    "manifest_path": "data/manifests/the_stack_sample/sample_1715.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: horologium\n  labels:\n    app: horologium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: horologium\n  template:\n    metadata:\n      labels:\n        app: horologium\n    spec:\n      serviceAccountName: horologium\n      containers:\n      - name: horologium\n        image: gcr.io/k8s-prow/horologium:v20220330-40eb179576\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --dry-run=false\n        ports:\n        - name: metrics\n          containerPort: 9090\n        volumeMounts:\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"horologium\" has memory limit 0"
  },
  {
    "id": "03863",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"crime-detail-api-endpoint\" is using an invalid container image, \"usfinthere/crime_detail_api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03864",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crime-detail-api-endpoint\" does not have a read-only root file system"
  },
  {
    "id": "03865",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crime-detail-api-endpoint\" is not set to runAsNonRoot"
  },
  {
    "id": "03866",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crime-detail-api-endpoint\" has cpu request 0"
  },
  {
    "id": "03867",
    "manifest_path": "data/manifests/the_stack_sample/sample_1717.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: crime-detail-api-endpoint\n  namespace: api-services\n  labels:\n    app: crime-detail-api-endpoint\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crime-detail-api-endpoint\n  template:\n    metadata:\n      labels:\n        app: crime-detail-api-endpoint\n    spec:\n      containers:\n      - name: crime-detail-api-endpoint\n        image: usfinthere/crime_detail_api:latest\n        imagePullPolicy: Always\n        envFrom:\n        - secretRef:\n            name: postgres-secret\n        ports:\n        - name: http\n          containerPort: 8080\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crime-detail-api-endpoint\" has memory limit 0"
  },
  {
    "id": "03868",
    "manifest_path": "data/manifests/the_stack_sample/sample_1718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n      --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n      --v=2 1>>/var/log/kube-controller-manager.log --leader-elect 2>&1\n    image: k8s.gcr.io/kube-controller-manager:fda24638d51a48baa13c35337fcd4793\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    name: kube-controller-manager\n    volumeMounts:\n    - mountPath: /srv/kubernetes\n      name: srvkube\n      readOnly: true\n    - mountPath: /var/log/kube-controller-manager.log\n      name: logfile\n    - mountPath: /etc/ssl\n      name: etcssl\n      readOnly: true\n    - mountPath: /usr/share/ssl\n      name: usrsharessl\n      readOnly: true\n    - mountPath: /var/ssl\n      name: varssl\n      readOnly: true\n    - mountPath: /usr/ssl\n      name: usrssl\n      readOnly: true\n    - mountPath: /usr/lib/ssl\n      name: usrlibssl\n      readOnly: true\n    - mountPath: /usr/local/openssl\n      name: usrlocalopenssl\n      readOnly: true\n    - mountPath: /etc/openssl\n      name: etcopenssl\n      readOnly: true\n    - mountPath: /etc/pki/tls\n      name: etcpkitls\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /srv/kubernetes\n    name: srvkube\n  - hostPath:\n      path: /var/log/kube-controller-manager.log\n      type: FileOrCreate\n    name: logfile\n  - hostPath:\n      path: /etc/ssl\n    name: etcssl\n  - hostPath:\n      path: /usr/share/ssl\n    name: usrsharessl\n  - hostPath:\n      path: /var/ssl\n    name: varssl\n  - hostPath:\n      path: /usr/ssl\n    name: usrssl\n  - hostPath:\n      path: /usr/lib/ssl\n    name: usrlibssl\n  - hostPath:\n      path: /usr/local/openssl\n    name: usrlocalopenssl\n  - hostPath:\n      path: /etc/openssl\n    name: etcopenssl\n  - hostPath:\n      path: /etc/pki/tls\n    name: etcpkitls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-controller-manager\" does not have a read-only root file system"
  },
  {
    "id": "03869",
    "manifest_path": "data/manifests/the_stack_sample/sample_1718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n      --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n      --v=2 1>>/var/log/kube-controller-manager.log --leader-elect 2>&1\n    image: k8s.gcr.io/kube-controller-manager:fda24638d51a48baa13c35337fcd4793\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    name: kube-controller-manager\n    volumeMounts:\n    - mountPath: /srv/kubernetes\n      name: srvkube\n      readOnly: true\n    - mountPath: /var/log/kube-controller-manager.log\n      name: logfile\n    - mountPath: /etc/ssl\n      name: etcssl\n      readOnly: true\n    - mountPath: /usr/share/ssl\n      name: usrsharessl\n      readOnly: true\n    - mountPath: /var/ssl\n      name: varssl\n      readOnly: true\n    - mountPath: /usr/ssl\n      name: usrssl\n      readOnly: true\n    - mountPath: /usr/lib/ssl\n      name: usrlibssl\n      readOnly: true\n    - mountPath: /usr/local/openssl\n      name: usrlocalopenssl\n      readOnly: true\n    - mountPath: /etc/openssl\n      name: etcopenssl\n      readOnly: true\n    - mountPath: /etc/pki/tls\n      name: etcpkitls\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /srv/kubernetes\n    name: srvkube\n  - hostPath:\n      path: /var/log/kube-controller-manager.log\n      type: FileOrCreate\n    name: logfile\n  - hostPath:\n      path: /etc/ssl\n    name: etcssl\n  - hostPath:\n      path: /usr/share/ssl\n    name: usrsharessl\n  - hostPath:\n      path: /var/ssl\n    name: varssl\n  - hostPath:\n      path: /usr/ssl\n    name: usrssl\n  - hostPath:\n      path: /usr/lib/ssl\n    name: usrlibssl\n  - hostPath:\n      path: /usr/local/openssl\n    name: usrlocalopenssl\n  - hostPath:\n      path: /etc/openssl\n    name: etcopenssl\n  - hostPath:\n      path: /etc/pki/tls\n    name: etcpkitls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-controller-manager\" is not set to runAsNonRoot"
  },
  {
    "id": "03870",
    "manifest_path": "data/manifests/the_stack_sample/sample_1718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n      --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n      --v=2 1>>/var/log/kube-controller-manager.log --leader-elect 2>&1\n    image: k8s.gcr.io/kube-controller-manager:fda24638d51a48baa13c35337fcd4793\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    name: kube-controller-manager\n    volumeMounts:\n    - mountPath: /srv/kubernetes\n      name: srvkube\n      readOnly: true\n    - mountPath: /var/log/kube-controller-manager.log\n      name: logfile\n    - mountPath: /etc/ssl\n      name: etcssl\n      readOnly: true\n    - mountPath: /usr/share/ssl\n      name: usrsharessl\n      readOnly: true\n    - mountPath: /var/ssl\n      name: varssl\n      readOnly: true\n    - mountPath: /usr/ssl\n      name: usrssl\n      readOnly: true\n    - mountPath: /usr/lib/ssl\n      name: usrlibssl\n      readOnly: true\n    - mountPath: /usr/local/openssl\n      name: usrlocalopenssl\n      readOnly: true\n    - mountPath: /etc/openssl\n      name: etcopenssl\n      readOnly: true\n    - mountPath: /etc/pki/tls\n      name: etcpkitls\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /srv/kubernetes\n    name: srvkube\n  - hostPath:\n      path: /var/log/kube-controller-manager.log\n      type: FileOrCreate\n    name: logfile\n  - hostPath:\n      path: /etc/ssl\n    name: etcssl\n  - hostPath:\n      path: /usr/share/ssl\n    name: usrsharessl\n  - hostPath:\n      path: /var/ssl\n    name: varssl\n  - hostPath:\n      path: /usr/ssl\n    name: usrssl\n  - hostPath:\n      path: /usr/lib/ssl\n    name: usrlibssl\n  - hostPath:\n      path: /usr/local/openssl\n    name: usrlocalopenssl\n  - hostPath:\n      path: /etc/openssl\n    name: etcopenssl\n  - hostPath:\n      path: /etc/pki/tls\n    name: etcpkitls\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-controller-manager\" has cpu request 0"
  },
  {
    "id": "03871",
    "manifest_path": "data/manifests/the_stack_sample/sample_1718.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: kube-controller-manager\nspec:\n  containers:\n  - command:\n    - /bin/sh\n    - -c\n    - /usr/local/bin/kube-controller-manager --master=127.0.0.1:8080 --cluster-name=e2e-test-bburns\n      --cluster-cidr=10.245.0.0/16 --allocate-node-cidrs=true --cloud-provider=gce  --service-account-private-key-file=/srv/kubernetes/server.key\n      --v=2 1>>/var/log/kube-controller-manager.log --leader-elect 2>&1\n    image: k8s.gcr.io/kube-controller-manager:fda24638d51a48baa13c35337fcd4793\n    livenessProbe:\n      httpGet:\n        path: /healthz\n        port: 10252\n      initialDelaySeconds: 15\n      timeoutSeconds: 1\n    name: kube-controller-manager\n    volumeMounts:\n    - mountPath: /srv/kubernetes\n      name: srvkube\n      readOnly: true\n    - mountPath: /var/log/kube-controller-manager.log\n      name: logfile\n    - mountPath: /etc/ssl\n      name: etcssl\n      readOnly: true\n    - mountPath: /usr/share/ssl\n      name: usrsharessl\n      readOnly: true\n    - mountPath: /var/ssl\n      name: varssl\n      readOnly: true\n    - mountPath: /usr/ssl\n      name: usrssl\n      readOnly: true\n    - mountPath: /usr/lib/ssl\n      name: usrlibssl\n      readOnly: true\n    - mountPath: /usr/local/openssl\n      name: usrlocalopenssl\n      readOnly: true\n    - mountPath: /etc/openssl\n      name: etcopenssl\n      readOnly: true\n    - mountPath: /etc/pki/tls\n      name: etcpkitls\n      readOnly: true\n  volumes:\n  - hostPath:\n      path: /srv/kubernetes\n    name: srvkube\n  - hostPath:\n      path: /var/log/kube-controller-manager.log\n      type: FileOrCreate\n    name: logfile\n  - hostPath:\n      path: /etc/ssl\n    name: etcssl\n  - hostPath:\n      path: /usr/share/ssl\n    name: usrsharessl\n  - hostPath:\n      path: /var/ssl\n    name: varssl\n  - hostPath:\n      path: /usr/ssl\n    name: usrssl\n  - hostPath:\n      path: /usr/lib/ssl\n    name: usrlibssl\n  - hostPath:\n      path: /usr/local/openssl\n    name: usrlocalopenssl\n  - hostPath:\n      path: /etc/openssl\n    name: etcopenssl\n  - hostPath:\n      path: /etc/pki/tls\n    name: etcpkitls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-controller-manager\" has memory limit 0"
  },
  {
    "id": "03872",
    "manifest_path": "data/manifests/the_stack_sample/sample_1720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  labels:\n    component: nats\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: nats\n  template:\n    metadata:\n      labels:\n        component: nats\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - nats\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nats\n        image: nats:1.1.0\n        args:\n        - --config\n        - /etc/nats/config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats/config\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nats\" does not have a read-only root file system"
  },
  {
    "id": "03873",
    "manifest_path": "data/manifests/the_stack_sample/sample_1720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  labels:\n    component: nats\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: nats\n  template:\n    metadata:\n      labels:\n        component: nats\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - nats\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nats\n        image: nats:1.1.0\n        args:\n        - --config\n        - /etc/nats/config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats/config\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nats\" is not set to runAsNonRoot"
  },
  {
    "id": "03874",
    "manifest_path": "data/manifests/the_stack_sample/sample_1720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  labels:\n    component: nats\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: nats\n  template:\n    metadata:\n      labels:\n        component: nats\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - nats\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nats\n        image: nats:1.1.0\n        args:\n        - --config\n        - /etc/nats/config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats/config\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nats\" has cpu request 0"
  },
  {
    "id": "03875",
    "manifest_path": "data/manifests/the_stack_sample/sample_1720.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: nats\n  labels:\n    component: nats\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      component: nats\n  template:\n    metadata:\n      labels:\n        component: nats\n    spec:\n      affinity:\n        podAntiAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n          - labelSelector:\n              matchExpressions:\n              - key: component\n                operator: In\n                values:\n                - nats\n            topologyKey: kubernetes.io/hostname\n      containers:\n      - name: nats\n        image: nats:1.1.0\n        args:\n        - --config\n        - /etc/nats/config/nats.conf\n        volumeMounts:\n        - name: config-volume\n          mountPath: /etc/nats/config\n        ports:\n        - containerPort: 4222\n          name: client\n        - containerPort: 6222\n          name: cluster\n        - containerPort: 8222\n          name: monitor\n        livenessProbe:\n          httpGet:\n            path: /\n            port: 8222\n          initialDelaySeconds: 10\n          timeoutSeconds: 5\n      volumes:\n      - name: config-volume\n        configMap:\n          name: nats-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nats\" has memory limit 0"
  },
  {
    "id": "03876",
    "manifest_path": "data/manifests/the_stack_sample/sample_1721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\n  labels:\n    app: jaeger\n    app.kubernetes.io/name: jaeger\n    app.kubernetes.io/component: agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger\n      app.kubernetes.io/name: jaeger\n      app.kubernetes.io/component: agent\n  template:\n    metadata:\n      labels:\n        app: jaeger\n        app.kubernetes.io/name: jaeger\n        app.kubernetes.io/component: agent\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '14271'\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:1.18.1\n        args:\n        - --config-file=/conf/agent.yaml\n        volumeMounts:\n        - name: jaeger-configuration-volume\n          mountPath: /conf\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        - containerPort: 14271\n          name: admin-http\n          protocol: TCP\n      volumes:\n      - configMap:\n          name: jaeger-configuration\n          items:\n          - key: agent\n            path: agent.yaml\n        name: jaeger-configuration-volume\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-agent\" does not have a read-only root file system"
  },
  {
    "id": "03877",
    "manifest_path": "data/manifests/the_stack_sample/sample_1721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\n  labels:\n    app: jaeger\n    app.kubernetes.io/name: jaeger\n    app.kubernetes.io/component: agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger\n      app.kubernetes.io/name: jaeger\n      app.kubernetes.io/component: agent\n  template:\n    metadata:\n      labels:\n        app: jaeger\n        app.kubernetes.io/name: jaeger\n        app.kubernetes.io/component: agent\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '14271'\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:1.18.1\n        args:\n        - --config-file=/conf/agent.yaml\n        volumeMounts:\n        - name: jaeger-configuration-volume\n          mountPath: /conf\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        - containerPort: 14271\n          name: admin-http\n          protocol: TCP\n      volumes:\n      - configMap:\n          name: jaeger-configuration\n          items:\n          - key: agent\n            path: agent.yaml\n        name: jaeger-configuration-volume\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "03878",
    "manifest_path": "data/manifests/the_stack_sample/sample_1721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\n  labels:\n    app: jaeger\n    app.kubernetes.io/name: jaeger\n    app.kubernetes.io/component: agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger\n      app.kubernetes.io/name: jaeger\n      app.kubernetes.io/component: agent\n  template:\n    metadata:\n      labels:\n        app: jaeger\n        app.kubernetes.io/name: jaeger\n        app.kubernetes.io/component: agent\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '14271'\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:1.18.1\n        args:\n        - --config-file=/conf/agent.yaml\n        volumeMounts:\n        - name: jaeger-configuration-volume\n          mountPath: /conf\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        - containerPort: 14271\n          name: admin-http\n          protocol: TCP\n      volumes:\n      - configMap:\n          name: jaeger-configuration\n          items:\n          - key: agent\n            path: agent.yaml\n        name: jaeger-configuration-volume\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-agent\" has cpu request 0"
  },
  {
    "id": "03879",
    "manifest_path": "data/manifests/the_stack_sample/sample_1721.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: jaeger-agent\n  labels:\n    app: jaeger\n    app.kubernetes.io/name: jaeger\n    app.kubernetes.io/component: agent\nspec:\n  selector:\n    matchLabels:\n      app: jaeger\n      app.kubernetes.io/name: jaeger\n      app.kubernetes.io/component: agent\n  template:\n    metadata:\n      labels:\n        app: jaeger\n        app.kubernetes.io/name: jaeger\n        app.kubernetes.io/component: agent\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '14271'\n    spec:\n      containers:\n      - name: jaeger-agent\n        image: jaegertracing/jaeger-agent:1.18.1\n        args:\n        - --config-file=/conf/agent.yaml\n        volumeMounts:\n        - name: jaeger-configuration-volume\n          mountPath: /conf\n        ports:\n        - containerPort: 5775\n          protocol: UDP\n        - containerPort: 6831\n          protocol: UDP\n        - containerPort: 6832\n          protocol: UDP\n        - containerPort: 5778\n          protocol: TCP\n        - containerPort: 14271\n          name: admin-http\n          protocol: TCP\n      volumes:\n      - configMap:\n          name: jaeger-configuration\n          items:\n          - key: agent\n            path: agent.yaml\n        name: jaeger-configuration-volume\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-agent\" has memory limit 0"
  },
  {
    "id": "03880",
    "manifest_path": "data/manifests/the_stack_sample/sample_1722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-aws-iam-controller\n  namespace: kube-system\n  labels:\n    application: kube-aws-iam-controller\n    version: latest\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-aws-iam-controller\n  template:\n    metadata:\n      labels:\n        application: kube-aws-iam-controller\n        version: latest\n    spec:\n      serviceAccountName: kube-aws-iam-controller\n      containers:\n      - name: kube-aws-iam-controller\n        image: registry.opensource.zalan.do/teapot/kube-aws-iam-controller:latest\n        resources:\n          limits:\n            cpu: 25m\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"kube-aws-iam-controller\" is using an invalid container image, \"registry.opensource.zalan.do/teapot/kube-aws-iam-controller:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03881",
    "manifest_path": "data/manifests/the_stack_sample/sample_1722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-aws-iam-controller\n  namespace: kube-system\n  labels:\n    application: kube-aws-iam-controller\n    version: latest\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-aws-iam-controller\n  template:\n    metadata:\n      labels:\n        application: kube-aws-iam-controller\n        version: latest\n    spec:\n      serviceAccountName: kube-aws-iam-controller\n      containers:\n      - name: kube-aws-iam-controller\n        image: registry.opensource.zalan.do/teapot/kube-aws-iam-controller:latest\n        resources:\n          limits:\n            cpu: 25m\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-aws-iam-controller\" does not have a read-only root file system"
  },
  {
    "id": "03882",
    "manifest_path": "data/manifests/the_stack_sample/sample_1722.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kube-aws-iam-controller\n  namespace: kube-system\n  labels:\n    application: kube-aws-iam-controller\n    version: latest\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      application: kube-aws-iam-controller\n  template:\n    metadata:\n      labels:\n        application: kube-aws-iam-controller\n        version: latest\n    spec:\n      serviceAccountName: kube-aws-iam-controller\n      containers:\n      - name: kube-aws-iam-controller\n        image: registry.opensource.zalan.do/teapot/kube-aws-iam-controller:latest\n        resources:\n          limits:\n            cpu: 25m\n            memory: 100Mi\n          requests:\n            cpu: 25m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-aws-iam-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "03883",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03884",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03885",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03886",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03887",
    "manifest_path": "data/manifests/the_stack_sample/sample_1723.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9766\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03888",
    "manifest_path": "data/manifests/the_stack_sample/sample_1726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nsc-memif\n  labels:\n    app: nsc-memif\nspec:\n  selector:\n    matchLabels:\n      app: nsc-memif\n  template:\n    metadata:\n      labels:\n        app: nsc-memif\n    spec:\n      containers:\n      - name: nsc\n        image: networkservicemeshci/cmd-nsc-vpp:da4acb28\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NSM_REQUEST_TIMEOUT\n          value: 1m\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nsc\" does not have a read-only root file system"
  },
  {
    "id": "03889",
    "manifest_path": "data/manifests/the_stack_sample/sample_1726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nsc-memif\n  labels:\n    app: nsc-memif\nspec:\n  selector:\n    matchLabels:\n      app: nsc-memif\n  template:\n    metadata:\n      labels:\n        app: nsc-memif\n    spec:\n      containers:\n      - name: nsc\n        image: networkservicemeshci/cmd-nsc-vpp:da4acb28\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NSM_REQUEST_TIMEOUT\n          value: 1m\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nsc\" is not set to runAsNonRoot"
  },
  {
    "id": "03890",
    "manifest_path": "data/manifests/the_stack_sample/sample_1726.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nsc-memif\n  labels:\n    app: nsc-memif\nspec:\n  selector:\n    matchLabels:\n      app: nsc-memif\n  template:\n    metadata:\n      labels:\n        app: nsc-memif\n    spec:\n      containers:\n      - name: nsc\n        image: networkservicemeshci/cmd-nsc-vpp:da4acb28\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NSM_REQUEST_TIMEOUT\n          value: 1m\n        - name: SPIFFE_ENDPOINT_SOCKET\n          value: unix:///run/spire/sockets/agent.sock\n        - name: NSM_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        volumeMounts:\n        - name: spire-agent-socket\n          mountPath: /run/spire/sockets\n          readOnly: true\n        - name: nsm-socket\n          mountPath: /var/lib/networkservicemesh\n          readOnly: true\n        resources:\n          limits:\n            memory: 400Mi\n            cpu: 500m\n      volumes:\n      - name: spire-agent-socket\n        hostPath:\n          path: /run/spire/sockets\n          type: Directory\n      - name: nsm-socket\n        hostPath:\n          path: /var/lib/networkservicemesh\n          type: DirectoryOrCreate\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nsc\" has cpu request 0"
  },
  {
    "id": "03891",
    "manifest_path": "data/manifests/the_stack_sample/sample_1727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cadvisor\nspec:\n  replicas: 50\n  selector:\n    app: cadvisor\n  template:\n    metadata:\n      labels:\n        app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.22.0\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        args:\n        - --profiling\n        - --housekeeping_interval=1s\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cadvisor\" does not have a read-only root file system"
  },
  {
    "id": "03892",
    "manifest_path": "data/manifests/the_stack_sample/sample_1727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cadvisor\nspec:\n  replicas: 50\n  selector:\n    app: cadvisor\n  template:\n    metadata:\n      labels:\n        app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.22.0\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        args:\n        - --profiling\n        - --housekeeping_interval=1s\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cadvisor\" is not set to runAsNonRoot"
  },
  {
    "id": "03893",
    "manifest_path": "data/manifests/the_stack_sample/sample_1727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cadvisor\nspec:\n  replicas: 50\n  selector:\n    app: cadvisor\n  template:\n    metadata:\n      labels:\n        app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.22.0\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        args:\n        - --profiling\n        - --housekeeping_interval=1s\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cadvisor\" has cpu request 0"
  },
  {
    "id": "03894",
    "manifest_path": "data/manifests/the_stack_sample/sample_1727.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: cadvisor\nspec:\n  replicas: 50\n  selector:\n    app: cadvisor\n  template:\n    metadata:\n      labels:\n        app: cadvisor\n    spec:\n      containers:\n      - name: cadvisor\n        image: google/cadvisor:v0.22.0\n        volumeMounts:\n        - name: rootfs\n          mountPath: /rootfs\n          readOnly: true\n        - name: var-run\n          mountPath: /var/run\n          readOnly: false\n        - name: sys\n          mountPath: /sys\n          readOnly: true\n        - name: docker\n          mountPath: /var/lib/docker\n          readOnly: true\n        ports:\n        - name: http\n          containerPort: 8080\n          protocol: TCP\n        args:\n        - --profiling\n        - --housekeeping_interval=1s\n      volumes:\n      - name: rootfs\n        hostPath:\n          path: /\n      - name: var-run\n        hostPath:\n          path: /var/run\n      - name: sys\n        hostPath:\n          path: /sys\n      - name: docker\n        hostPath:\n          path: /var/lib/docker\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cadvisor\" has memory limit 0"
  },
  {
    "id": "03895",
    "manifest_path": "data/manifests/the_stack_sample/sample_1729.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 0.51.1\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: kube-prometheus\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: prometheus-operator\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 0.51.1\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.51.1\n        image: quay.io/prometheus-operator/prometheus-operator:v0.51.1\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        image: quay.io/brancz/kube-rbac-proxy:v0.11.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: prometheus-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy\" does not have a read-only root file system"
  },
  {
    "id": "03896",
    "manifest_path": "data/manifests/the_stack_sample/sample_1729.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/component: controller\n    app.kubernetes.io/name: prometheus-operator\n    app.kubernetes.io/part-of: kube-prometheus\n    app.kubernetes.io/version: 0.51.1\n  name: prometheus-operator\n  namespace: monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: controller\n      app.kubernetes.io/name: prometheus-operator\n      app.kubernetes.io/part-of: kube-prometheus\n  template:\n    metadata:\n      annotations:\n        kubectl.kubernetes.io/default-container: prometheus-operator\n      labels:\n        app.kubernetes.io/component: controller\n        app.kubernetes.io/name: prometheus-operator\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 0.51.1\n    spec:\n      containers:\n      - args:\n        - --kubelet-service=kube-system/kubelet\n        - --prometheus-config-reloader=quay.io/prometheus-operator/prometheus-config-reloader:v0.51.1\n        image: quay.io/prometheus-operator/prometheus-operator:v0.51.1\n        name: prometheus-operator\n        ports:\n        - containerPort: 8080\n          name: http\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        securityContext:\n          allowPrivilegeEscalation: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305\n        - --upstream=http://127.0.0.1:8080/\n        image: quay.io/brancz/kube-rbac-proxy:v0.11.0\n        name: kube-rbac-proxy\n        ports:\n        - containerPort: 8443\n          name: https\n        resources:\n          limits:\n            cpu: 20m\n            memory: 40Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        securityContext:\n          runAsGroup: 65532\n          runAsNonRoot: true\n          runAsUser: 65532\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 65534\n      serviceAccountName: prometheus-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"prometheus-operator\" does not have a read-only root file system"
  },
  {
    "id": "03897",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cronjob-ctr\" is using an invalid container image, \"perl\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03898",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cronjob-ctr\" does not have a read-only root file system"
  },
  {
    "id": "03899",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cronjob-ctr\" is not set to runAsNonRoot"
  },
  {
    "id": "03900",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cronjob-ctr\" has cpu request 0"
  },
  {
    "id": "03901",
    "manifest_path": "data/manifests/the_stack_sample/sample_1730.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: cronjob-km\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        labels:\n          parent: cronjobpi\n      spec:\n        containers:\n        - name: cronjob-ctr\n          image: perl\n          command:\n          - perl\n          - -Mbignum=bpi\n          - -wle\n          - print bpi(2000)\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cronjob-ctr\" has memory limit 0"
  },
  {
    "id": "03902",
    "manifest_path": "data/manifests/the_stack_sample/sample_1734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        args:\n        - -namespace\n        - openshift-marketplace\n        - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest\n        - -writeStatusName\n        - operator-lifecycle-manager-catalog\n        - -tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - -tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:93751ae9d398d571c2cb3d11b0ac4ae052117fd1726025364a6f2f3a5caef68e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: catalog-operator-serving-cert\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"catalog-operator\" does not have a read-only root file system"
  },
  {
    "id": "03903",
    "manifest_path": "data/manifests/the_stack_sample/sample_1734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        args:\n        - -namespace\n        - openshift-marketplace\n        - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest\n        - -writeStatusName\n        - operator-lifecycle-manager-catalog\n        - -tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - -tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:93751ae9d398d571c2cb3d11b0ac4ae052117fd1726025364a6f2f3a5caef68e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: catalog-operator-serving-cert\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"catalog-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03904",
    "manifest_path": "data/manifests/the_stack_sample/sample_1734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        args:\n        - -namespace\n        - openshift-marketplace\n        - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest\n        - -writeStatusName\n        - operator-lifecycle-manager-catalog\n        - -tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - -tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:93751ae9d398d571c2cb3d11b0ac4ae052117fd1726025364a6f2f3a5caef68e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: catalog-operator-serving-cert\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"catalog-operator\" has cpu request 0"
  },
  {
    "id": "03905",
    "manifest_path": "data/manifests/the_stack_sample/sample_1734.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: catalog-operator\n  namespace: openshift-operator-lifecycle-manager\n  labels:\n    app: catalog-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: catalog-operator\n  template:\n    metadata:\n      labels:\n        app: catalog-operator\n    spec:\n      serviceAccountName: olm-operator-serviceaccount\n      containers:\n      - name: catalog-operator\n        command:\n        - /bin/catalog\n        args:\n        - -namespace\n        - openshift-marketplace\n        - -configmapServerImage=quay.io/operator-framework/configmap-operator-registry:latest\n        - -writeStatusName\n        - operator-lifecycle-manager-catalog\n        - -tls-cert\n        - /var/run/secrets/serving-cert/tls.crt\n        - -tls-key\n        - /var/run/secrets/serving-cert/tls.key\n        image: quay.io/operator-framework/olm@sha256:93751ae9d398d571c2cb3d11b0ac4ae052117fd1726025364a6f2f3a5caef68e\n        imagePullPolicy: IfNotPresent\n        ports:\n        - containerPort: 8080\n        - containerPort: 8081\n          name: metrics\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n        env:\n        - name: RELEASE_VERSION\n          value: 0.0.1-snapshot\n        volumeMounts:\n        - mountPath: /var/run/secrets/serving-cert\n          name: serving-cert\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: catalog-operator-serving-cert\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"catalog-operator\" has memory limit 0"
  },
  {
    "id": "03906",
    "manifest_path": "data/manifests/the_stack_sample/sample_1741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-users-allowed\n  labels:\n    app: nginx-users\nspec:\n  securityContext:\n    supplementalGroups:\n    - 199\n    fsGroup: 199\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsUser: 199\n      runAsGroup: 199\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03907",
    "manifest_path": "data/manifests/the_stack_sample/sample_1741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-users-allowed\n  labels:\n    app: nginx-users\nspec:\n  securityContext:\n    supplementalGroups:\n    - 199\n    fsGroup: 199\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsUser: 199\n      runAsGroup: 199\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03908",
    "manifest_path": "data/manifests/the_stack_sample/sample_1741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-users-allowed\n  labels:\n    app: nginx-users\nspec:\n  securityContext:\n    supplementalGroups:\n    - 199\n    fsGroup: 199\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsUser: 199\n      runAsGroup: 199\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03909",
    "manifest_path": "data/manifests/the_stack_sample/sample_1741.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: nginx-users-allowed\n  labels:\n    app: nginx-users\nspec:\n  securityContext:\n    supplementalGroups:\n    - 199\n    fsGroup: 199\n  containers:\n  - name: nginx\n    image: nginx\n    securityContext:\n      runAsUser: 199\n      runAsGroup: 199\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03910",
    "manifest_path": "data/manifests/the_stack_sample/sample_1746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: general\n  name: ddns-updater-anagno-dev\nspec:\n  selector:\n    matchLabels:\n      app: ddns\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ddns\n    spec:\n      containers:\n      - name: ddns\n        image: anagno/gandi-ddns:0.3\n        env:\n        - name: DOMAIN\n          value: anagno.dev\n        - name: SUBDOMAIN\n          value: zh\n        - name: APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: gandi\n              key: API_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ddns\" does not have a read-only root file system"
  },
  {
    "id": "03911",
    "manifest_path": "data/manifests/the_stack_sample/sample_1746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: general\n  name: ddns-updater-anagno-dev\nspec:\n  selector:\n    matchLabels:\n      app: ddns\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ddns\n    spec:\n      containers:\n      - name: ddns\n        image: anagno/gandi-ddns:0.3\n        env:\n        - name: DOMAIN\n          value: anagno.dev\n        - name: SUBDOMAIN\n          value: zh\n        - name: APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: gandi\n              key: API_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ddns\" is not set to runAsNonRoot"
  },
  {
    "id": "03912",
    "manifest_path": "data/manifests/the_stack_sample/sample_1746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: general\n  name: ddns-updater-anagno-dev\nspec:\n  selector:\n    matchLabels:\n      app: ddns\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ddns\n    spec:\n      containers:\n      - name: ddns\n        image: anagno/gandi-ddns:0.3\n        env:\n        - name: DOMAIN\n          value: anagno.dev\n        - name: SUBDOMAIN\n          value: zh\n        - name: APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: gandi\n              key: API_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ddns\" has cpu request 0"
  },
  {
    "id": "03913",
    "manifest_path": "data/manifests/the_stack_sample/sample_1746.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: general\n  name: ddns-updater-anagno-dev\nspec:\n  selector:\n    matchLabels:\n      app: ddns\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: ddns\n    spec:\n      containers:\n      - name: ddns\n        image: anagno/gandi-ddns:0.3\n        env:\n        - name: DOMAIN\n          value: anagno.dev\n        - name: SUBDOMAIN\n          value: zh\n        - name: APIKEY\n          valueFrom:\n            secretKeyRef:\n              name: gandi\n              key: API_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ddns\" has memory limit 0"
  },
  {
    "id": "03914",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable SECRETS_email_password in container \"zulip\" found"
  },
  {
    "id": "03915",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable SECRETS_secret_key in container \"zulip\" found"
  },
  {
    "id": "03916",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"memcached\" is using an invalid container image, \"quay.io/sameersbn/memcached:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03917",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"postgresql\" is using an invalid container image, \"quay.io/galexrt/zulip-postgresql-tsearchextras:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03918",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"redis\" is using an invalid container image, \"quay.io/sameersbn/redis:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03919",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"memcached\" does not have a read-only root file system"
  },
  {
    "id": "03920",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"postgresql\" does not have a read-only root file system"
  },
  {
    "id": "03921",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rabbitmq\" does not have a read-only root file system"
  },
  {
    "id": "03922",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"redis\" does not have a read-only root file system"
  },
  {
    "id": "03923",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"zulip\" does not have a read-only root file system"
  },
  {
    "id": "03924",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"memcached\" is not set to runAsNonRoot"
  },
  {
    "id": "03925",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"postgresql\" is not set to runAsNonRoot"
  },
  {
    "id": "03926",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rabbitmq\" is not set to runAsNonRoot"
  },
  {
    "id": "03927",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"redis\" is not set to runAsNonRoot"
  },
  {
    "id": "03928",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"zulip\" is not set to runAsNonRoot"
  },
  {
    "id": "03929",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"memcached\" has cpu request 0"
  },
  {
    "id": "03930",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"postgresql\" has cpu request 0"
  },
  {
    "id": "03931",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"rabbitmq\" has cpu request 0"
  },
  {
    "id": "03932",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"redis\" has cpu request 0"
  },
  {
    "id": "03933",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"zulip\" has cpu request 0"
  },
  {
    "id": "03934",
    "manifest_path": "data/manifests/the_stack_sample/sample_1748.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: zulip-1\n  labels:\n    version: 1.8.0-2\n    app: zulip\nspec:\n  replicas: 1\n  selector:\n    version: 1.8.0-2\n    app: zulip\n  template:\n    metadata:\n      labels:\n        version: 1.8.0-2\n        app: zulip\n    spec:\n      containers:\n      - name: redis\n        image: quay.io/sameersbn/redis:latest\n        resources:\n          limits:\n            cpu: 50m\n        volumeMounts:\n        - name: redis-persistent-storage\n          mountPath: /var/lib/redis\n      - name: memcached\n        image: quay.io/sameersbn/memcached:latest\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n      - name: rabbitmq\n        image: rabbitmq:3.5.5\n        resources:\n          limits:\n            cpu: 75m\n            memory: 768Mi\n        env:\n        - name: RABBITMQ_DEFAULT_USER\n          value: zulip\n        - name: RABBITMQ_DEFAULT_PASS\n          value: zulip\n      - name: postgresql\n        image: quay.io/galexrt/zulip-postgresql-tsearchextras:latest\n        resources:\n          limits:\n            cpu: 80m\n            memory: 768Mi\n        env:\n        - name: DB_NAME\n          value: zulip\n        - name: DB_USER\n          value: zulip\n        - name: DB_PASS\n          value: zulip\n        volumeMounts:\n        - name: postgresql-persistent-storage\n          mountPath: /var/lib/postgresql\n      - name: zulip\n        image: quay.io/galexrt/zulip:1.8.0-2\n        resources:\n          limits:\n            cpu: 100m\n            memory: 3584Mi\n        env:\n        - name: DB_HOST\n          value: database\n        - name: MEMCACHED_HOST\n          value: memcached\n        - name: REDIS_HOST\n          value: redis\n        - name: RABBITMQ_HOST\n          value: rabbitmq\n        - name: ZULIP_AUTH_BACKENDS\n          value: EmailAuthBackend\n        - name: SECRETS_email_password\n          value: '123456789'\n        - name: SETTING_EXTERNAL_HOST\n          value: localhost\n        - name: SETTING_ZULIP_ADMINISTRATOR\n          value: admin@example.com\n        - name: SETTING_ADMIN_DOMAIN\n          value: zerbytes.net\n        - name: SETTING_NOREPLY_EMAIL_ADDRESS\n          value: noreply@example.com\n        - name: SETTING_DEFAULT_FROM_EMAIL\n          value: Zulip <noreply@example.com>\n        - name: SETTING_EMAIL_HOST\n          value: smtp.example.com\n        - name: SETTING_EMAIL_HOST_USER\n          value: noreply@example.com\n        - name: ZULIP_USER_EMAIL\n          value: example@example.com\n        - name: ZULIP_USER_DOMAIN\n          value: example.com\n        - name: ZULIP_USER_PASS\n          value: '123456789'\n        - name: SECRETS_secret_key\n          value: REPLCAE_WITH_SECURE_SECRET_KEY\n        ports:\n        - containerPort: 80\n          name: http\n          protocol: TCP\n        - containerPort: 443\n          name: https\n          protocol: TCP\n        volumeMounts:\n        - name: zulip-persistent-storage\n          mountPath: /data\n      volumes:\n      - name: redis-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/redis\n      - name: postgresql-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/postgresql\n      - name: zulip-persistent-storage\n        hostPath:\n          path: /opt/docker/zulip/zulip\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"redis\" has memory limit 0"
  },
  {
    "id": "03935",
    "manifest_path": "data/manifests/the_stack_sample/sample_1752.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: olegim89/mysql-operator:v0.1\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"operator\" does not have a read-only root file system"
  },
  {
    "id": "03936",
    "manifest_path": "data/manifests/the_stack_sample/sample_1752.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: olegim89/mysql-operator:v0.1\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03937",
    "manifest_path": "data/manifests/the_stack_sample/sample_1752.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: olegim89/mysql-operator:v0.1\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"operator\" has cpu request 0"
  },
  {
    "id": "03938",
    "manifest_path": "data/manifests/the_stack_sample/sample_1752.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: mysql-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: mysql-operator\n  template:\n    metadata:\n      labels:\n        name: mysql-operator\n    spec:\n      serviceAccountName: mysql-operator\n      containers:\n      - name: operator\n        image: olegim89/mysql-operator:v0.1\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"operator\" has memory limit 0"
  },
  {
    "id": "03939",
    "manifest_path": "data/manifests/the_stack_sample/sample_1753.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: product-composite\nspec:\n  selector:\n    matchLabels:\n      app: product-composite\n  template:\n    metadata:\n      labels:\n        app: product-composite\n        version: v2\n    spec:\n      containers:\n      - name: comp\n        image: hands-on/product-composite-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"comp\" does not have a read-only root file system"
  },
  {
    "id": "03940",
    "manifest_path": "data/manifests/the_stack_sample/sample_1753.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: product-composite\nspec:\n  selector:\n    matchLabels:\n      app: product-composite\n  template:\n    metadata:\n      labels:\n        app: product-composite\n        version: v2\n    spec:\n      containers:\n      - name: comp\n        image: hands-on/product-composite-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"comp\" is not set to runAsNonRoot"
  },
  {
    "id": "03941",
    "manifest_path": "data/manifests/the_stack_sample/sample_1753.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: product-composite\nspec:\n  selector:\n    matchLabels:\n      app: product-composite\n  template:\n    metadata:\n      labels:\n        app: product-composite\n        version: v2\n    spec:\n      containers:\n      - name: comp\n        image: hands-on/product-composite-service:v2\n        env:\n        - name: SPRING_PROFILES_ACTIVE\n          value: docker,prod\n        resources:\n          requests:\n            memory: 200Mi\n          limits:\n            memory: 400Mi\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"comp\" has cpu request 0"
  },
  {
    "id": "03942",
    "manifest_path": "data/manifests/the_stack_sample/sample_1754.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallenge97396acr.azurecr.io/captureorder:cf1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"captureorder\" does not have a read-only root file system"
  },
  {
    "id": "03943",
    "manifest_path": "data/manifests/the_stack_sample/sample_1754.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: captureorder\nspec:\n  selector:\n    matchLabels:\n      app: captureorder\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: captureorder\n    spec:\n      containers:\n      - name: captureorder\n        image: akschallenge97396acr.azurecr.io/captureorder:cf1\n        imagePullPolicy: Always\n        readinessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        livenessProbe:\n          httpGet:\n            port: 8080\n            path: /healthz\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 100m\n          limits:\n            memory: 128Mi\n            cpu: 500m\n        env:\n        - name: TEAMNAME\n          value: team-azch\n        - name: MONGOHOST\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoHost\n        - name: MONGOUSER\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoUser\n        - name: MONGOPASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: mongodb\n              key: mongoPassword\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"captureorder\" is not set to runAsNonRoot"
  },
  {
    "id": "03944",
    "manifest_path": "data/manifests/the_stack_sample/sample_1766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: querier\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: loki\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: 2.2.0\n  name: observatorium-xyz-loki-querier\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: querier\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: loki\n      app.kubernetes.io/part-of: observatorium\n      loki.grafana.com/gossip: 'true'\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: querier\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: loki\n        app.kubernetes.io/part-of: observatorium\n        loki.grafana.com/gossip: 'true'\n    spec:\n      containers:\n      - args:\n        - -target=querier\n        - -config.file=/etc/loki/config/config.yaml\n        - -limits.per-user-override-config=/etc/loki/config/overrides.yaml\n        - -log.level=error\n        - -s3.url=$(S3_URL)\n        - -s3.force-path-style=true\n        - -distributor.replication-factor=1\n        env:\n        - name: S3_URL\n          valueFrom:\n            secretKeyRef:\n              key: endpoint\n              name: loki-objectstorage\n        image: docker.io/grafana/loki:2.2.0\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /metrics\n            port: 3100\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-xyz-loki-querier\n        ports:\n        - containerPort: 3100\n          name: metrics\n        - containerPort: 9095\n          name: grpc\n        - containerPort: 7946\n          name: gossip-ring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3100\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - mountPath: /etc/loki/config/\n          name: config\n          readOnly: false\n        - mountPath: /data\n          name: storage\n          readOnly: false\n      volumes:\n      - configMap:\n          name: observatorium-xyz-loki\n        name: config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"observatorium-xyz-loki-querier\" does not have a read-only root file system"
  },
  {
    "id": "03945",
    "manifest_path": "data/manifests/the_stack_sample/sample_1766.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  labels:\n    app.kubernetes.io/component: querier\n    app.kubernetes.io/instance: observatorium-xyz\n    app.kubernetes.io/name: loki\n    app.kubernetes.io/part-of: observatorium\n    app.kubernetes.io/version: 2.2.0\n  name: observatorium-xyz-loki-querier\n  namespace: observatorium\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: querier\n      app.kubernetes.io/instance: observatorium-xyz\n      app.kubernetes.io/name: loki\n      app.kubernetes.io/part-of: observatorium\n      loki.grafana.com/gossip: 'true'\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: querier\n        app.kubernetes.io/instance: observatorium-xyz\n        app.kubernetes.io/name: loki\n        app.kubernetes.io/part-of: observatorium\n        loki.grafana.com/gossip: 'true'\n    spec:\n      containers:\n      - args:\n        - -target=querier\n        - -config.file=/etc/loki/config/config.yaml\n        - -limits.per-user-override-config=/etc/loki/config/overrides.yaml\n        - -log.level=error\n        - -s3.url=$(S3_URL)\n        - -s3.force-path-style=true\n        - -distributor.replication-factor=1\n        env:\n        - name: S3_URL\n          valueFrom:\n            secretKeyRef:\n              key: endpoint\n              name: loki-objectstorage\n        image: docker.io/grafana/loki:2.2.0\n        livenessProbe:\n          failureThreshold: 10\n          httpGet:\n            path: /metrics\n            port: 3100\n            scheme: HTTP\n          periodSeconds: 30\n        name: observatorium-xyz-loki-querier\n        ports:\n        - containerPort: 3100\n          name: metrics\n        - containerPort: 9095\n          name: grpc\n        - containerPort: 7946\n          name: gossip-ring\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3100\n            scheme: HTTP\n          initialDelaySeconds: 15\n          timeoutSeconds: 1\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n        volumeMounts:\n        - mountPath: /etc/loki/config/\n          name: config\n          readOnly: false\n        - mountPath: /data\n          name: storage\n          readOnly: false\n      volumes:\n      - configMap:\n          name: observatorium-xyz-loki\n        name: config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"observatorium-xyz-loki-querier\" is not set to runAsNonRoot"
  },
  {
    "id": "03946",
    "manifest_path": "data/manifests/the_stack_sample/sample_1768.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failed-pod\nspec:\n  containers:\n  - name: motor\n    image: busybox\n    resources:\n      limits:\n        cpu: 100m\n        memory: 16Mi\n      requests:\n        cpu: 100m\n        memory: 16Mi\n    command:\n    - sh\n    - -c\n    args:\n    - echo \"This is failed phase\"; exit 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"motor\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03947",
    "manifest_path": "data/manifests/the_stack_sample/sample_1768.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failed-pod\nspec:\n  containers:\n  - name: motor\n    image: busybox\n    resources:\n      limits:\n        cpu: 100m\n        memory: 16Mi\n      requests:\n        cpu: 100m\n        memory: 16Mi\n    command:\n    - sh\n    - -c\n    args:\n    - echo \"This is failed phase\"; exit 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"motor\" does not have a read-only root file system"
  },
  {
    "id": "03948",
    "manifest_path": "data/manifests/the_stack_sample/sample_1768.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: failed-pod\nspec:\n  containers:\n  - name: motor\n    image: busybox\n    resources:\n      limits:\n        cpu: 100m\n        memory: 16Mi\n      requests:\n        cpu: 100m\n        memory: 16Mi\n    command:\n    - sh\n    - -c\n    args:\n    - echo \"This is failed phase\"; exit 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"motor\" is not set to runAsNonRoot"
  },
  {
    "id": "03949",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"salesforce-connector\" is using an invalid container image, \"gfwdockerhub/salesforce-connector\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03950",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"salesforce-connector\" does not have a read-only root file system"
  },
  {
    "id": "03951",
    "manifest_path": "data/manifests/the_stack_sample/sample_1771.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    name: salesforce-connector\n  name: salesforce-connector\n  namespace: gfw\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: salesforce-connector\n  template:\n    metadata:\n      labels:\n        name: salesforce-connector\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: type\n                operator: In\n                values:\n                - gfw\n      containers:\n      - args:\n        - start\n        env:\n        - name: PORT\n          value: '9500'\n        - name: LOGGER_TYPE\n          value: console\n        - name: LOGGER_LEVEL\n          value: debug\n        - name: NODE_ENV\n          value: staging\n        - name: NODE_PATH\n          value: app/src\n        - name: MICROSERVICE_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: MICROSERVICE_TOKEN\n              name: mssecrets\n        - name: GATEWAY_URL\n          valueFrom:\n            secretKeyRef:\n              key: GATEWAY_URL\n              name: mssecrets\n        - name: FASTLY_ENABLED\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_ENABLED\n              name: mssecrets\n        - name: FASTLY_APIKEY\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_APIKEY\n              name: mssecrets\n              optional: true\n        - name: FASTLY_SERVICEID\n          valueFrom:\n            secretKeyRef:\n              key: FASTLY_SERVICEID\n              name: mssecrets\n              optional: true\n        - name: SALESFORCE_URL\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_URL\n              name: mssecrets\n        - name: SALESFORCE_USERNAME\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_USERNAME\n              name: mssecrets\n        - name: SALESFORCE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              key: SALESFORCE_PASSWORD\n              name: mssecrets\n        image: gfwdockerhub/salesforce-connector\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: salesforce-connector\n        ports:\n        - containerPort: 9500\n          protocol: TCP\n        readinessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /healthcheck\n            port: 9500\n            scheme: HTTP\n          initialDelaySeconds: 30\n          periodSeconds: 15\n          successThreshold: 1\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: '1'\n            memory: 512M\n          requests:\n            cpu: 350m\n            memory: 256M\n      securityContext: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"salesforce-connector\" is not set to runAsNonRoot"
  },
  {
    "id": "03952",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"systemize-user-api\" is using an invalid container image, \"systemize-user-api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03953",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"systemize-user-api\" does not have a read-only root file system"
  },
  {
    "id": "03954",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"systemize-user-api\" is not set to runAsNonRoot"
  },
  {
    "id": "03955",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"systemize-user-api\" has cpu request 0"
  },
  {
    "id": "03956",
    "manifest_path": "data/manifests/the_stack_sample/sample_1774.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: systemize-user-api\nspec:\n  selector:\n    matchLabels:\n      app: systemize-user-api\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: systemize-user-api\n    spec:\n      containers:\n      - name: systemize-user-api\n        image: systemize-user-api:latest\n        ports:\n        - containerPort: 3000\n        livenessProbe:\n          initialDelaySeconds: 20\n          periodSeconds: 5\n          httpGet:\n            path: /health\n            port: 3000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"systemize-user-api\" has memory limit 0"
  },
  {
    "id": "03957",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03958",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03959",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03960",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03961",
    "manifest_path": "data/manifests/the_stack_sample/sample_1781.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-2476\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03962",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sensorapp\" is using an invalid container image, \"03021994/sensor:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03963",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sensorapp\" does not have a read-only root file system"
  },
  {
    "id": "03964",
    "manifest_path": "data/manifests/the_stack_sample/sample_1782.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sensordeploy\n  labels:\n    app: sensorapp\nspec:\n  selector:\n    matchLabels:\n      app: sensorapp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: sensorapp\n    spec:\n      containers:\n      - name: sensorapp\n        image: 03021994/sensor:latest\n        command:\n        - python\n        - ./app/sensor-test.py\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 5000\n        - containerPort: 8083\n        resources:\n          requests:\n            memory: 64Mi\n            cpu: 50m\n          limits:\n            memory: 256Mi\n            cpu: 500m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sensorapp\" is not set to runAsNonRoot"
  },
  {
    "id": "03965",
    "manifest_path": "data/manifests/the_stack_sample/sample_1785.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: percona-xtradb-cluster-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: operator\n      app.kubernetes.io/instance: percona-xtradb-cluster-operator\n      app.kubernetes.io/name: percona-xtradb-cluster-operator\n      app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: operator\n        app.kubernetes.io/instance: percona-xtradb-cluster-operator\n        app.kubernetes.io/name: percona-xtradb-cluster-operator\n        app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n    spec:\n      containers:\n      - command:\n        - percona-xtradb-cluster-operator\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: percona-xtradb-cluster-operator\n        image: perconalab/percona-xtradb-cluster-operator:main\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /metrics\n            port: metrics\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        name: percona-xtradb-cluster-operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n      serviceAccountName: percona-xtradb-cluster-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"percona-xtradb-cluster-operator\" does not have a read-only root file system"
  },
  {
    "id": "03966",
    "manifest_path": "data/manifests/the_stack_sample/sample_1785.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: percona-xtradb-cluster-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: operator\n      app.kubernetes.io/instance: percona-xtradb-cluster-operator\n      app.kubernetes.io/name: percona-xtradb-cluster-operator\n      app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/component: operator\n        app.kubernetes.io/instance: percona-xtradb-cluster-operator\n        app.kubernetes.io/name: percona-xtradb-cluster-operator\n        app.kubernetes.io/part-of: percona-xtradb-cluster-operator\n    spec:\n      containers:\n      - command:\n        - percona-xtradb-cluster-operator\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: percona-xtradb-cluster-operator\n        image: perconalab/percona-xtradb-cluster-operator:main\n        imagePullPolicy: Always\n        livenessProbe:\n          failureThreshold: 3\n          httpGet:\n            path: /metrics\n            port: metrics\n            scheme: HTTP\n        resources:\n          limits:\n            cpu: 200m\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 20Mi\n        name: percona-xtradb-cluster-operator\n        ports:\n        - containerPort: 8080\n          name: metrics\n          protocol: TCP\n      serviceAccountName: percona-xtradb-cluster-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"percona-xtradb-cluster-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "03967",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "03968",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "03969",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "03970",
    "manifest_path": "data/manifests/the_stack_sample/sample_1786.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20201015-deb1bd1036\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "03971",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"my-nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03972",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"my-nginx\" does not have a read-only root file system"
  },
  {
    "id": "03973",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"my-nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03974",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"my-nginx\" has cpu request 0"
  },
  {
    "id": "03975",
    "manifest_path": "data/manifests/the_stack_sample/sample_1788.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: my-nginx\nspec:\n  selector:\n    matchLabels:\n      run: my-nginx\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        run: my-nginx\n    spec:\n      containers:\n      - name: my-nginx\n        image: nginx\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"my-nginx\" has memory limit 0"
  },
  {
    "id": "03976",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ibms\" is using an invalid container image, \"ghcr.io/dbca-wa/ibms:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03977",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ibms\" does not have a read-only root file system"
  },
  {
    "id": "03978",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ibms\" is not set to runAsNonRoot"
  },
  {
    "id": "03979",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ibms\" has cpu request 0"
  },
  {
    "id": "03980",
    "manifest_path": "data/manifests/the_stack_sample/sample_1791.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ibms-deployment\n  labels:\n    app: ibms-uat\nspec:\n  selector:\n    matchLabels:\n      app: ibms-uat\n  template:\n    metadata:\n      labels:\n        app: ibms-uat\n    spec:\n      containers:\n      - name: ibms\n        image: ghcr.io/dbca-wa/ibms:latest\n        imagePullPolicy: Always\n        env:\n        - name: IBMS_URL\n          value: https://ibms-uat.dbca.wa.gov.au\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: DATABASE_URL\n        - name: SECRET_KEY\n          valueFrom:\n            secretKeyRef:\n              name: ibms-env-uat\n              key: SECRET_KEY\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ibms\" has memory limit 0"
  },
  {
    "id": "03981",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03982",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "03983",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "03984",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "03985",
    "manifest_path": "data/manifests/the_stack_sample/sample_1792.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-3123\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "03986",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cilium-agent\" is using an invalid container image, \"docker.io/cilium/cilium:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "03987",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "03988",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "03989",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03990",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "03991",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cilium-agent\" is privileged"
  },
  {
    "id": "03992",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"clean-cilium-state\" is privileged"
  },
  {
    "id": "03993",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "03994",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "03995",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "03996",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "03997",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "03998",
    "manifest_path": "data/manifests/the_stack_sample/sample_1793.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --container-runtime=crio\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/crio/crio.sock\n          name: crio-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /var/run/crio/crio.sock\n          type: Socket\n        name: crio-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "03999",
    "manifest_path": "data/manifests/the_stack_sample/sample_1794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: a337134d0e64ec21fde04859d87d697b47666d6c4d404b7e8f1eb39a41ffc05d\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: ranggaperwiratama\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 46bcf702811226edac90b77c43cdbfec99bc6fe6\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"lighthouse-foghorn\" does not have a read-only root file system"
  },
  {
    "id": "04000",
    "manifest_path": "data/manifests/the_stack_sample/sample_1794.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: lighthouse-foghorn\n  labels:\n    chart: lighthouse-1.1.52\n    app: lighthouse-foghorn\n    gitops.jenkins-x.io/pipeline: namespaces\n  annotations:\n    meta.helm.sh/release-name: lighthouse\n    wave.pusher.com/update-on-config-change: 'true'\n  namespace: jx\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: lighthouse-foghorn\n  template:\n    metadata:\n      labels:\n        app: lighthouse-foghorn\n      annotations:\n        jenkins-x.io/hash: a337134d0e64ec21fde04859d87d697b47666d6c4d404b7e8f1eb39a41ffc05d\n    spec:\n      serviceAccountName: lighthouse-foghorn\n      containers:\n      - name: lighthouse-foghorn\n        image: ghcr.io/jenkins-x/lighthouse-foghorn:1.1.52\n        imagePullPolicy: IfNotPresent\n        args:\n        - --namespace=jx\n        env:\n        - name: GIT_KIND\n          value: github\n        - name: GIT_SERVER\n          value: https://github.com\n        - name: GIT_USER\n          value: ranggaperwiratama\n        - name: GIT_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-oauth-token\n              key: oauth\n        - name: HMAC_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: lighthouse-hmac-token\n              key: hmac\n              optional: false\n        - name: JX_LOG_FORMAT\n          value: json\n        - name: LOGRUS_FORMAT\n          value: json\n        - name: LOGRUS_SERVICE\n          value: lighthouse\n        - name: LOGRUS_SERVICE_VERSION\n          value: 1.1.52\n        - name: LOGRUS_STACK_SKIP\n          value: ''\n        - name: DEFAULT_PIPELINE_RUN_SERVICE_ACCOUNT\n          value: tekton-bot\n        - name: DEFAULT_PIPELINE_RUN_TIMEOUT\n          value: 2h0m0s\n        - name: FILE_BROWSER\n          value: git\n        - name: JX_DEFAULT_IMAGE\n          value: ghcr.io/jenkins-x/builder-maven:2.1.149-768\n        - name: LIGHTHOUSE_DASHBOARD_TEMPLATE\n          value: namespaces/{{ .Namespace }}/pipelineruns/{{ .PipelineRun }}\n        - name: LIGHTHOUSE_VERSIONSTREAM_JENKINS_X_JX3_PIPELINE_CATALOG\n          value: 46bcf702811226edac90b77c43cdbfec99bc6fe6\n        resources:\n          limits:\n            cpu: 100m\n            memory: 256Mi\n          requests:\n            cpu: 80m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"lighthouse-foghorn\" is not set to runAsNonRoot"
  },
  {
    "id": "04001",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04002",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04003",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04004",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04005",
    "manifest_path": "data/manifests/the_stack_sample/sample_1795.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-9889\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04006",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"rollem-shard-20\" does not have a read-only root file system"
  },
  {
    "id": "04007",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"rollem-shard-20\" is not set to runAsNonRoot"
  },
  {
    "id": "04008",
    "manifest_path": "data/manifests/the_stack_sample/sample_1801.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: rollem-prod-20-deployment\nspec:\n  selector:\n    matchLabels:\n      app: rollem-prod\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: rollem-prod\n    spec:\n      containers:\n      - name: rollem-shard-20\n        image: lemtzas/rollem-discord:2.6.4\n        resources:\n          requests:\n            cpu: 50m\n            memory: 250M\n        env:\n        - name: reboot\n          value: '2021-04-20'\n        - name: DISCORD_BOT_SHARD_ID\n          value: '20'\n        - name: DISCORD_BOT_SHARD_COUNT\n          value: '50'\n        - name: DISCORD_BOT_USER_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DISCORD_BOT_USER_TOKEN\n        - name: APPINSIGHTS_CONNECTIONSTRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: APPINSIGHTS_CONNECTIONSTRING\n        - name: DEFER_TO_CLIENT_IDS\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DEFER_TO_CLIENT_IDS\n        - name: DB_CONNECTION_STRING\n          valueFrom:\n            secretKeyRef:\n              name: rollem-prod-2\n              key: DB_CONNECTION_STRING\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"rollem-shard-20\" has memory limit 0"
  },
  {
    "id": "04009",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"digilocker-support-api\" is using an invalid container image, \"REGISTRY/digilocker_support_api:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04010",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"digilocker-support-api\" does not have a read-only root file system"
  },
  {
    "id": "04011",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"digilocker-support-api\" is not set to runAsNonRoot"
  },
  {
    "id": "04012",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"digilocker-support-api\" has cpu request 0"
  },
  {
    "id": "04013",
    "manifest_path": "data/manifests/the_stack_sample/sample_1802.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    k8s-app: digilocker-support-api\n  name: digilocker-support-api\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      k8s-app: digilocker-support-api\n  template:\n    metadata:\n      labels:\n        k8s-app: digilocker-support-api\n    spec:\n      containers:\n      - image: REGISTRY/digilocker_support_api:latest\n        imagePullPolicy: Always\n        name: digilocker-support-api\n        envFrom:\n        - configMapRef:\n            name: divoc-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"digilocker-support-api\" has memory limit 0"
  },
  {
    "id": "04014",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "04015",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "04016",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "04017",
    "manifest_path": "data/manifests/the_stack_sample/sample_1803.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  namespace: prow\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20210723-55eee17612\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --only=ti-community-infra/tichi,ti-community-infra/configs,ti-community-infra/ti-community-bot,ti-community-infra/ti-challenge-bot,ti-community-infra/rfcs,ti-community-infra/devstats,ti-community-infra/devstats-dev-guide,pingcap/community,pingcap/docs,pingcap/docs-cn,pingcap/docs-dm,pingcap/docs-tidb-operator,pingcap/ticdc,tikv/community,tikv/pd,chaos-mesh/website,chaos-mesh/website-zh\n          - --token=/etc/github/token\n          volumeMounts:\n          - name: github-token\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: github-token\n          secret:\n            secretName: github-token\n        - name: config\n          configMap:\n            name: labels-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "04018",
    "manifest_path": "data/manifests/the_stack_sample/sample_1810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: smtp\n  name: smtp\n  namespace: utils\nspec:\n  selector:\n    matchLabels:\n      app: smtp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: smtp\n    spec:\n      containers:\n      - image: devture/exim-relay:4.94.2-r0-3\n        imagePullPolicy: Always\n        name: smtp\n        env:\n        - name: TZ\n          value: Europe/Zurich\n        - name: smtp.int.eighty-three.me\n        - name: SMARTHOST\n          value: smtp.sendgrid.net::587\n        - name: SMTP_USERNAME\n          value: apikey\n        - name: SMTP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: smtp-password\n              key: password\n        resources:\n          requests:\n            cpu: 10m\n            memory: 10Mi\n          limits:\n            cpu: 400m\n            memory: 100Mi\n        ports:\n        - containerPort: 8025\n        livenessProbe:\n          exec:\n            command:\n            - exim\n            - -bt\n            - test@tuxpeople.org\n          initialDelaySeconds: 10\n          periodSeconds: 10\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smtp\" does not have a read-only root file system"
  },
  {
    "id": "04019",
    "manifest_path": "data/manifests/the_stack_sample/sample_1810.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: smtp\n  name: smtp\n  namespace: utils\nspec:\n  selector:\n    matchLabels:\n      app: smtp\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: smtp\n    spec:\n      containers:\n      - image: devture/exim-relay:4.94.2-r0-3\n        imagePullPolicy: Always\n        name: smtp\n        env:\n        - name: TZ\n          value: Europe/Zurich\n        - name: smtp.int.eighty-three.me\n        - name: SMARTHOST\n          value: smtp.sendgrid.net::587\n        - name: SMTP_USERNAME\n          value: apikey\n        - name: SMTP_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: smtp-password\n              key: password\n        resources:\n          requests:\n            cpu: 10m\n            memory: 10Mi\n          limits:\n            cpu: 400m\n            memory: 100Mi\n        ports:\n        - containerPort: 8025\n        livenessProbe:\n          exec:\n            command:\n            - exim\n            - -bt\n            - test@tuxpeople.org\n          initialDelaySeconds: 10\n          periodSeconds: 10\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smtp\" is not set to runAsNonRoot"
  },
  {
    "id": "04020",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"motive-back-end-container\" is using an invalid container image, \"$FULL_IMAGE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04021",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"motive-back-end-container\" does not have a read-only root file system"
  },
  {
    "id": "04022",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"motive-back-end-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04023",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"motive-back-end-container\" has cpu request 0"
  },
  {
    "id": "04024",
    "manifest_path": "data/manifests/the_stack_sample/sample_1811.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: motive-back-end-deployment\nspec:\n  template:\n    metadata:\n      name: motive-back-end-pod\n      labels:\n        app: motive-back-end\n    spec:\n      containers:\n      - name: motive-back-end-container\n        image: $FULL_IMAGE\n        imagePullPolicy: Always\n        env:\n        - name: DATABASE_URL\n          value: $DATABASE_JDBC_URL\n        - name: DATABASE_USERNAME\n          valueFrom:\n            configMapKeyRef:\n              name: motive-config\n              key: postgres-username\n              optional: false\n        - name: DATABASE_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: $APP_DATABASE_NAME-postgresql\n              key: postgres-password\n              optional: false\n        ports:\n        - containerPort: 8080\n        readinessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n        livenessProbe:\n          initialDelaySeconds: 120\n          httpGet:\n            port: 8080\n            path: /actuator/health\n  replicas: 2\n  selector:\n    matchLabels:\n      app: motive-back-end\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"motive-back-end-container\" has memory limit 0"
  },
  {
    "id": "04025",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-sriovdp\" does not have a read-only root file system"
  },
  {
    "id": "04026",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"kube-sriovdp\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04027",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"kube-sriovdp\" is privileged"
  },
  {
    "id": "04028",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-sriovdp\" is not set to runAsNonRoot"
  },
  {
    "id": "04029",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"kube-sriovdp\" has cpu request 0"
  },
  {
    "id": "04030",
    "manifest_path": "data/manifests/the_stack_sample/sample_1812.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: kube-sriov-device-plugin-amd64\n  namespace: kube-system\n  labels:\n    tier: node\n    app: sriovdp\nspec:\n  selector:\n    matchLabels:\n      name: sriov-device-plugin\n  template:\n    metadata:\n      labels:\n        name: sriov-device-plugin\n        tier: node\n        app: sriovdp\n    spec:\n      serviceAccountName: sriov-device-plugin\n      containers:\n      - name: kube-sriovdp\n        image: docker.io/nfvpe/sriov-device-plugin:v3.1\n        imagePullPolicy: Never\n        args:\n        - --log-dir=sriovdp\n        - --log-level=10\n        securityContext:\n          privileged: true\n        volumeMounts:\n        - name: devicesock\n          mountPath: /var/lib/kubelet/\n          readOnly: false\n        - name: log\n          mountPath: /var/log\n        - name: config-volume\n          mountPath: /etc/pcidp\n      volumes:\n      - name: devicesock\n        hostPath:\n          path: /var/lib/kubelet/\n      - name: log\n        hostPath:\n          path: /var/log\n      - name: config-volume\n        configMap:\n          name: sriovdp-config\n          items:\n          - key: config.json\n            path: config.json\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-sriovdp\" has memory limit 0"
  },
  {
    "id": "04031",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"goproxy\" does not have a read-only root file system"
  },
  {
    "id": "04032",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iproute-add\" does not have a read-only root file system"
  },
  {
    "id": "04033",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"testground-daemon\" does not have a read-only root file system"
  },
  {
    "id": "04034",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"iproute-add\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04035",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"testground-daemon\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04036",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"iproute-add\" is privileged"
  },
  {
    "id": "04037",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"testground-daemon\" is privileged"
  },
  {
    "id": "04038",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"goproxy\" is not set to runAsNonRoot"
  },
  {
    "id": "04039",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iproute-add\" is not set to runAsNonRoot"
  },
  {
    "id": "04040",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"testground-daemon\" is not set to runAsNonRoot"
  },
  {
    "id": "04041",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"iproute-add\" has cpu request 0"
  },
  {
    "id": "04042",
    "manifest_path": "data/manifests/the_stack_sample/sample_1816.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: testground-daemon\n  labels:\n    app: testground-daemon\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: testground-daemon\n  template:\n    metadata:\n      labels:\n        app: testground-daemon\n    spec:\n      serviceAccountName: testground-daemon\n      initContainers:\n      - name: iproute-add\n        image: busybox:1.31.1\n        securityContext:\n          privileged: true\n        command:\n        - sh\n        - -ac\n        - 'while [ \"$GW\" = \"\" ]; do export GW=$(ip route | grep cni0 | awk ''{print\n          $7}''); echo \"Got GW: $GW\"; sleep 5; done; echo $GW && ip route && ip route\n          add 100.64.0.0/16 via $GW && ip route || true;\n\n          '\n      containers:\n      - name: goproxy\n        image: iptestground/goproxy:2.0.2\n        ports:\n        - containerPort: 8081\n          hostPort: 8081\n        volumeMounts:\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        - name: efs-pvc\n          mountPath: /go\n        resources:\n          requests:\n            memory: 512Mi\n            cpu: 500m\n          limits:\n            memory: 512Mi\n      - name: testground-daemon\n        image: iptestground/testground:edge\n        imagePullPolicy: Always\n        env:\n        - name: REDIS_HOST\n          value: testground-infra-redis-headless\n        securityContext:\n          privileged: true\n        ports:\n        - containerPort: 8042\n          hostPort: 8042\n        volumeMounts:\n        - name: daemon-datadir\n          mountPath: /root/testground/\n        - name: efs-pvc\n          mountPath: /efs\n        - name: dockersock\n          mountPath: /var/run/docker.sock\n        - name: envtoml\n          mountPath: /root/testground/.env.toml\n          subPath: .env.toml\n        resources:\n          requests:\n            memory: 2048Mi\n            cpu: 2000m\n          limits:\n            memory: 2048Mi\n      volumes:\n      - name: efs-pvc\n        persistentVolumeClaim:\n          claimName: efs\n      - name: daemon-datadir\n        persistentVolumeClaim:\n          claimName: testground-daemon-datadir-pvc\n      - name: dockersock\n        hostPath:\n          path: /var/run/docker.sock\n      - name: envtoml\n        configMap:\n          name: env-toml-cfg\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"iproute-add\" has memory limit 0"
  },
  {
    "id": "04043",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"shippingservice\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04044",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "04045",
    "manifest_path": "data/manifests/the_stack_sample/sample_1819.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: shippingservice\nspec:\n  selector:\n    matchLabels:\n      app: shippingservice\n  template:\n    metadata:\n      labels:\n        app: shippingservice\n    spec:\n      containers:\n      - name: server\n        image: shippingservice\n        ports:\n        - containerPort: 50051\n        env:\n        - name: PORT\n          value: '50051'\n        readinessProbe:\n          periodSeconds: 5\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        livenessProbe:\n          exec:\n            command:\n            - /bin/grpc_health_probe\n            - -addr=:50051\n        resources:\n          requests:\n            cpu: 100m\n            memory: 64Mi\n          limits:\n            cpu: 200m\n            memory: 128Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "04046",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"strimzi-user-operator\" does not have a read-only root file system"
  },
  {
    "id": "04047",
    "manifest_path": "data/manifests/the_stack_sample/sample_1823.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: strimzi-user-operator\n  labels:\n    app: strimzi\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: strimzi-user-operator\n  template:\n    metadata:\n      labels:\n        name: strimzi-user-operator\n    spec:\n      serviceAccountName: strimzi-user-operator\n      containers:\n      - name: strimzi-user-operator\n        image: quay.io/strimzi/operator:0.25.0\n        args:\n        - /opt/strimzi/bin/user_operator_run.sh\n        env:\n        - name: STRIMZI_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: STRIMZI_LABELS\n          value: strimzi.io/cluster=my-cluster\n        - name: STRIMZI_CA_CERT_NAME\n          value: my-cluster-clients-ca-cert\n        - name: STRIMZI_CA_KEY_NAME\n          value: my-cluster-clients-ca\n        - name: STRIMZI_FULL_RECONCILIATION_INTERVAL_MS\n          value: '120000'\n        - name: STRIMZI_LOG_LEVEL\n          value: INFO\n        - name: STRIMZI_GC_LOG_ENABLED\n          value: 'true'\n        - name: STRIMZI_CA_VALIDITY\n          value: '365'\n        - name: STRIMZI_CA_RENEWAL\n          value: '30'\n        livenessProbe:\n          httpGet:\n            path: /healthy\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 30\n        resources:\n          limits:\n            memory: 256Mi\n            cpu: 500m\n          requests:\n            memory: 256Mi\n            cpu: 100m\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"strimzi-user-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "04048",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sistema-noticias\" does not have a read-only root file system"
  },
  {
    "id": "04049",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sistema-noticias\" is not set to runAsNonRoot"
  },
  {
    "id": "04050",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sistema-noticias\" has cpu request 0"
  },
  {
    "id": "04051",
    "manifest_path": "data/manifests/the_stack_sample/sample_1825.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: sistema-noticias-statefulset\nspec:\n  selector:\n    matchLabels:\n      app: sistema-noticias\n  template:\n    metadata:\n      name: sistema-noticias\n      labels:\n        app: sistema-noticias\n    spec:\n      containers:\n      - name: sistema-noticias\n        image: aluracursos/sistema-noticias:1\n        ports:\n        - containerPort: 80\n        envFrom:\n        - configMapRef:\n            name: sistema-noticias-config-map\n        volumeMounts:\n        - mountPath: /var/www/html/uploads\n          name: imagens-pvc\n        - mountPath: /tmp\n          name: sessao-pvc\n      volumes:\n      - name: imagens-pvc\n        persistentVolumeClaim:\n          claimName: imagens-pvc\n      - name: sessao-pvc\n        persistentVolumeClaim:\n          claimName: sessao-pvc\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sistema-noticias\" has memory limit 0"
  },
  {
    "id": "04052",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"api\" is using an invalid container image, \"agus7fauzi/hari\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04053",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"api\" does not have a read-only root file system"
  },
  {
    "id": "04054",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"api\" is not set to runAsNonRoot"
  },
  {
    "id": "04055",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"api\" has cpu request 0"
  },
  {
    "id": "04056",
    "manifest_path": "data/manifests/the_stack_sample/sample_1831.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: micro-api\n  labels:\n    micro: runtime\n    name: micro-api\n  annotations:\n    name: go.micro.api\n    version: latest\n    source: github.com/xinhari/hari\n    owner: micro\n    group: micro\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: micro-api\n      micro: runtime\n  template:\n    metadata:\n      labels:\n        name: micro-api\n        micro: runtime\n    spec:\n      containers:\n      - name: api\n        env:\n        - name: MICRO_AUTH\n          value: service\n        - name: MICRO_AUTH_PUBLIC_KEY\n          valueFrom:\n            secretKeyRef:\n              name: micro-keypair\n              key: public\n        - name: MICRO_API_NAMESPACE\n          value: domain\n        - name: MICRO_ENABLE_STATS\n          value: 'true'\n        - name: MICRO_BROKER\n          value: nats\n        - name: MICRO_BROKER_ADDRESS\n          value: nats-cluster\n        - name: MICRO_REGISTRY\n          value: etcd\n        - name: MICRO_REGISTRY_ADDRESS\n          value: etcd-cluster-client\n        - name: MICRO_REGISTER_TTL\n          value: '60'\n        - name: MICRO_REGISTER_INTERVAL\n          value: '30'\n        - name: MICRO_ENABLE_ACME\n          value: 'true'\n        - name: MICRO_ACME_PROVIDER\n          value: certmagic\n        - name: MICRO_ACME_HOSTS\n          value: '*.micro.mu,*.cloud.micro.mu,micro.mu'\n        - name: MICRO_STORE\n          value: service\n        - name: MICRO_STORE_DATABASE\n          value: micro\n        - name: MICRO_STORE_TABLE\n          value: micro\n        - name: CF_API_TOKEN\n          valueFrom:\n            secretKeyRef:\n              key: CF_API_TOKEN\n              name: cloudflare-credentials\n        args:\n        - api\n        image: agus7fauzi/hari\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 443\n          name: api-port\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"api\" has memory limit 0"
  },
  {
    "id": "04057",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04058",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04059",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04060",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04061",
    "manifest_path": "data/manifests/the_stack_sample/sample_1835.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: app-a1\n  namespace: test-kahoy\n  labels:\n    app: app-a1\n  annotations:\n    app: app-a1\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: app-a1\n  template:\n    metadata:\n      labels:\n        app: app-a1\n    spec:\n      containers:\n      - name: nginx\n        image: nginx:latest\n        ports:\n        - name: http\n          containerPort: 80\n          protocol: TCP\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04062",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"downloader\" is using an invalid container image, \"$CORTEX_IMAGE_DOWNLOADER\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04063",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"python-predictor-cpu\" is using an invalid container image, \"$CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04064",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tensorflow-predictor\" is using an invalid container image, \"$CORTEX_IMAGE_TENSORFLOW_PREDICTOR\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04065",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"tensorflow-serving-cpu\" is using an invalid container image, \"$CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04066",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"downloader\" does not have a read-only root file system"
  },
  {
    "id": "04067",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"python-predictor-cpu\" does not have a read-only root file system"
  },
  {
    "id": "04068",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tensorflow-predictor\" does not have a read-only root file system"
  },
  {
    "id": "04069",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"tensorflow-serving-cpu\" does not have a read-only root file system"
  },
  {
    "id": "04070",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"downloader\" is not set to runAsNonRoot"
  },
  {
    "id": "04071",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"python-predictor-cpu\" is not set to runAsNonRoot"
  },
  {
    "id": "04072",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tensorflow-predictor\" is not set to runAsNonRoot"
  },
  {
    "id": "04073",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"tensorflow-serving-cpu\" is not set to runAsNonRoot"
  },
  {
    "id": "04074",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"downloader\" has cpu request 0"
  },
  {
    "id": "04075",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"python-predictor-cpu\" has cpu request 0"
  },
  {
    "id": "04076",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tensorflow-predictor\" has cpu request 0"
  },
  {
    "id": "04077",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"tensorflow-serving-cpu\" has cpu request 0"
  },
  {
    "id": "04078",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"downloader\" has memory limit 0"
  },
  {
    "id": "04079",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"python-predictor-cpu\" has memory limit 0"
  },
  {
    "id": "04080",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tensorflow-predictor\" has memory limit 0"
  },
  {
    "id": "04081",
    "manifest_path": "data/manifests/the_stack_sample/sample_1838.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: image-downloader\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      name: image-downloader\n  template:\n    metadata:\n      labels:\n        name: image-downloader\n    spec:\n      containers:\n      - name: python-predictor-cpu\n        image: $CORTEX_IMAGE_PYTHON_PREDICTOR_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-serving-cpu\n        image: $CORTEX_IMAGE_TENSORFLOW_SERVING_CPU\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: tensorflow-predictor\n        image: $CORTEX_IMAGE_TENSORFLOW_PREDICTOR\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n      - name: downloader\n        image: $CORTEX_IMAGE_DOWNLOADER\n        command:\n        - /bin/sh\n        args:\n        - -c\n        - sleep 1000000\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"tensorflow-serving-cpu\" has memory limit 0"
  },
  {
    "id": "04082",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"yeah-it-works\" does not have a read-only root file system"
  },
  {
    "id": "04083",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"yeah-it-works\" is not set to runAsNonRoot"
  },
  {
    "id": "04084",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"yeah-it-works\" has cpu request 0"
  },
  {
    "id": "04085",
    "manifest_path": "data/manifests/the_stack_sample/sample_1840.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: yeah-it-works\nspec:\n  template:\n    spec:\n      containers:\n      - name: yeah-it-works\n        image: python:3.6-alpine\n        command:\n        - python\n        - -c\n        - print('Yeah, it works in a Job!!!')\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"yeah-it-works\" has memory limit 0"
  },
  {
    "id": "04086",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"modeldb-artifact-store\" does not have a read-only root file system"
  },
  {
    "id": "04087",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"modeldb-artifact-store\" is not set to runAsNonRoot"
  },
  {
    "id": "04088",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"modeldb-artifact-store\" has cpu request 0"
  },
  {
    "id": "04089",
    "manifest_path": "data/manifests/the_stack_sample/sample_1844.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: modeldb\n  name: modeldb-artifact-store\nspec:\n  selector:\n    matchLabels:\n      app: modeldb\n      tier: artifact-store\n  template:\n    metadata:\n      labels:\n        app: modeldb\n        tier: artifact-store\n      annotations:\n        sidecar.istio.io/inject: 'true'\n    spec:\n      containers:\n      - env:\n        - name: VERTA_ARTIFACT_CONFIG\n          value: /config/config.yaml\n        image: vertaaiofficial/modeldb-artifact-store:kubeflow\n        imagePullPolicy: Always\n        name: modeldb-artifact-store\n        ports:\n        - containerPort: 8086\n        volumeMounts:\n        - mountPath: /config\n          name: modeldb-artifact-store-config\n          readOnly: true\n      volumes:\n      - configMap:\n          name: modeldb-artifact-store-config\n        name: modeldb-artifact-store-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"modeldb-artifact-store\" has memory limit 0"
  },
  {
    "id": "04090",
    "manifest_path": "data/manifests/the_stack_sample/sample_1853.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: compliance-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: compliance-operator\n  template:\n    metadata:\n      labels:\n        name: compliance-operator\n      annotations:\n        workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n    spec:\n      serviceAccountName: compliance-operator\n      containers:\n      - name: compliance-operator\n        image: quay.io/compliance-operator/compliance-operator:latest\n        command:\n        - compliance-operator\n        - operator\n        imagePullPolicy: Always\n        securityContext:\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n          limits:\n            memory: 200Mi\n            cpu: 100m\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: compliance-operator\n        - name: RELATED_IMAGE_OPENSCAP\n          value: quay.io/compliance-operator/openscap-ocp:1.3.5\n        - name: RELATED_IMAGE_OPERATOR\n          value: quay.io/compliance-operator/compliance-operator:latest\n        - name: RELATED_IMAGE_PROFILE\n          value: quay.io/complianceascode/ocp4:latest\n        volumeMounts:\n        - name: serving-cert\n          mountPath: /var/run/secrets/serving-cert\n          readOnly: true\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: compliance-operator-serving-cert\n          optional: true\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"compliance-operator\" is using an invalid container image, \"quay.io/compliance-operator/compliance-operator:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04091",
    "manifest_path": "data/manifests/the_stack_sample/sample_1853.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: compliance-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: compliance-operator\n  template:\n    metadata:\n      labels:\n        name: compliance-operator\n      annotations:\n        workload.openshift.io/management: '{\"effect\": \"PreferredDuringScheduling\"}'\n    spec:\n      serviceAccountName: compliance-operator\n      containers:\n      - name: compliance-operator\n        image: quay.io/compliance-operator/compliance-operator:latest\n        command:\n        - compliance-operator\n        - operator\n        imagePullPolicy: Always\n        securityContext:\n          readOnlyRootFilesystem: true\n          allowPrivilegeEscalation: false\n        resources:\n          requests:\n            memory: 20Mi\n            cpu: 10m\n          limits:\n            memory: 200Mi\n            cpu: 100m\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: compliance-operator\n        - name: RELATED_IMAGE_OPENSCAP\n          value: quay.io/compliance-operator/openscap-ocp:1.3.5\n        - name: RELATED_IMAGE_OPERATOR\n          value: quay.io/compliance-operator/compliance-operator:latest\n        - name: RELATED_IMAGE_PROFILE\n          value: quay.io/complianceascode/ocp4:latest\n        volumeMounts:\n        - name: serving-cert\n          mountPath: /var/run/secrets/serving-cert\n          readOnly: true\n      volumes:\n      - name: serving-cert\n        secret:\n          secretName: compliance-operator-serving-cert\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"compliance-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "04092",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cinder-csi-plugin\" is using an invalid container image, \"docker.io/k8scloudprovider/cinder-csi-plugin:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04093",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cinder-csi-plugin\" does not have a read-only root file system"
  },
  {
    "id": "04094",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "04095",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cinder-csi-plugin\" has AllowPrivilegeEscalation set to true."
  },
  {
    "id": "04096",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cinder-csi-plugin\" is privileged"
  },
  {
    "id": "04097",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cinder-csi-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "04098",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "04099",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cinder-csi-plugin\" has cpu request 0"
  },
  {
    "id": "04100",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-driver-registrar\" has cpu request 0"
  },
  {
    "id": "04101",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cinder-csi-plugin\" has memory limit 0"
  },
  {
    "id": "04102",
    "manifest_path": "data/manifests/the_stack_sample/sample_1854.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: csi-cinder-nodeplugin\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-cinder-nodeplugin\n  template:\n    metadata:\n      labels:\n        app: csi-cinder-nodeplugin\n    spec:\n      serviceAccount: csi-cinder-node-sa\n      containers:\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v1.1.0\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: cinder-csi-plugin\n        securityContext:\n          privileged: true\n          capabilities:\n            add:\n            - SYS_ADMIN\n          allowPrivilegeEscalation: true\n        image: docker.io/k8scloudprovider/cinder-csi-plugin:latest\n        args:\n        - /bin/cinder-csi-plugin\n        - --nodeid=$(NODE_ID)\n        - --endpoint=$(CSI_ENDPOINT)\n        - --cloud-config=$(CLOUD_CONFIG)\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/config/cloud.conf\n        imagePullPolicy: IfNotPresent\n        volumeMounts:\n        - name: socket-dir\n          mountPath: /csi\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: pods-cloud-data\n          mountPath: /var/lib/cloud/data\n          readOnly: true\n        - name: pods-probe-dir\n          mountPath: /dev\n          mountPropagation: HostToContainer\n        - name: secret-cinderplugin\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: socket-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/cinder.csi.openstack.org\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: pods-cloud-data\n        hostPath:\n          path: /var/lib/cloud/data\n          type: Directory\n      - name: pods-probe-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: secret-cinderplugin\n        secret:\n          secretName: cloud-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "04103",
    "manifest_path": "data/manifests/the_stack_sample/sample_1855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: podinfo-ds\n  labels:\n    app.kubernetes.io/name: podinfo-ds\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: podinfo-ds\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app.kubernetes.io/name: podinfo-ds\n    spec:\n      containers:\n      - name: podinfod\n        image: ghcr.io/stefanprodan/podinfo:6.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        livenessProbe:\n          httpGet:\n            port: 9898\n            path: /healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            port: 9898\n            path: /readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"podinfod\" does not have a read-only root file system"
  },
  {
    "id": "04104",
    "manifest_path": "data/manifests/the_stack_sample/sample_1855.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: podinfo-ds\n  labels:\n    app.kubernetes.io/name: podinfo-ds\nspec:\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: podinfo-ds\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9797'\n      labels:\n        app.kubernetes.io/name: podinfo-ds\n    spec:\n      containers:\n      - name: podinfod\n        image: ghcr.io/stefanprodan/podinfo:6.0.0\n        imagePullPolicy: IfNotPresent\n        ports:\n        - name: http\n          containerPort: 9898\n          protocol: TCP\n        - name: http-metrics\n          containerPort: 9797\n          protocol: TCP\n        - name: grpc\n          containerPort: 9999\n          protocol: TCP\n        command:\n        - ./podinfo\n        - --port=9898\n        - --port-metrics=9797\n        - --grpc-port=9999\n        - --grpc-service-name=podinfo\n        - --level=info\n        - --random-delay=false\n        - --random-error=false\n        livenessProbe:\n          httpGet:\n            port: 9898\n            path: /healthz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        readinessProbe:\n          httpGet:\n            port: 9898\n            path: /readyz\n          initialDelaySeconds: 5\n          timeoutSeconds: 5\n        resources:\n          limits:\n            cpu: 1000m\n            memory: 128Mi\n          requests:\n            cpu: 1m\n            memory: 16Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"podinfod\" is not set to runAsNonRoot"
  },
  {
    "id": "04105",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04106",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04107",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04108",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04109",
    "manifest_path": "data/manifests/the_stack_sample/sample_1864.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7290\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04110",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"minio-client\" is using an invalid container image, \"minio/mc\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04111",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"minio-client\" does not have a read-only root file system"
  },
  {
    "id": "04112",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"minio-client\" is not set to runAsNonRoot"
  },
  {
    "id": "04113",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"minio-client\" has cpu request 0"
  },
  {
    "id": "04114",
    "manifest_path": "data/manifests/the_stack_sample/sample_1865.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: example-minio-data\nspec:\n  template:\n    spec:\n      containers:\n      - name: minio-client\n        image: minio/mc\n        command:\n        - /bin/sh\n        - -c\n        - 'sleep 100 && mc config host add minio-service http://minio-service:9000\n          minio minio123 && mc mb minio-service/my-bucket && echo ''hello'' > file1.txt\n          && echo ''world'' > file2.txt && mc cp *.txt minio-service/my-bucket '\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"minio-client\" has memory limit 0"
  },
  {
    "id": "04115",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"base\" is using an invalid container image, \"dummy_image\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04116",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"base\" does not have a read-only root file system"
  },
  {
    "id": "04117",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"base\" has cpu request 0"
  },
  {
    "id": "04118",
    "manifest_path": "data/manifests/the_stack_sample/sample_1866.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: dummy-name\nspec:\n  containers:\n  - env:\n    - name: AIRFLOW__CORE__EXECUTOR\n      value: LocalExecutor\n    - name: AIRFLOW__CORE__FERNET_KEY\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-fernet-key\n          key: fernet-key\n    - name: AIRFLOW__DATABASE__SQL_ALCHEMY_CONN\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    - name: AIRFLOW_CONN_AIRFLOW_DB\n      valueFrom:\n        secretKeyRef:\n          name: RELEASE-NAME-airflow-metadata\n          key: connection\n    image: dummy_image\n    imagePullPolicy: IfNotPresent\n    name: base\n    volumeMounts:\n    - mountPath: /opt/airflow/logs\n      name: airflow-logs\n    - mountPath: /opt/airflow/airflow.cfg\n      name: airflow-config\n      readOnly: true\n      subPath: airflow.cfg\n  securityContext:\n    runAsUser: 50000\n    fsGroup: 50000\n  serviceAccountName: RELEASE-NAME-worker-serviceaccount\n  volumes:\n  - emptyDir: {}\n    name: airflow-logs\n  - configMap:\n      name: RELEASE-NAME-airflow-config\n    name: airflow-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"base\" has memory limit 0"
  },
  {
    "id": "04119",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vote\" does not have a read-only root file system"
  },
  {
    "id": "04120",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vote\" is not set to runAsNonRoot"
  },
  {
    "id": "04121",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vote\" has cpu request 0"
  },
  {
    "id": "04122",
    "manifest_path": "data/manifests/the_stack_sample/sample_1868.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vote\n  labels:\n    app: vote\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: vote\n      env: dev\n  template:\n    metadata:\n      labels:\n        app: vote\n        env: dev\n    spec:\n      containers:\n      - name: vote\n        image: dockersamples/examplevotingapp_vote:before\n        ports:\n        - containerPort: 80\n          name: vote\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vote\" has memory limit 0"
  },
  {
    "id": "04123",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04124",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04125",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04126",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04127",
    "manifest_path": "data/manifests/the_stack_sample/sample_1870.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6441\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04128",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"main\" does not have a read-only root file system"
  },
  {
    "id": "04129",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"main\" is not set to runAsNonRoot"
  },
  {
    "id": "04130",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"main\" has cpu request 0"
  },
  {
    "id": "04131",
    "manifest_path": "data/manifests/the_stack_sample/sample_1872.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: org2-install-k8s-builder\nspec:\n  template:\n    metadata:\n      name: org2-install-k8s-builder\n    spec:\n      containers:\n      - name: main\n        image: ghcr.io/hyperledgendary/k8s-fabric-peer:${K8S_CHAINCODE_BUILDER_VERSION}\n        imagePullPolicy: IfNotPresent\n        command:\n        - sh\n        - -c\n        - mkdir -p /mnt/fabric-org2/fabric/external_builders && cp -rv /opt/hyperledger/k8s_builder\n          /mnt/fabric-org2/fabric/external_builders/\n        volumeMounts:\n        - name: fabric-org2-volume\n          mountPath: /mnt/fabric-org2\n      volumes:\n      - name: fabric-org2-volume\n        persistentVolumeClaim:\n          claimName: fabric-org2\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"main\" has memory limit 0"
  },
  {
    "id": "04132",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"server\" is using an invalid container image, \"k3s-cluster-docs\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04133",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"server\" does not have a read-only root file system"
  },
  {
    "id": "04134",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"server\" is not set to runAsNonRoot"
  },
  {
    "id": "04135",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"server\" has cpu request 0"
  },
  {
    "id": "04136",
    "manifest_path": "data/manifests/the_stack_sample/sample_1879.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: k3s-cluster-doc\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: k3s-cluster-docs\n  template:\n    metadata:\n      labels:\n        app: k3s-cluster-docs\n    spec:\n      containers:\n      - name: server\n        image: k3s-cluster-docs\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"server\" has memory limit 0"
  },
  {
    "id": "04137",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04138",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04139",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04140",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04141",
    "manifest_path": "data/manifests/the_stack_sample/sample_1880.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6664\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04142",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-attacher\" does not have a read-only root file system"
  },
  {
    "id": "04143",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-provisioner\" does not have a read-only root file system"
  },
  {
    "id": "04144",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "04145",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sfs-turbo-csi-plugin\" does not have a read-only root file system"
  },
  {
    "id": "04146",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-attacher\" is not set to runAsNonRoot"
  },
  {
    "id": "04147",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-provisioner\" is not set to runAsNonRoot"
  },
  {
    "id": "04148",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "04149",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sfs-turbo-csi-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "04150",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-attacher\" has cpu request 0"
  },
  {
    "id": "04151",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"csi-provisioner\" has cpu request 0"
  },
  {
    "id": "04152",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "04153",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sfs-turbo-csi-plugin\" has cpu request 0"
  },
  {
    "id": "04154",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-attacher\" has memory limit 0"
  },
  {
    "id": "04155",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"csi-provisioner\" has memory limit 0"
  },
  {
    "id": "04156",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "04157",
    "manifest_path": "data/manifests/the_stack_sample/sample_1884.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: csi-sfs-turbo-controller\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      app: csi-sfs-turbo-controller\n  template:\n    metadata:\n      labels:\n        app: csi-sfs-turbo-controller\n    spec:\n      serviceAccountName: csi-sfs-turbo-controller-sa\n      containers:\n      - name: csi-provisioner\n        image: quay.io/k8scsi/csi-provisioner:v1.4.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        - --enable-leader-election\n        - --leader-election-type=leases\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: csi-attacher\n        image: quay.io/k8scsi/csi-attacher:v3.1.0\n        args:\n        - -v=5\n        - --csi-address=$(ADDRESS)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      - name: sfs-turbo-csi-plugin\n        image: registry.eu-west-0.prod-cloud-ocb.orange-business.com/official/sfsturbo-csi-plugin:v1.8\n        args:\n        - --v=2\n        - --logtostderr\n        - --endpoint=$(CSI_ENDPOINT)\n        - --nodeid=$(NODE_ID)\n        - --cloud-config=$(CLOUD_CONFIG)\n        ports:\n        - containerPort: 28888\n          name: healthz\n          protocol: TCP\n        livenessProbe:\n          failureThreshold: 5\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 30\n          timeoutSeconds: 10\n          periodSeconds: 30\n        env:\n        - name: NODE_ID\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        - name: CSI_ENDPOINT\n          value: unix://csi/csi.sock\n        - name: CLOUD_CONFIG\n          value: /etc/sfs-turbo/cloud-config\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n        - mountPath: /etc/sfs-turbo/\n          name: sfs-turbo-config\n      - name: liveness-probe\n        imagePullPolicy: Always\n        image: quay.io/k8scsi/livenessprobe:v1.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        - --connection-timeout=3s\n        - --health-port=28888\n        - --v=5\n        volumeMounts:\n        - mountPath: /csi\n          name: socket-dir\n      volumes:\n      - name: socket-dir\n        emptyDir: {}\n      - name: sfs-turbo-config\n        hostPath:\n          path: /etc/sfs-turbo/\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sfs-turbo-csi-plugin\" has memory limit 0"
  },
  {
    "id": "04158",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"soak\" is using an invalid container image, \"docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04159",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"soak\" does not have a read-only root file system"
  },
  {
    "id": "04160",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"soak\" is not set to runAsNonRoot"
  },
  {
    "id": "04161",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"soak\" has cpu request 0"
  },
  {
    "id": "04162",
    "manifest_path": "data/manifests/the_stack_sample/sample_1887.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: cluster-soak-tests\nspec:\n  template:\n    spec:\n      containers:\n      - name: soak\n        image: docker-registry-default.centralpark2.lightbend.com/akka-long-running/cluster-soak-tests\n        imagePullPolicy: Always\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"soak\" has memory limit 0"
  },
  {
    "id": "04163",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"msg-path-demo-container\" is using an invalid container image, \"debian\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04164",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"msg-path-demo-container\" does not have a read-only root file system"
  },
  {
    "id": "04165",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"msg-path-demo-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04166",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"msg-path-demo-container\" has cpu request 0"
  },
  {
    "id": "04167",
    "manifest_path": "data/manifests/the_stack_sample/sample_1892.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: msg-path-demo\nspec:\n  containers:\n  - name: msg-path-demo-container\n    image: debian\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"msg-path-demo-container\" has memory limit 0"
  },
  {
    "id": "04168",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cirros-vm\" is using an invalid container image, \"virtlet.cloud/cirros\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04169",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cirros-vm\" does not have a read-only root file system"
  },
  {
    "id": "04170",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cirros-vm\" is not set to runAsNonRoot"
  },
  {
    "id": "04171",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cirros-vm\" has cpu request 0"
  },
  {
    "id": "04172",
    "manifest_path": "data/manifests/the_stack_sample/sample_1900.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cirros-vm\n  annotations:\n    kubernetes.io/target-runtime: virtlet.cloud\nspec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n          - key: extraRuntime\n            operator: In\n            values:\n            - virtlet\n  containers:\n  - name: cirros-vm\n    imagePullPolicy: IfNotPresent\n    image: virtlet.cloud/cirros\n  volumes:\n  - name: raw\n    flexVolume:\n      driver: virtlet/flexvolume_driver\n      options:\n        type: raw\n        path: /dev/loop0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cirros-vm\" has memory limit 0"
  },
  {
    "id": "04173",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"test-rc\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04174",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"test-rc\" does not have a read-only root file system"
  },
  {
    "id": "04175",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"test-rc\" is not set to runAsNonRoot"
  },
  {
    "id": "04176",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"test-rc\" has cpu request 0"
  },
  {
    "id": "04177",
    "manifest_path": "data/manifests/the_stack_sample/sample_1906.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: ReplicationController\nmetadata:\n  name: test-rc\n  labels:\n    name: test-rc\nspec:\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        name: test-rc\n    spec:\n      containers:\n      - name: test-rc\n        image: nginx\n        args:\n        - -random_flag=%s@domain.com\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"test-rc\" has memory limit 0"
  },
  {
    "id": "04178",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"yolo\" is using an invalid container image, \"github.com/josephburnett/kubecon-seattle-2018/yolo\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04179",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"yolo\" does not have a read-only root file system"
  },
  {
    "id": "04180",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"yolo\" is not set to runAsNonRoot"
  },
  {
    "id": "04181",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"yolo\" has cpu request 0"
  },
  {
    "id": "04182",
    "manifest_path": "data/manifests/the_stack_sample/sample_1911.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: yolo-controller\n  namespace: knative-serving\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: yolo-controller\n  template:\n    metadata:\n      labels:\n        app: yolo-controller\n    spec:\n      serviceAccountName: controller\n      containers:\n      - name: yolo\n        image: github.com/josephburnett/kubecon-seattle-2018/yolo\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"yolo\" has memory limit 0"
  },
  {
    "id": "04183",
    "manifest_path": "data/manifests/the_stack_sample/sample_1912.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-nrfin00009-pov1\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-nrfin00009-pov1\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers NRFIN_00009\n      pov_1 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cyborg-seeker-qualifiers-nrfin00009-pov1\" does not have a read-only root file system"
  },
  {
    "id": "04184",
    "manifest_path": "data/manifests/the_stack_sample/sample_1912.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: cyborg-seeker-qualifiers-nrfin00009-pov1\n  labels:\n    type: cyborg-seeker\nspec:\n  volumes:\n  - name: cyborg-results\n    persistentVolumeClaim:\n      claimName: cyborg-results\n  containers:\n  - name: cyborg-seeker-qualifiers-nrfin00009-pov1\n    image: zardus/research:cyborg-generator\n    command:\n    - /bin/bash\n    - -c\n    - python /home/angr/cyborg-generator/kubernetes_seeker.py qualifiers NRFIN_00009\n      pov_1 3600\n    imagePullPolicy: Always\n    volumeMounts:\n    - name: cyborg-results\n      mountPath: /results\n    resources:\n      limits:\n        cpu: 1\n        memory: 10Gi\n      requests:\n        cpu: 1\n        memory: 10Gi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cyborg-seeker-qualifiers-nrfin00009-pov1\" is not set to runAsNonRoot"
  },
  {
    "id": "04185",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04186",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04187",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04188",
    "manifest_path": "data/manifests/the_stack_sample/sample_1914.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx\nspec:\n  replicas: 4\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: rmt.example.com:5000/nginx:1.12.0\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04189",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"sleeping-container\" is using an invalid container image, \"sergiofgonzalez/sleeping-container:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04190",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"sleeping-container\" does not have a read-only root file system"
  },
  {
    "id": "04191",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"sleeping-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04192",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"sleeping-container\" has cpu request 0"
  },
  {
    "id": "04193",
    "manifest_path": "data/manifests/the_stack_sample/sample_1916.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep-1\nspec:\n  selector:\n    matchLabels:\n      app: sleep-1\n  template:\n    metadata:\n      labels:\n        app: sleep-1\n    spec:\n      containers:\n      - name: sleeping-container\n        image: sergiofgonzalez/sleeping-container:latest\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"sleeping-container\" has memory limit 0"
  },
  {
    "id": "04194",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"crier\" does not have a read-only root file system"
  },
  {
    "id": "04195",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"crier\" is not set to runAsNonRoot"
  },
  {
    "id": "04196",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"crier\" has cpu request 0"
  },
  {
    "id": "04197",
    "manifest_path": "data/manifests/the_stack_sample/sample_1918.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: crier\n  labels:\n    app: crier\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: crier\n  template:\n    metadata:\n      labels:\n        app: crier\n    spec:\n      serviceAccountName: crier\n      containers:\n      - name: crier\n        image: gcr.io/k8s-prow/crier:v20220217-362d6ac4a0\n        env:\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /etc/credentials/service-account.json\n        args:\n        - --blob-storage-workers=1\n        - --config-path=/etc/config/config.yaml\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --github-workers=5\n        - --job-config-path=/etc/job-config\n        - --kubeconfig=/etc/kubeconfig/config\n        - --kubernetes-blob-storage-workers=1\n        - --pubsub-workers=5\n        - --slack-token-file=/etc/slack/token\n        - --slack-workers=1\n        - --gcs-credentials-file=/etc/credentials/service-account.json\n        volumeMounts:\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: slack\n          mountPath: /etc/slack\n          readOnly: true\n        - name: gcs-service-account\n          mountPath: /etc/credentials\n          readOnly: true\n      volumes:\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: slack\n        secret:\n          secretName: slack-token\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: workload-clusters-kubeconfig\n      - name: gcs-service-account\n        secret:\n          secretName: sa-crier\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"crier\" has memory limit 0"
  },
  {
    "id": "04198",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04199",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04200",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04201",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04202",
    "manifest_path": "data/manifests/the_stack_sample/sample_1919.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-7229\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04203",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"csi-driver-registrar\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04204",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"iks-vpc-block-node-driver\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04205",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"liveness-probe\" is using an invalid container image, \"MUSTPATCHWITHKUSTOMIZE\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04206",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"csi-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "04207",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"iks-vpc-block-node-driver\" does not have a read-only root file system"
  },
  {
    "id": "04208",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "04209",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"iks-vpc-block-node-driver\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04210",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"iks-vpc-block-node-driver\" is privileged"
  },
  {
    "id": "04211",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"csi-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "04212",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"iks-vpc-block-node-driver\" is not set to runAsNonRoot"
  },
  {
    "id": "04213",
    "manifest_path": "data/manifests/the_stack_sample/sample_1922.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ibm-vpc-block-csi-node\nspec:\n  selector:\n    matchLabels:\n      app: ibm-vpc-block-csi-driver\n  template:\n    metadata:\n      annotations:\n        prometheus.io/scrape: 'true'\n        prometheus.io/port: '9080'\n        prometheus.io/path: /metrics\n      labels:\n        app: ibm-vpc-block-csi-driver\n    spec:\n      serviceAccountName: ibm-vpc-block-node-sa\n      containers:\n      - name: csi-driver-registrar\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --v=5\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REGISTRATION_SOCK)\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REGISTRATION_SOCK\n          value: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/csi.sock\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        resources:\n          limits:\n            cpu: 100m\n            memory: 100Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: iks-vpc-block-node-driver\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: true\n        image: MUSTPATCHWITHKUSTOMIZE\n        imagePullPolicy: Always\n        args:\n        - --v=5\n        - --endpoint=unix:/csi/csi.sock\n        env:\n        - name: KUBE_NODE_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: spec.nodeName\n        envFrom:\n        - configMapRef:\n            name: ibm-vpc-block-csi-configmap\n        resources:\n          limits:\n            cpu: 200m\n            memory: 250Mi\n          requests:\n            cpu: 20m\n            memory: 50Mi\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n        volumeMounts:\n        - name: kubelet-data-dir\n          mountPath: /var/data/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        - name: etcudevpath\n          mountPath: /etc/udev\n        - name: runudevpath\n          mountPath: /run/udev\n        - name: libudevpath\n          mountPath: /lib/udev\n        - name: syspath\n          mountPath: /sys\n        - name: customer-auth\n          readOnly: true\n          mountPath: /etc/storage_ibmc\n        - name: cluster-info\n          readOnly: true\n          mountPath: /etc/storage_ibmc/cluster_info\n      - name: liveness-probe\n        image: MUSTPATCHWITHKUSTOMIZE\n        securityContext:\n          runAsNonRoot: false\n          runAsUser: 0\n          privileged: false\n        args:\n        - --csi-address=/csi/csi.sock\n        resources:\n          limits:\n            cpu: 50m\n            memory: 50Mi\n          requests:\n            cpu: 5m\n            memory: 10Mi\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: kubelet-data-dir\n        hostPath:\n          path: /var/data/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/csi-plugins/vpc.block.csi.ibm.io/\n          type: DirectoryOrCreate\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n      - name: etcudevpath\n        hostPath:\n          path: /etc/udev\n          type: Directory\n      - name: runudevpath\n        hostPath:\n          path: /run/udev\n          type: Directory\n      - name: libudevpath\n        hostPath:\n          path: /lib/udev\n          type: Directory\n      - name: syspath\n        hostPath:\n          path: /sys\n          type: Directory\n      - name: customer-auth\n        secret:\n          secretName: storage-secret-store\n      - name: cluster-info\n        configMap:\n          name: cluster-info\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "04214",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"bitnami/nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04215",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04216",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04217",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04218",
    "manifest_path": "data/manifests/the_stack_sample/sample_1929.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx7\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: bitnami/nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04219",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04220",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04221",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04222",
    "manifest_path": "data/manifests/the_stack_sample/sample_1930.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: web\nspec:\n  selector:\n    matchLabels:\n      app: nginx\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n      - name: nginx\n        image: k8s.gcr.io/nginx-slim:0.8\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: www\n          mountPath: /usr/share/nginx/html\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04223",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable STANDARD_AUTHSERVICE_CERT_SECRET_NAME in container \"address-space-controller\" found"
  },
  {
    "id": "04224",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME in container \"address-space-controller\" found"
  },
  {
    "id": "04225",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"address-space-controller\" does not have a read-only root file system"
  },
  {
    "id": "04226",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"address-space-controller\" is not set to runAsNonRoot"
  },
  {
    "id": "04227",
    "manifest_path": "data/manifests/the_stack_sample/sample_1932.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: enmasse\n    name: address-space-controller\n  name: address-space-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: enmasse\n      name: address-space-controller\n  template:\n    metadata:\n      labels:\n        app: enmasse\n        name: address-space-controller\n    spec:\n      affinity:\n        nodeAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 1\n            preference:\n              matchExpressions:\n              - key: node-role.enmasse.io/operator-infra\n                operator: In\n                values:\n                - 'true'\n      containers:\n      - env:\n        - name: JAVA_OPTS\n          value: -verbose:gc\n        - name: ENABLE_EVENT_LOGGER\n          value: 'true'\n        - name: EXPOSE_ENDPOINTS_BY_DEFAULT\n          valueFrom:\n            configMapKeyRef:\n              key: exposeEndpointsByDefault\n              name: address-space-controller-config\n              optional: true\n        - name: ENVIRONMENT\n          valueFrom:\n            configMapKeyRef:\n              key: environment\n              name: address-space-controller-config\n              optional: true\n        - name: TEMPLATE_DIR\n          value: /opt/templates\n        - name: RESOURCES_DIR\n          value: /opt\n        - name: STANDARD_AUTHSERVICE_CONFIG_NAME\n          value: keycloak-config\n        - name: STANDARD_AUTHSERVICE_CREDENTIALS_SECRET_NAME\n          value: keycloak-credentials\n        - name: STANDARD_AUTHSERVICE_CERT_SECRET_NAME\n          value: standard-authservice-cert\n        - name: WILDCARD_ENDPOINT_CERT_SECRET\n          valueFrom:\n            configMapKeyRef:\n              key: wildcardEndpointCertSecret\n              name: address-space-controller-config\n              optional: true\n        - name: RESYNC_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: resyncInterval\n              name: address-space-controller-config\n              optional: true\n        - name: RECHECK_INTERVAL\n          valueFrom:\n            configMapKeyRef:\n              key: recheckInterval\n              name: address-space-controller-config\n              optional: true\n        - name: IMAGE_PULL_POLICY\n          value: Always\n        - name: ROUTER_IMAGE\n          value: registry.redhat.io/amq7/amq-interconnect:1.4\n        - name: STANDARD_CONTROLLER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-standard-controller:dev\n        - name: AGENT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-agent:dev\n        - name: BROKER_IMAGE\n          value: registry.redhat.io/amq-broker-7/amq-broker-73-openshift:latest\n        - name: BROKER_PLUGIN_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-broker-plugin:dev\n        - name: TOPIC_FORWARDER_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-topic-forwarder:dev\n        - name: MQTT_GATEWAY_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-gateway:dev\n        - name: MQTT_LWT_IMAGE\n          value: registry.redhat.io/amq7/amq-online-1-mqtt-lwt:dev\n        image: registry.redhat.io/amq7/amq-online-1-address-space-controller:dev\n        imagePullPolicy: Always\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        name: address-space-controller\n        ports:\n        - containerPort: 8080\n          name: http\n        readinessProbe:\n          httpGet:\n            path: /healthz\n            port: http\n            scheme: HTTP\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            memory: 256Mi\n      serviceAccountName: address-space-controller\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"address-space-controller\" has cpu request 0"
  },
  {
    "id": "04228",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"dc-api\" is using an invalid container image, \"gcr.io/neural-pattern-278618/dc-api\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04229",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"dc-api\" does not have a read-only root file system"
  },
  {
    "id": "04230",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"dc-api\" is not set to runAsNonRoot"
  },
  {
    "id": "04231",
    "manifest_path": "data/manifests/the_stack_sample/sample_1933.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: dc-api\n  labels:\n    app: dc-api\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: dc-api\n  template:\n    metadata:\n      labels:\n        app: dc-api\n    spec:\n      containers:\n      - name: dc-api\n        image: gcr.io/neural-pattern-278618/dc-api\n        imagePullPolicy: Always\n        ports:\n        - containerPort: 8080\n        envFrom:\n        - configMapRef:\n            name: dc-api\n        - secretRef:\n            name: dc-api\n        - secretRef:\n            name: postgres\n        resources:\n          limits:\n            cpu: '0.3'\n          requests:\n            cpu: '0.3'\n        readinessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          initialDelaySeconds: 10\n          periodSeconds: 3\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 8080\n          periodSeconds: 5\n          initialDelaySeconds: 200\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"dc-api\" has memory limit 0"
  },
  {
    "id": "04232",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"simpleapp\" is using an invalid container image, \"dgkanatsios/simpleapp\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04233",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"simpleapp\" does not have a read-only root file system"
  },
  {
    "id": "04234",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"simpleapp\" is not set to runAsNonRoot"
  },
  {
    "id": "04235",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"simpleapp\" has cpu request 0"
  },
  {
    "id": "04236",
    "manifest_path": "data/manifests/the_stack_sample/sample_1934.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app: foo\n  name: foo\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: foo\n  template:\n    metadata:\n      labels:\n        app: foo\n    spec:\n      containers:\n      - image: dgkanatsios/simpleapp\n        name: simpleapp\n        ports:\n        - containerPort: 8080\n        resources: {}\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"simpleapp\" has memory limit 0"
  },
  {
    "id": "04237",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"init-sysctl\" is using an invalid container image, \"busybox\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04238",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"banias-frontend\" does not have a read-only root file system"
  },
  {
    "id": "04239",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"init-sysctl\" does not have a read-only root file system"
  },
  {
    "id": "04240",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"init-sysctl\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04241",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"init-sysctl\" is privileged"
  },
  {
    "id": "04242",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"banias-frontend\" is not set to runAsNonRoot"
  },
  {
    "id": "04243",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"init-sysctl\" is not set to runAsNonRoot"
  },
  {
    "id": "04244",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"init-sysctl\" has cpu request 0"
  },
  {
    "id": "04245",
    "manifest_path": "data/manifests/the_stack_sample/sample_1938.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: banias-frontend\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: banias-frontend\n  template:\n    metadata:\n      labels:\n        app: banias-frontend\n        k8s-app: banias-frontend\n    spec:\n      initContainers:\n      - name: init-sysctl\n        image: busybox\n        command:\n        - /bin/sh\n        - -c\n        - 'sysctl -w net.ipv4.ip_forward=0\n\n          sysctl -w net.ipv4.conf.default.rp_filter=1\n\n          sysctl -w net.ipv4.conf.default.accept_source_route=0\n\n          sysctl -w net.ipv4.icmp_echo_ignore_broadcasts=1\n\n          sysctl -w net.ipv4.icmp_ignore_bogus_error_responses=1\n\n          sysctl -w kernel.msgmnb=65536\n\n          sysctl -w kernel.msgmax=65536\n\n          sysctl -w kernel.shmmax=68719476736\n\n          sysctl -w kernel.shmall=4294967296\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w fs.file-max=100000\n\n          sysctl -w net.ipv4.tcp_syncookies=1\n\n          sysctl -w net.ipv4.conf.all.log_martians=0\n\n          sysctl -w net.core.somaxconn=50000\n\n          sysctl -w net.ipv4.tcp_max_syn_backlog=30000\n\n          sysctl -w net.ipv4.conf.all.send_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_redirects=0\n\n          sysctl -w net.ipv4.conf.all.accept_source_route=0\n\n          sysctl -w net.ipv6.conf.all.forwarding=0\n\n          sysctl -w net.ipv4.tcp_slow_start_after_idle=0\n\n          sysctl -w net.ipv4.tcp_window_scaling=1\n\n          sysctl -w net.ipv4.tcp_timestamp=1\n\n          sysctl -w net.ipv4.tcp_sack=1\n\n          sysctl -w net.ipv4.tcp_congestion_control=htcp\n\n          sysctl -w net.ipv4.tcp_keepalive_time=60\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=10\n\n          sysctl -w net.netfilter.nf_conntrack_tcp_timeout_established=300\n\n          sysctl -w net.netfilter.nf_conntrack_generic_timeout=300\n\n          sysctl -w net.ipv4.tcp_max_tw_buckets=2000000\n\n          sysctl -w net.ipv4.tcp_fin_timeout=10\n\n          sysctl -w net.ipv4.tcp_tw_reuse=1\n\n          sysctl -w net.ipv4.tcp_keepalive_intvl=15\n\n          sysctl -w net.ipv4.tcp_keepalive_probes=5\n\n          '\n        imagePullPolicy: IfNotPresent\n        securityContext:\n          privileged: true\n      containers:\n      - name: banias-frontend\n        image: gcr.io/my-project/banias-frontend:test\n        resources:\n          limits:\n            cpu: '3'\n            memory: 12G\n          requests:\n            cpu: 500m\n            memory: 200Mi\n        volumeMounts:\n        - name: google-cloud-key\n          mountPath: /var/secrets/google\n        ports:\n        - name: http\n          containerPort: 8081\n        - name: prom-metrics\n          containerPort: 8080\n        env:\n        - name: BANIAS_PROJECTID\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PROJECTID\n              name: banias-frontend-config\n        - name: BANIAS_DEBUG\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_DEBUG\n              name: banias-frontend-config\n        - name: BANIAS_TOPIC\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_TOPIC\n              name: banias-frontend-config\n        - name: BANIAS_PORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PORT\n              name: banias-frontend-config\n        - name: BANIAS_METRICSPORT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_METRICSPORT\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXBATCH\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXBATCH\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBAGGRIGATORS\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBAGGRIGATORS\n              name: banias-frontend-config\n        - name: BANIAS_PUBSUBMAXPUBLISHDELAY\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_PUBSUBMAXPUBLISHDELAY\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINESAMOUNT\n              name: banias-frontend-config\n        - name: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n          valueFrom:\n            configMapKeyRef:\n              key: BANIAS_MAXPUBSUBGOROUTINEIDLEDURATION\n              name: banias-frontend-config\n        - name: GOOGLE_APPLICATION_CREDENTIALS\n          value: /var/secrets/google/key.json\n        imagePullPolicy: Always\n      volumes:\n      - name: google-cloud-key\n        secret:\n          secretName: pubsub-key\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"init-sysctl\" has memory limit 0"
  },
  {
    "id": "04246",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"nginx\" is using an invalid container image, \"nginx\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04247",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"nginx\" does not have a read-only root file system"
  },
  {
    "id": "04248",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"nginx\" is not set to runAsNonRoot"
  },
  {
    "id": "04249",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"nginx\" has cpu request 0"
  },
  {
    "id": "04250",
    "manifest_path": "data/manifests/the_stack_sample/sample_1940.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: image-demo-6857\nspec:\n  containers:\n  - name: nginx\n    image: nginx\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"nginx\" has memory limit 0"
  },
  {
    "id": "04251",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"deck\" does not have a read-only root file system"
  },
  {
    "id": "04252",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"deck\" is not set to runAsNonRoot"
  },
  {
    "id": "04253",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"deck\" has cpu request 0"
  },
  {
    "id": "04254",
    "manifest_path": "data/manifests/the_stack_sample/sample_1943.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: deck\n  labels:\n    app: deck\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: deck\n  template:\n    metadata:\n      labels:\n        app: deck\n    spec:\n      serviceAccountName: deck\n      containers:\n      - name: deck\n        image: gcr.io/k8s-prow/deck:v20210305-350f3b2f2e\n        imagePullPolicy: Always\n        ports:\n        - name: http\n          containerPort: 8080\n        args:\n        - --kubeconfig=/etc/kubeconfig/config\n        - --tide-url=http://tide/\n        - --hook-url=http://hook:8888/plugin-help\n        - --redirect-http-to=prow.k8s.io\n        - --oauth-url=/github-login\n        - --config-path=/etc/config/config.yaml\n        - --job-config-path=/etc/job-config\n        - --spyglass=true\n        - --rerun-creates-job\n        - --github-token-path=/etc/github/oauth\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-oauth-config-file=/etc/githuboauth/secret\n        - --cookie-secret=/etc/cookie/secret\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth-config\n          mountPath: /etc/githuboauth\n          readOnly: true\n        - name: cookie-secret\n          mountPath: /etc/cookie\n          readOnly: true\n        - mountPath: /etc/kubeconfig\n          name: kubeconfig\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: oauth-token\n          mountPath: /etc/github\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8081\n          initialDelaySeconds: 3\n          periodSeconds: 3\n        readinessProbe:\n          httpGet:\n            path: /healthz/ready\n            port: 8081\n          initialDelaySeconds: 10\n          periodSeconds: 3\n          timeoutSeconds: 600\n      volumes:\n      - name: oauth-config\n        secret:\n          secretName: github-oauth-config\n      - name: oauth-token\n        secret:\n          secretName: oauth-token\n      - name: cookie-secret\n        secret:\n          secretName: cookie\n      - name: kubeconfig\n        secret:\n          defaultMode: 420\n          secretName: kubeconfig\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"deck\" has memory limit 0"
  },
  {
    "id": "04255",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-main\" does not have a read-only root file system"
  },
  {
    "id": "04256",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-rbac-proxy-self\" does not have a read-only root file system"
  },
  {
    "id": "04257",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-state-metrics\" does not have a read-only root file system"
  },
  {
    "id": "04258",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-main\" is not set to runAsNonRoot"
  },
  {
    "id": "04259",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-rbac-proxy-self\" is not set to runAsNonRoot"
  },
  {
    "id": "04260",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-state-metrics\" is not set to runAsNonRoot"
  },
  {
    "id": "04261",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-main\" has memory limit 0"
  },
  {
    "id": "04262",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-rbac-proxy-self\" has memory limit 0"
  },
  {
    "id": "04263",
    "manifest_path": "data/manifests/the_stack_sample/sample_1944.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-state-metrics\n    app.kubernetes.io/version: v1.9.5\n  name: kube-state-metrics\n  namespace: openshift-monitoring\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: kube-state-metrics\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: kube-state-metrics\n        app.kubernetes.io/version: v1.9.5\n    spec:\n      containers:\n      - args:\n        - --host=127.0.0.1\n        - --port=8081\n        - --telemetry-host=127.0.0.1\n        - --telemetry-port=8082\n        - --metric-blacklist=kube_secret_labels\n        image: quay.io/coreos/kube-state-metrics:v1.9.5\n        name: kube-state-metrics\n        resources:\n          requests:\n            cpu: 2m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /tmp\n          name: volume-directive-shadow\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:8443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8081/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-main\n        ports:\n        - containerPort: 8443\n          name: https-main\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      - args:\n        - --logtostderr\n        - --secure-listen-address=:9443\n        - --tls-cipher-suites=TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_RSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256\n        - --upstream=http://127.0.0.1:8082/\n        - --tls-cert-file=/etc/tls/private/tls.crt\n        - --tls-private-key-file=/etc/tls/private/tls.key\n        image: quay.io/coreos/kube-rbac-proxy:v0.4.1\n        name: kube-rbac-proxy-self\n        ports:\n        - containerPort: 9443\n          name: https-self\n        resources:\n          requests:\n            cpu: 1m\n            memory: 40Mi\n        securityContext: {}\n        volumeMounts:\n        - mountPath: /etc/tls/private\n          name: kube-state-metrics-tls\n          readOnly: false\n      securityContext: {}\n      serviceAccountName: kube-state-metrics\n      volumes:\n      - emptyDir: {}\n        name: volume-directive-shadow\n      - name: kube-state-metrics-tls\n        secret:\n          secretName: kube-state-metrics-tls\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"kube-state-metrics\" has memory limit 0"
  },
  {
    "id": "04264",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "04265",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "04266",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "04267",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "04268",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "04269",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "04270",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "04271",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "04272",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "04273",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "04274",
    "manifest_path": "data/manifests/the_stack_sample/sample_1947.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.4.0-classifier-efficientnet-func-v2-8\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.4.0\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/vision/image_classification/classifier_trainer.py\n          - --data_dir=$(IMAGENET_DIR)\n          - --model_type=efficientnet\n          - --dataset=imagenet\n          - --mode=train_and_eval\n          - --model_dir=$(MODEL_DIR)\n          - \"--params_override=\\\"evaluation\\\":\\n  \\\"epochs_between_evals\\\": 1\\n\\\"\\\n            model\\\":\\n  \\\"model_params\\\":\\n    \\\"model_name\\\": \\\"efficientnet-b0\\\"\\\n            \\n\\\"train\\\":\\n  \\\"epochs\\\": 1\\n\\\"train_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\\n            \\n\\\"validation_dataset\\\":\\n  \\\"builder\\\": \\\"records\\\"\\n\"\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --config_file=official/vision/image_classification/configs/examples/efficientnet/imagenet/efficientnet-b0-tpu.yaml\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.4.0\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 8\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.4.0/classifier-efficientnet/func/v2-8/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.4.0-classifier-efficientnet-func-v2-8\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "04275",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"results-app\" is using an invalid container image, \"dockersamples/examplevotingapp_result\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04276",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"results-app\" does not have a read-only root file system"
  },
  {
    "id": "04277",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"results-app\" is not set to runAsNonRoot"
  },
  {
    "id": "04278",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"results-app\" has cpu request 0"
  },
  {
    "id": "04279",
    "manifest_path": "data/manifests/the_stack_sample/sample_1954.yaml",
    "manifest_yaml": "apiVersion: v1\nkind: Pod\nmetadata:\n  name: results-app\n  labels:\n    app: my-voting-app\n    tier: results\nspec:\n  containers:\n  - image: dockersamples/examplevotingapp_result\n    name: results-app\n    ports:\n    - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"results-app\" has memory limit 0"
  },
  {
    "id": "04280",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"label-sync\" does not have a read-only root file system"
  },
  {
    "id": "04281",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"label-sync\" is not set to runAsNonRoot"
  },
  {
    "id": "04282",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"label-sync\" has cpu request 0"
  },
  {
    "id": "04283",
    "manifest_path": "data/manifests/the_stack_sample/sample_1962.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: label-sync\nspec:\n  jobTemplate:\n    template:\n      spec:\n        containers:\n        - name: label-sync\n          image: gcr.io/k8s-prow/label_sync:v20200608-16190316cf\n          args:\n          - --config=/etc/config/labels.yaml\n          - --confirm=true\n          - --orgs=kubernetes,kubernetes-client,kubernetes-csi,kubernetes-incubator,kubernetes-sigs\n          - --token=/etc/github/oauth\n          volumeMounts:\n          - name: oauth\n            mountPath: /etc/github\n            readOnly: true\n          - name: config\n            mountPath: /etc/config\n            readOnly: true\n        volumes:\n        - name: oauth\n          secret:\n            secretName: oauth-token\n        - name: config\n          configMap:\n            name: label-config\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"label-sync\" has memory limit 0"
  },
  {
    "id": "04284",
    "manifest_path": "data/manifests/the_stack_sample/sample_1963.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: kube-janitor\n  labels:\n    app: kube-janitor\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: kube-janitor\n        containers:\n        - name: kube-janitor\n          image: themagicalkarp/kube-janitor:v0.1.0\n          imagePullPolicy: Always\n          command:\n          - /kube-janitor\n          - -expiration=2880\n          - -annotation=kube.janitor.io\n          - -pendingJobExpiration=60\n          - -verbose\n          resources:\n            limits:\n              cpu: 200m\n              memory: 100Mi\n            requests:\n              cpu: 50m\n              memory: 50Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"kube-janitor\" does not have a read-only root file system"
  },
  {
    "id": "04285",
    "manifest_path": "data/manifests/the_stack_sample/sample_1963.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: kube-janitor\n  labels:\n    app: kube-janitor\nspec:\n  jobTemplate:\n    template:\n      spec:\n        serviceAccountName: kube-janitor\n        containers:\n        - name: kube-janitor\n          image: themagicalkarp/kube-janitor:v0.1.0\n          imagePullPolicy: Always\n          command:\n          - /kube-janitor\n          - -expiration=2880\n          - -annotation=kube.janitor.io\n          - -pendingJobExpiration=60\n          - -verbose\n          resources:\n            limits:\n              cpu: 200m\n              memory: 100Mi\n            requests:\n              cpu: 50m\n              memory: 50Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"kube-janitor\" is not set to runAsNonRoot"
  },
  {
    "id": "04286",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"signalfx-agent\" does not have a read-only root file system"
  },
  {
    "id": "04287",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"signalfx-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "04288",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"signalfx-agent\" has cpu request 0"
  },
  {
    "id": "04289",
    "manifest_path": "data/manifests/the_stack_sample/sample_1965.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: signalfx-agent\n  labels:\n    app: signalfx-agent\n    version: 5.1.4\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: signalfx-agent\n  template:\n    metadata:\n      labels:\n        app: signalfx-agent\n        version: 5.1.4\n      annotations: {}\n    spec:\n      serviceAccountName: signalfx-agent\n      containers:\n      - name: signalfx-agent\n        image: quay.io/signalfx/signalfx-agent:5.1.4\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/signalfx-agent\n        volumeMounts:\n        - mountPath: /etc/signalfx\n          name: config\n        resources: {}\n        env:\n        - name: SFX_ACCESS_TOKEN\n          valueFrom:\n            secretKeyRef:\n              name: signalfx-agent\n              key: access-token\n        - name: MY_POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: MY_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: MY_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n      volumes:\n      - name: config\n        configMap:\n          name: signalfx-agent-v5\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"signalfx-agent\" has memory limit 0"
  },
  {
    "id": "04290",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"vppagent-client\" is using an invalid container image, \"networkservicemesh/vpp-icmp-vppagent-client:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04291",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"vppagent-client\" does not have a read-only root file system"
  },
  {
    "id": "04292",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"vppagent-client\" is not set to runAsNonRoot"
  },
  {
    "id": "04293",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"vppagent-client\" has cpu request 0"
  },
  {
    "id": "04294",
    "manifest_path": "data/manifests/the_stack_sample/sample_1966.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: vppagent-client\nspec:\n  selector:\n    matchLabels:\n      networkservicemesh.io/app: vppagent-client\n  replicas: 4\n  template:\n    metadata:\n      labels:\n        networkservicemesh.io/app: vppagent-client\n    spec:\n      containers:\n      - name: vppagent-client\n        image: networkservicemesh/vpp-icmp-vppagent-client:latest\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: OUTGOING_NSC_NAME\n          value: icmp-responder\n        - name: OUTGOING_NSC_LABELS\n          value: app=vppagent-endpoint\n        resources:\n          limits:\n            networkservicemesh.io/socket: 1\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"vppagent-client\" has memory limit 0"
  },
  {
    "id": "04295",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "env-var-secret",
    "violation_text": "environment variable TLS_SECRET_NAME in container \"proxy\" found"
  },
  {
    "id": "04296",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"proxy\" does not have a read-only root file system"
  },
  {
    "id": "04297",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"proxy\" is not set to runAsNonRoot"
  },
  {
    "id": "04298",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"proxy\" has cpu request 0"
  },
  {
    "id": "04299",
    "manifest_path": "data/manifests/the_stack_sample/sample_1968.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kurl-proxy-kotsadm\n  labels:\n    app: kurl-proxy-kotsadm\n    kots.io/kotsadm: \\\"true\\\"\n    velero.io/exclude-from-backup: \\\"true\\\"\nspec:\n  selector:\n    matchLabels:\n      app: kurl-proxy-kotsadm\n  template:\n    metadata:\n      labels:\n        app: kurl-proxy-kotsadm\n        kots.io/kotsadm: \\\"true\\\"\n        velero.io/exclude-from-backup: \\\"true\\\"\n    spec:\n      containers:\n      - name: proxy\n        image: kotsadm/kurl-proxy:alpha\n        imagePullPolicy: IfNotPresent\n        env:\n        - name: NODE_PORT\n          value: \\\"8800\\\"\n        - name: UPSTREAM_ORIGIN\n          value: http://kotsadm:3000\n        - name: TLS_SECRET_NAME\n          value: kotsadm-tls\n        - name: NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        volumeMounts:\n        - name: kotsadm-config\n          mountPath: /etc/kotsadm\n      serviceAccount: kurl-proxy\n      volumes:\n      - name: kotsadm-config\n        configMap:\n          name: kotsadm-application-metadata\n          optional: true\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"proxy\" has memory limit 0"
  },
  {
    "id": "04300",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"cilium-agent\" is using an invalid container image, \"docker.io/cilium/cilium:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04301",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"cilium-agent\" does not have a read-only root file system"
  },
  {
    "id": "04302",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"clean-cilium-state\" does not have a read-only root file system"
  },
  {
    "id": "04303",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"cilium-agent\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04304",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"clean-cilium-state\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04305",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"cilium-agent\" is privileged"
  },
  {
    "id": "04306",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"clean-cilium-state\" is privileged"
  },
  {
    "id": "04307",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"cilium-agent\" is not set to runAsNonRoot"
  },
  {
    "id": "04308",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"clean-cilium-state\" is not set to runAsNonRoot"
  },
  {
    "id": "04309",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"cilium-agent\" has cpu request 0"
  },
  {
    "id": "04310",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"clean-cilium-state\" has cpu request 0"
  },
  {
    "id": "04311",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"cilium-agent\" has memory limit 0"
  },
  {
    "id": "04312",
    "manifest_path": "data/manifests/the_stack_sample/sample_1975.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  labels:\n    k8s-app: cilium\n    kubernetes.io/cluster-service: 'true'\n  name: cilium\n  namespace: kube-system\nspec:\n  selector:\n    matchLabels:\n      k8s-app: cilium\n      kubernetes.io/cluster-service: 'true'\n  template:\n    metadata:\n      annotations:\n        prometheus.io/port: '9090'\n        prometheus.io/scrape: 'true'\n        scheduler.alpha.kubernetes.io/critical-pod: ''\n        scheduler.alpha.kubernetes.io/tolerations: '[{\"key\":\"dedicated\",\"operator\":\"Equal\",\"value\":\"master\",\"effect\":\"NoSchedule\"}]'\n      labels:\n        k8s-app: cilium\n        kubernetes.io/cluster-service: 'true'\n    spec:\n      containers:\n      - args:\n        - --kvstore=etcd\n        - --kvstore-opt=etcd.config=/var/lib/etcd-config/etcd.config\n        - --config-dir=/tmp/cilium/config-map\n        command:\n        - cilium-agent\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: CILIUM_K8S_NAMESPACE\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: metadata.namespace\n        - name: CILIUM_FLANNEL_MASTER_DEVICE\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-master-device\n              name: cilium-config\n              optional: true\n        - name: CILIUM_FLANNEL_UNINSTALL_ON_EXIT\n          valueFrom:\n            configMapKeyRef:\n              key: flannel-uninstall-on-exit\n              name: cilium-config\n              optional: true\n        - name: CILIUM_PROMETHEUS_SERVE_ADDR\n          valueFrom:\n            configMapKeyRef:\n              key: prometheus-serve-addr\n              name: cilium-metrics-config\n              optional: true\n        - name: CILIUM_CLUSTERMESH_CONFIG\n          value: /var/lib/cilium/clustermesh/\n        image: docker.io/cilium/cilium:latest\n        imagePullPolicy: Always\n        livenessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 10\n          initialDelaySeconds: 120\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        name: cilium-agent\n        ports:\n        - containerPort: 9090\n          hostPort: 9090\n          name: prometheus\n          protocol: TCP\n        readinessProbe:\n          exec:\n            command:\n            - cilium\n            - status\n            - --brief\n          failureThreshold: 3\n          initialDelaySeconds: 5\n          periodSeconds: 30\n          successThreshold: 1\n          timeoutSeconds: 5\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n            - SYS_MODULE\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n        - mountPath: /host/opt/cni/bin\n          name: cni-path\n        - mountPath: /host/etc/cni/net.d\n          name: etc-cni-netd\n        - mountPath: /var/run/docker.sock\n          name: docker-socket\n          readOnly: true\n        - mountPath: /var/lib/etcd-config\n          name: etcd-config-path\n          readOnly: true\n        - mountPath: /var/lib/etcd-secrets\n          name: etcd-secrets\n          readOnly: true\n        - mountPath: /var/lib/cilium/clustermesh\n          name: clustermesh-secrets\n          readOnly: true\n        - mountPath: /tmp/cilium/config-map\n          name: cilium-config-path\n          readOnly: true\n        - mountPath: /lib/modules\n          name: lib-modules\n          readOnly: true\n        - mountPath: /sbin/modprobe\n          name: sbin-modprobe\n          readOnly: true\n      initContainers:\n      - command:\n        - /init-container.sh\n        env:\n        - name: CILIUM_ALL_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_BPF_STATE\n          valueFrom:\n            configMapKeyRef:\n              key: clean-cilium-bpf-state\n              name: cilium-config\n              optional: true\n        - name: CILIUM_WAIT_BPF_MOUNT\n          valueFrom:\n            configMapKeyRef:\n              key: wait-bpf-mount\n              name: cilium-config\n              optional: true\n        image: docker.io/cilium/cilium-init:2019-04-05\n        imagePullPolicy: IfNotPresent\n        name: clean-cilium-state\n        securityContext:\n          capabilities:\n            add:\n            - NET_ADMIN\n          privileged: true\n        volumeMounts:\n        - mountPath: /sys/fs/bpf\n          name: bpf-maps\n        - mountPath: /var/run/cilium\n          name: cilium-run\n      serviceAccount: cilium\n      serviceAccountName: cilium\n      volumes:\n      - hostPath:\n          path: /var/run/cilium\n          type: DirectoryOrCreate\n        name: cilium-run\n      - hostPath:\n          path: /sys/fs/bpf\n          type: DirectoryOrCreate\n        name: bpf-maps\n      - hostPath:\n          path: /var/run/docker.sock\n          type: Socket\n        name: docker-socket\n      - hostPath:\n          path: /opt/cni/bin\n          type: DirectoryOrCreate\n        name: cni-path\n      - hostPath:\n          path: /etc/cni/net.d\n          type: DirectoryOrCreate\n        name: etc-cni-netd\n      - hostPath:\n          path: /lib/modules\n          type: Directory\n        name: lib-modules\n      - hostPath:\n          path: /sbin/modprobe\n          type: File\n        name: sbin-modprobe\n      - configMap:\n          defaultMode: 420\n          items:\n          - key: etcd-config\n            path: etcd.config\n          name: cilium-config\n        name: etcd-config-path\n      - name: etcd-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-etcd-secrets\n      - name: clustermesh-secrets\n        secret:\n          defaultMode: 420\n          optional: true\n          secretName: cilium-clustermesh\n      - configMap:\n          name: cilium-config\n        name: cilium-config-path\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"clean-cilium-state\" has memory limit 0"
  },
  {
    "id": "04313",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"statusreconciler\" does not have a read-only root file system"
  },
  {
    "id": "04314",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"statusreconciler\" is not set to runAsNonRoot"
  },
  {
    "id": "04315",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"statusreconciler\" has cpu request 0"
  },
  {
    "id": "04316",
    "manifest_path": "data/manifests/the_stack_sample/sample_1978.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  namespace: default\n  name: statusreconciler\n  labels:\n    app.kubernetes.io/part-of: prow\n    app: statusreconciler\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: statusreconciler\n  template:\n    metadata:\n      labels:\n        app: statusreconciler\n    spec:\n      containers:\n      - name: statusreconciler\n        image: gcr.io/k8s-prow/status-reconciler:v20200910-8c70361b39\n        imagePullPolicy: Always\n        args:\n        - --config-path=/etc/config/config.yaml\n        - --continue-on-error=true\n        - --dry-run=false\n        - --github-endpoint=http://ghproxy\n        - --github-endpoint=https://api.github.com\n        - --github-token-path=/etc/github/oauth\n        - --job-config-path=/etc/job-config\n        - --plugin-config=/etc/plugins/plugins.yaml\n        volumeMounts:\n        - name: oauth\n          mountPath: /etc/github\n          readOnly: true\n        - name: config\n          mountPath: /etc/config\n          readOnly: true\n        - name: job-config\n          mountPath: /etc/job-config\n          readOnly: true\n        - name: plugins\n          mountPath: /etc/plugins\n          readOnly: true\n      volumes:\n      - name: oauth\n        secret:\n          secretName: oauth-token\n      - name: config\n        configMap:\n          name: config\n      - name: job-config\n        configMap:\n          name: job-config\n      - name: plugins\n        configMap:\n          name: plugins\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"statusreconciler\" has memory limit 0"
  },
  {
    "id": "04317",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ansibletest\" does not have a read-only root file system"
  },
  {
    "id": "04318",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ansibletest\" is not set to runAsNonRoot"
  },
  {
    "id": "04319",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ansibletest\" has cpu request 0"
  },
  {
    "id": "04320",
    "manifest_path": "data/manifests/the_stack_sample/sample_1982.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  namespace: litmus\nspec:\n  template:\n    metadata:\n      name: litmus\n      labels:\n        app: wordpress-litmus\n    spec:\n      serviceAccountName: litmus\n      containers:\n      - name: ansibletest\n        image: openebs/ansible-runner:ci\n        imagePullPolicy: Always\n        env:\n        - name: ANSIBLE_STDOUT_CALLBACK\n          value: default\n        - name: PROVIDER_STORAGE_CLASS\n          value: openebs-cstor-disk\n        - name: STORAGE_CLASS\n          value: openebs-nfs\n        - name: MYSQL_APP_PVC\n          value: openebs-mysql\n        - name: MYSQL_PASS\n          value: w0rdpres5\n        - name: WORDPRESS_APP_PVC\n          value: openebs-wordpress\n        - name: APP_LABEL\n          value: app=wordpress\n        - name: AFFINITY_LABEL\n          value: openebs.io/target-affinity\n        - name: APP_NAMESPACE\n          value: app-wordpress-ns\n        - name: APP_REPLICA\n          value: replicas=1\n        - name: ACTION\n          value: provision\n        - name: MYSQL_PV_CAPACITY\n          value: 5G\n        - name: WORDPRESS_PV_CAPACITY\n          value: 5G\n        - name: PVC_ACCESS_MODE\n          value: ReadWriteMany\n        command:\n        - /bin/bash\n        args:\n        - -c\n        - ansible-playbook ./apps/wordpress/deployers/test.yml -i /etc/ansible/hosts\n          -v; exit 0\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ansibletest\" has memory limit 0"
  },
  {
    "id": "04321",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"helloworld\" does not have a read-only root file system"
  },
  {
    "id": "04322",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"helloworld\" is not set to runAsNonRoot"
  },
  {
    "id": "04323",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"helloworld\" has cpu request 0"
  },
  {
    "id": "04324",
    "manifest_path": "data/manifests/the_stack_sample/sample_1983.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: canary\n  labels:\n    app: canary\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: canary\n      pipecd.dev/variant: primary\n  template:\n    metadata:\n      labels:\n        app: canary\n        pipecd.dev/variant: primary\n    spec:\n      containers:\n      - name: helloworld\n        image: gcr.io/pipecd/helloworld:v0.6.0\n        args:\n        - server\n        ports:\n        - containerPort: 9085\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"helloworld\" has memory limit 0"
  },
  {
    "id": "04325",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"container\" does not have a read-only root file system"
  },
  {
    "id": "04326",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"container\" is not set to runAsNonRoot"
  },
  {
    "id": "04327",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"container\" has cpu request 0"
  },
  {
    "id": "04328",
    "manifest_path": "data/manifests/the_stack_sample/sample_1985.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: s01-deployment\n  namespace: scenario01\n  labels:\n    app: s01-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: s01-app\n  template:\n    metadata:\n      labels:\n        app: s01-app\n    spec:\n      containers:\n      - name: container\n        image: wordpress:latast\n        ports:\n        - containerPort: 80\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"container\" has memory limit 0"
  },
  {
    "id": "04329",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"monitor\" does not have a read-only root file system"
  },
  {
    "id": "04330",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"publisher\" does not have a read-only root file system"
  },
  {
    "id": "04331",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"train\" does not have a read-only root file system"
  },
  {
    "id": "04332",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"monitor\" is not set to runAsNonRoot"
  },
  {
    "id": "04333",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"publisher\" is not set to runAsNonRoot"
  },
  {
    "id": "04334",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"train\" is not set to runAsNonRoot"
  },
  {
    "id": "04335",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"monitor\" has cpu request 0"
  },
  {
    "id": "04336",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"publisher\" has cpu request 0"
  },
  {
    "id": "04337",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"monitor\" has memory limit 0"
  },
  {
    "id": "04338",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"publisher\" has memory limit 0"
  },
  {
    "id": "04339",
    "manifest_path": "data/manifests/the_stack_sample/sample_1989.yaml",
    "manifest_yaml": "apiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: tf-r2.3.1-transformer-translate-conv-v2-32\n  namespace: automated\nspec:\n  jobTemplate:\n    template:\n      metadata:\n        annotations:\n          tf-version.cloud-tpus.google.com: 2.3.1\n      spec:\n        containers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          image: gcr.io/xl-ml-test/health-monitor:stable\n          imagePullPolicy: Always\n          name: monitor\n        - args:\n          - python3\n          - official/nlp/transformer/transformer_main.py\n          - --param_set=big\n          - --max_length=64\n          - --data_dir=$(TRANSFORMER_DIR)\n          - --vocab_file=$(TRANSFORMER_DIR)/vocab.ende.32768\n          - --bleu_source=$(TRANSFORMER_DIR)/newstest2014.en\n          - --bleu_ref=$(TRANSFORMER_DIR)/newstest2014.de\n          - --enable_tensorboard\n          - --model_dir=$(MODEL_DIR)\n          - --tpu=$(KUBE_GOOGLE_CLOUD_TPU_ENDPOINTS)\n          - --distribution_strategy=tpu\n          - --steps_between_evals=10000\n          - --static_batch=true\n          - --use_ctl=true\n          - --padded_decode=true\n          - --decode_batch_size=32\n          - --decode_max_length=97\n          - --batch_size=24576\n          - --train_steps=50000\n          env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/tensorflow:r2.3.1\n          imagePullPolicy: Always\n          name: train\n          resources:\n            limits:\n              cloud-tpus.google.com/v2: 32\n            requests:\n              cpu: 2\n              memory: 20G\n          volumeMounts:\n          - mountPath: /dev/shm\n            name: dshm\n            readOnly: false\n        initContainers:\n        - env:\n          - name: POD_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: POD_UID\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.uid\n          - name: POD_NAMESPACE\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.namespace\n          - name: JOB_NAME\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.labels['job-name']\n          - name: MODEL_DIR\n            value: $(OUTPUT_BUCKET)/tf-r2.3.1/transformer-translate/conv/v2-32/$(JOB_NAME)\n          - name: METRIC_CONFIG\n            value: \"{\\n \\\"metric_collection_config\\\": {\\n  \\\"default_aggregation_strategies\\\"\\\n              : [\\n   \\\"final\\\"\\n  ],\\n  \\\"metric_to_aggregation_strategies\\\": {\\n\\\n              \\   \\\"examples_per_second\\\": [\\n    \\\"average\\\"\\n   ]\\n  },\\n  \\\"use_run_name_prefix\\\"\\\n              : true,\\n  \\\"write_to_bigquery\\\": true\\n },\\n \\\"regression_test_config\\\"\\\n              : {\\n  \\\"metric_success_conditions\\\": {\\n   \\\"examples_per_second_average\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"greater_or_equal\\\",\\n    \\\"success_threshold\\\"\\\n              : {\\n     \\\"stddevs_from_mean\\\": 2\\n    }\\n   },\\n   \\\"total_wall_time\\\"\\\n              : {\\n    \\\"comparison\\\": \\\"less\\\",\\n    \\\"success_threshold\\\": {\\n \\\n              \\    \\\"stddevs_from_mean\\\": 5\\n    },\\n    \\\"wait_for_n_points_of_history\\\"\\\n              : 10\\n   }\\n  }\\n },\\n \\\"test_name\\\": \\\"tf-r2.3.1-transformer-translate-conv-v2-32\\\"\\\n              \\n}\\n\"\n          envFrom:\n          - configMapRef:\n              name: gcs-buckets\n          image: gcr.io/xl-ml-test/publisher:stable\n          imagePullPolicy: Always\n          name: publisher\n        volumes:\n        - emptyDir:\n            medium: Memory\n          name: dshm\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"train\" has memory limit 0"
  },
  {
    "id": "04340",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"load\" is using an invalid container image, \"robotshop/rs-load:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04341",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"load\" does not have a read-only root file system"
  },
  {
    "id": "04342",
    "manifest_path": "data/manifests/the_stack_sample/sample_1992.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: load\n  labels:\n    service: load\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      service: load\n  template:\n    metadata:\n      labels:\n        service: load\n    spec:\n      containers:\n      - name: load\n        env:\n        - name: HOST\n          value: http://payment.robotshop:8080/health\n        - name: NUM_CLIENTS\n          value: '15'\n        - name: SILENT\n          value: '1'\n        - name: ERROR\n          value: '1'\n        image: robotshop/rs-load:latest\n        resources:\n          limits:\n            cpu: 200m\n            memory: 200Mi\n          requests:\n            cpu: 100m\n            memory: 100Mi\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"load\" is not set to runAsNonRoot"
  },
  {
    "id": "04343",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"mlperf-inference-container\" is using an invalid container image, \"aferikoglou/mlperf-inference:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04344",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"mlperf-inference-container\" does not have a read-only root file system"
  },
  {
    "id": "04345",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"mlperf-inference-container\" is not set to runAsNonRoot"
  },
  {
    "id": "04346",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"mlperf-inference-container\" has cpu request 0"
  },
  {
    "id": "04347",
    "manifest_path": "data/manifests/the_stack_sample/sample_1999.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: tf-ssdmobilenet-default-1024-26\nspec:\n  template:\n    metadata:\n      name: tf-ssdmobilenet-default-1024-26-pod\n    spec:\n      containers:\n      - command:\n        - sh\n        - -c\n        - cp /root/configs/1024/mlperf.conf /root/inference/v0.5/ && cd /root/inference/v0.5/classification_and_detection\n          && MODEL_DIR=/root/models/tf-ssdmobilenet-default DATA_DIR=/root/datasets/coco-300\n          ./run_local.sh tf ssd-mobilenet gpu --scenario SingleStream\n        image: aferikoglou/mlperf-inference:latest\n        name: mlperf-inference-container\n        resources:\n          limits:\n            aliyun.com/gpu-mem: 24\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"mlperf-inference-container\" has memory limit 0"
  },
  {
    "id": "04348",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"ebs-plugin\" is using an invalid container image, \"k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04349",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"ebs-plugin\" does not have a read-only root file system"
  },
  {
    "id": "04350",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"liveness-probe\" does not have a read-only root file system"
  },
  {
    "id": "04351",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"node-driver-registrar\" does not have a read-only root file system"
  },
  {
    "id": "04352",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "privilege-escalation-container",
    "violation_text": "container \"ebs-plugin\" is Privileged hence allows privilege escalation."
  },
  {
    "id": "04353",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "privileged-container",
    "violation_text": "container \"ebs-plugin\" is privileged"
  },
  {
    "id": "04354",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"ebs-plugin\" is not set to runAsNonRoot"
  },
  {
    "id": "04355",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"liveness-probe\" is not set to runAsNonRoot"
  },
  {
    "id": "04356",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"node-driver-registrar\" is not set to runAsNonRoot"
  },
  {
    "id": "04357",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"ebs-plugin\" has cpu request 0"
  },
  {
    "id": "04358",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"liveness-probe\" has cpu request 0"
  },
  {
    "id": "04359",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"node-driver-registrar\" has cpu request 0"
  },
  {
    "id": "04360",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"ebs-plugin\" has memory limit 0"
  },
  {
    "id": "04361",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"liveness-probe\" has memory limit 0"
  },
  {
    "id": "04362",
    "manifest_path": "data/manifests/the_stack_sample/sample_2000.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: ebs-csi-node\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: aws-ebs-csi-driver\nspec:\n  selector:\n    matchLabels:\n      app: ebs-csi-node\n      app.kubernetes.io/name: aws-ebs-csi-driver\n  template:\n    metadata:\n      labels:\n        app: ebs-csi-node\n        app.kubernetes.io/name: aws-ebs-csi-driver\n    spec:\n      affinity:\n        nodeAffinity:\n          requiredDuringSchedulingIgnoredDuringExecution:\n            nodeSelectorTerms:\n            - matchExpressions:\n              - key: eks.amazonaws.com/compute-type\n                operator: NotIn\n                values:\n                - fargate\n      serviceAccountName: ebs-csi-node-sa\n      containers:\n      - name: ebs-plugin\n        securityContext:\n          privileged: true\n        image: k8s.gcr.io/provider-aws/aws-ebs-csi-driver:latest\n        args:\n        - node\n        - --endpoint=$(CSI_ENDPOINT)\n        - --logtostderr\n        - --v=5\n        env:\n        - name: CSI_ENDPOINT\n          value: unix:/csi/csi.sock\n        volumeMounts:\n        - name: kubelet-dir\n          mountPath: /var/lib/kubelet\n          mountPropagation: Bidirectional\n        - name: plugin-dir\n          mountPath: /csi\n        - name: device-dir\n          mountPath: /dev\n        ports:\n        - name: healthz\n          containerPort: 9808\n          protocol: TCP\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: healthz\n          initialDelaySeconds: 10\n          timeoutSeconds: 3\n          periodSeconds: 10\n          failureThreshold: 5\n      - name: node-driver-registrar\n        image: quay.io/k8scsi/csi-node-driver-registrar:v2.0.1\n        args:\n        - --csi-address=$(ADDRESS)\n        - --kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)\n        - --v=5\n        env:\n        - name: ADDRESS\n          value: /csi/csi.sock\n        - name: DRIVER_REG_SOCK_PATH\n          value: /var/lib/kubelet/plugins/ebs.csi.aws.com/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n        - name: registration-dir\n          mountPath: /registration\n      - name: liveness-probe\n        image: quay.io/k8scsi/livenessprobe:v2.1.0\n        args:\n        - --csi-address=/csi/csi.sock\n        volumeMounts:\n        - name: plugin-dir\n          mountPath: /csi\n      volumes:\n      - name: kubelet-dir\n        hostPath:\n          path: /var/lib/kubelet\n          type: Directory\n      - name: plugin-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins/ebs.csi.aws.com/\n          type: DirectoryOrCreate\n      - name: registration-dir\n        hostPath:\n          path: /var/lib/kubelet/plugins_registry/\n          type: Directory\n      - name: device-dir\n        hostPath:\n          path: /dev\n          type: Directory\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"node-driver-registrar\" has memory limit 0"
  },
  {
    "id": "04363",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "latest-tag",
    "violation_text": "The container \"smi-adapter-istio\" is using an invalid container image, \"layer5/smi-istio:latest\". Please use images that are not blocked by the `BlockList` criteria : [\".*:(latest)$\" \"^[^:]*$\" \"(.*/[^:]+)$\"]"
  },
  {
    "id": "04364",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"smi-adapter-istio\" does not have a read-only root file system"
  },
  {
    "id": "04365",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"smi-adapter-istio\" is not set to runAsNonRoot"
  },
  {
    "id": "04366",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"smi-adapter-istio\" has cpu request 0"
  },
  {
    "id": "04367",
    "manifest_path": "data/manifests/the_stack_sample/sample_2002.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: smi-adapter-istio\n  namespace: istio-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: smi-adapter-istio\n  template:\n    metadata:\n      labels:\n        name: smi-adapter-istio\n      annotations:\n        sidecar.istio.io/inject: 'false'\n    spec:\n      serviceAccountName: smi-adapter-istio\n      containers:\n      - name: smi-adapter-istio\n        image: layer5/smi-istio:latest\n        command:\n        - smi-adapter-istio\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          value: ''\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: smi-adapter-istio\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"smi-adapter-istio\" has memory limit 0"
  },
  {
    "id": "04368",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"certs\" does not have a read-only root file system"
  },
  {
    "id": "04369",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"certs\" is not set to runAsNonRoot"
  },
  {
    "id": "04370",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"certs\" has cpu request 0"
  },
  {
    "id": "04371",
    "manifest_path": "data/manifests/the_stack_sample/sample_2003.yaml",
    "manifest_yaml": "apiVersion: batch/v1\nkind: Job\nmetadata:\n  name: ca-certificates-(( random.String 5 \"[a-z]\" ))\nspec:\n  template:\n    spec:\n      containers:\n      - name: certs\n        image: qlik-docker-qsefe.bintray.io/edge-auth:2.57.1\n        imagePullPolicy: IfNotPresent\n        command:\n        - /bin/sh\n        - -c\n        args:\n        - cp /etc/ssl/certs/ca-certificates.crt /mnt/certs/ca-certificates.crt; $(CERTS_COMMAND)\n        env:\n        - name: CERTS_COMMAND\n          valueFrom:\n            configMapKeyRef:\n              key: caCommand\n              name: configs\n        - name: CUSTOM_CERTS\n          valueFrom:\n            secretKeyRef:\n              key: caCertificates\n              name: secrets\n        volumeMounts:\n        - name: ca-certificates\n          mountPath: /mnt/certs\n      volumes:\n      - name: ca-certificates\n        persistentVolumeClaim:\n          claimName: ca-certificates\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"certs\" has memory limit 0"
  },
  {
    "id": "04372",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "no-read-only-root-fs",
    "violation_text": "container \"jaeger-operator\" does not have a read-only root file system"
  },
  {
    "id": "04373",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "run-as-non-root",
    "violation_text": "container \"jaeger-operator\" is not set to runAsNonRoot"
  },
  {
    "id": "04374",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-cpu-requirements",
    "violation_text": "container \"jaeger-operator\" has cpu request 0"
  },
  {
    "id": "04375",
    "manifest_path": "data/manifests/the_stack_sample/sample_2004.yaml",
    "manifest_yaml": "apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: jaeger-operator\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      name: jaeger-operator\n  template:\n    metadata:\n      labels:\n        name: jaeger-operator\n    spec:\n      serviceAccountName: jaeger-operator\n      containers:\n      - name: jaeger-operator\n        image: jaegertracing/jaeger-operator:1.8.2\n        ports:\n        - containerPort: 60000\n          name: metrics\n        args:\n        - start\n        - --platform=openshift\n        imagePullPolicy: Always\n        env:\n        - name: WATCH_NAMESPACE\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.namespace\n        - name: POD_NAME\n          valueFrom:\n            fieldRef:\n              fieldPath: metadata.name\n        - name: OPERATOR_NAME\n          value: jaeger-operator\n",
    "policy_id": "unset-memory-requirements",
    "violation_text": "container \"jaeger-operator\" has memory limit 0"
  }
]